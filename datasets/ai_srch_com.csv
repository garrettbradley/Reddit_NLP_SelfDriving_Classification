,author,body,created_utc,id,parent_id,subreddit,subreddit_id,year
0,Xochipilli,"Hi,

I'm a moderator of /r/aiclass. To anyone who is curious: We deleted one post of bigbasterd because he was quoting and enabling a crazy spammer we had for the last few days. If anyone is curious this is his post that was deleted:

&gt;&gt;Sebastian Thrun who was the project leader of this self-driving car project had deliberately trapped me in collateral with a criminal suspect named Gabriele Scheler in Stanford. People behind Thrun had systematically molested many years of my life without an end. Google’s Eric Schmidt had threatened my life with a real murder case of Stanford student May Zhou (http://www.mayzhou.com) for sake of Sebastian Thrun during their fight with Stanford.
&gt;&gt;Investigation from authorities after my tip confirms that it is people on Schmidt and Thrun’s side who’s behind May Zhou’s murder case in order to threaten me and to terrorize Stanford. And the power on their side did try to plot a murder on me while I was in California-. Before the case could be publicly clarified, neither Thrun nor Schmidt’s name is clear in such plotted murder. So far, they dare not deny such accusation-s but pretend not seeing while publicly losing their faces.
&gt;&gt;In the past, Thrun’s bosses had tried to get me work with Sebastian Thrun as a settlement of crimes from Thrun’s side, but I never compromised a bit, because as I told the investigat-ors, that it is unfair to that innocently murdered girl May Zhou.
&gt;&gt;It’s unfair to myself as well, as Eric Schmidt, Sebastian Thrun and Gabriele Scheler’s side did try to murder me while I was in California;
&gt;&gt;Who wants to work with a professor who’s misbehaviors had caused the murder of an innocent student of their own school anyway.


&gt;from the comments on http://www.webpronews.com/google-self-driving-cars-accident-2011-08
&gt;wow......... yeah seems mad

I banned him from /r/aiclass because he was verbally assaulting users and moderators.
",1315903603.0,c2jihk4,t3_kdha9,artificial,t5_2qhfb,2011
1,jeremiah256,"Obviously, companies like IBM with their Watson or Google and their self-driving cars come to mind with regard to need for AI Engineers.  But also look at companies where AI, as a tool set, would work in solving problems, especially with business intelligence and data mining.  The job description would probably not be listed ""Wanted - AI Engineer"" but the KSAs would fit.  That opens you up to medical fields where they've been experimenting with AI (example would be how diseases spread in the game World of Warcraft), disaster recovery, military and national intelligence, etc.  

AI and bots are the future.  Every car commercial advertises how their cars can keep us humans from killing ourselves by relying on their smart vehicles.  If costs need to be contained, AI is not only going to be part of medical research but medical procedures.  Wall Street relies on AI to do trading magnitudes faster than any human can (for good and bad).

I guess what I'm trying to say is that if you want to go into AI and can afford it, it's a field that, IMHO, can be used in practically any other field.

Good luck!",1316526962.0,c2l89km,t3_ki141,artificial,t5_2qhfb,2011
0,Gyrodiot,"If you're comparing natural intelligence with AI, you can compare human vs. computer performances in various games (chess, checkers, Jeopardy, etc.)

There isn't any generic intelligence test for computers (there isn't even one for humans), but if you're looking for comparison, any experiment that tries to perform a highly difficult task (self-driving cars, natural language processing) will fit to your project.",1329605611.0,c3sm4wh,t3_pvmvy,artificial,t5_2qhfb,2011
1,deong,"And it specialized itself for object tagging. So it got about 100th of the way to solving 1/1,000,000th of the requirements of what we're talking about here.

How many more nodes would you need to throw at the trained system before it could figure out how to write an iPhone app? Google Translate once told me that an apartment I was looking at was located ""in a low testicle""; your same neural network is going to need to understand that that isn't right. The self-driving car needs to be able to understand directions like, ""you go almost up to the big green building -- you'll see it -- and then the road's going to fork. If you go left, you can get there, but there's some construction going on, so you're better off going right up to the street...oh, it starts with an 'S'....""

The mind *is* the brain -- I'm not arguing that there's some non-physical process that has to be there to generate intelligence. However, I doubt that our current neural network models are rich enough to actually model the brain with much fidelity at all, and I suspect that, if the way forward to try to mimic a brain via some connectionist approach, we're going to have to both learn more about how brains work as well as putting that knowledge to use with richer models.",1346064599.0,c5ze2f5,t1_c5zawha,artificial,t5_2qhfb,2011
2,mikevdg,"There isn't anything impeding our progress. There is research occurring in universities into specific aspects of AI (c.f. AGI). There is commercial interest (Google driverless cars, Darpa stuff, Mars rovers). There are individuals such as myself who are trying to make their own. We already have lots of computing power and more is on the way.

AGI (strong AI) is *hard*. 

Look at the only example we have, the [human brain](http://en.wikipedia.org/wiki/List_of_regions_in_the_human_brain). It has 80 to 120 billion neurons, each with possibly thousands of synapses. Not only that, but the general configuration of those neurons at a macroscopic and microscopic level is predetermined by our DNA which has undergone evolution for millions of years. That evolution is important; our brains are preconfigured in a VERY COMPLEX way to not only have a basic level of functionality (i.e. reflexes, instincts) but also have several learning systems to allow us to accumulate, filter, process and store information.

Personally, I think an AGI will have a list of hundreds of modules not dissimilar to the human brain link above. Each of those will require research. Each of those will take years to get right. We're currently working on them, bit by bit.

It may eventuate that an AGI isn't actually that useful. What we really wanted was something less intelligent but more useful in specific scenarios. For example, a self-driving vehicle doesn't need to be a full AGI.",1346208142.0,c60a4tp,t3_yvpi1,artificial,t5_2qhfb,2011
3,howlin,"&gt; They say ai significantly smarter than homo sapiens that can contact with human (even only with text messaging) can always persuade dumber people to ""let them out of the box"".

That depends on what ""smarter"" means.  Does it simply mean ""write essays and converse and critique movies, [at the same capability level as a human]"" as marshallp claims, or does it mean to have the desire to be let out of the box, the as well as the awareness that it is inside a box, and have the capacity to have a two-way conversation to convince someone it should be let out.  These are not the same thing.

Google's efforts can and will lead to ""idiot savants"" that are really good at solving particular tasks based on human-generated constraints. (Humans had to tell the self-driving car what it means to drive well)  Google's efforts won't lead to GAI without a lot more basic research into what constitutes a mind.",1346620096.0,c62hjcf,t1_c62gkpn,artificial,t5_2qhfb,2011
4,marshallp,"What is ""Strong AI""? Is it a machine that learns anything? We already have that - it's called machine learning.

Irrevelant stimuli - that is the whole point of machine learning - to generalize by removing noise

scheduling learning - when do humans schedule it? Humans are machine that performs some goal function. Machine learning can do that too, even to that point of approximating the ""human goal"" function (as defined by a data set)

training vs operating - same issue as scheduling

autonomy - same as above - you give it a goal function (as defined by training on a dataset) and set it free in the world

... other points reduce to the above

logical reasoning - how do humans do it? they are not equipped with prolog engine - it is simply an ""emergent"" artifact of the neural network (striving to a goal function)

What exactly is reasoning? They're needs to be a precise definition to attempt to answer that. If ""human reasoning"" - then it's simply an artifact of the goal function of humans.

... a lot of questions reduce to ""does it have memory"". yes, neural networks can have memory

DLN's have been used to physically control robots - the LIGAR project of yann lecun and his current darpa drone project. Traffic sign competition won by schmidhuber that will probably be in autonomous cars.

grad students training etc. - any AI needs to be built somehow. If you train it to an autonomous function (as defined by a data set) then it can go out and be autonomous in the manner desired (self-driving car, drone, ""human equivalent"" teacher/fireman/nurse)
",1347552819.0,c67n2s4,t3_ztriv,artificial,t5_2qhfb,2011
5,needlzor,"I think you have two ""good"" options, depending on your goal. 

If you want to attract, if you want to target the average Joe, go for the applicative road: get a set of applications of AI and then detail the techniques and the fields involved. For instance self-driving cars, boardgames AIs, search engines, recommendation systems, OCR systems, etc. all rely heavily on AI techniques.

If you want to be more detailed, if you don't care for the average Joe and your public is already interested in the field and you merely want to inform them in a more structured manner, go for the academic road: group it by field instead of method, and then give details on some hot topics of the field (obviously you won't be able to be exhaustive, so pick the most interesting/biggest ones). So instead of GAs, ANNs, etc.  you would have for instance:

* Knowledge representation: logical representations, graphical representations, advantages of each in terms of expressivity vs. computational complexity
* Automated reasoning: symbolic reasoning (rule based systems), statistical reasoning (typically inductive reasoning: build generic knowledge from a collection of data and use that generic knowledge to reason about a specific problem), analogical reasoning (a is to b as c is to d) and its specialization: case-based reasoning (this new problem is similar to this old problem, so I should adapt its solution and see if it fits), reasoning under uncertainty (probability theory, possibility theory, dempster-shafer theory of evidence, etc. [this](http://www.cost-ic0702.org/summercourse/files/dubois_and_prade_2009_uncertainty.pdf) amazing paper gives a good overview of the field), monotonic vs nonmonotonic reasoning, commonsense reasoning...
* Combinatorial Optimization: genetic algorithms and all those fancy naturally inspired metaheuristics (ant colony optimization, etc.)",1351857705.0,c6v9vdc,t3_12glui,artificial,t5_2qhfb,2011
6,_bfrs_,"&gt; When it comes to perception of the real world, deep learning is unchallenged. Today, te are at the point where we can say that the perceptive portion of a ""Lt. Cmdr. Data"" will be a deep learning network.

It looks like gradually everybody is reaching this consensus. For example, Ben Goertzel plans on using DeSTIN (another DLN) for the perception part of OpenCog (instead of the earlier plan of using PLNs (Probabalistic Logic Networks)  all the way down), and if I remember right, Sebastian Thrun hinted (in his 2011 AI class) that while the Google self driving cars currently don't use DLNs for perception, in future they would be incorporated into the system. 

&gt; DLNs cannot adapt their behavior in realtime to changes in the environment. They can't really be used to plan either. DLNs are fantastic at static imagery and static audio. But there are other networks that are significantly better suited to spatio-temporal movement. (eg. Izhikevich's polychronous networks )

Any idea what Hinton's take is on **DLNs current inability to find spatio-temporal patterns, i.e., learn verbs and not just nouns?** 

BTW *static audio* is a nice oxymoron.",1353976386.0,c778vhz,t1_c773w1z,artificial,t5_2qhfb,2011
0,HastyToweling,"Well you're absolutely right about the end of clock doublings, and no one can doubt it.  To this point, he's got one graph that I've found in The singularity Is Near that shows and expected speed of about 11MHz in 2013.  He's pretty clearly off the mark here, but that is just one prediction (albiet an important one)--he has many others that have turned out to be exactly right (self driving cars being a good recent example).  

[Moore's law](http://en.wikipedia.org/wiki/File:Transistor_Count_and_Moore%27s_Law_-_2011.svg) itself (transistor count) has continued unabated to this day--although it appears intel is speculating an end coming pretty soon.   

The end of clock speed doublings is unfortunate, but most Machine Learning algorithms are easily parallelized (the human brain is the ultimate example).  I'm not sure we can so easily dimiss the idea of accelerating change based on clock speed alone.  

Edit: holy hell, just saw your longer post below.  Ignore what I said about closk speed alone.",1359744482.0,c87n3xv,t1_c87esqu,artificial,t5_2qhfb,2013
1,itikka08,"These fields and paths first came up my mind:

* Software consulting companies specialized in somehow difficult problems. Also normal consulting companies might employ a couple of specialist developers per team/company for more algorithmic tasks.
* Machine Learning type Data Mining and analysis companies. This might or might not be just a buzz, but there are some of these companies and when looking at their recruitment pages, AI know-how is on their wish-list.
* Gaming industry if you are lucky. Also look for middle-ware companies that create tools for game making. In this field AI should be most of all fun to play with/against, and it should not hog all of the resources.
* Startups. There are plenty of crazy ideas, some of which require AI as a part of it.
* If you can do non-trivial computer vision, I think many industrial automation companies would love to have you developing their products. Same might be with speech recognition, there are large companies that really would like to have those kind of features in their future products.
* How about neuromorphic computing... Just last month I noticed Qualcomm is searching a dev for their research lab.
* Self-driving vehicles, i.e. car companies.

I have no idea where people really would use old-skool symbolic AI. I would recommend to become an ace software developer, how to be a part of a software development team, how to design software and how to implement maintainable code. Those are things that are usually not taught in universities. Those are also the things all the recruiters I have seen have listed as requisite.

Do not restrict yourself too much.",1361469841.0,c8j6tn4,t3_18ynaj,artificial,t5_2qhfb,2013
2,visarga,"&gt; There is a notion of success ... which I think is novel in the history of science. It interprets success as approximating unanalyzed data.

This 'approximation' is good enough to drive cars better than humans, beat the best people at Jeopardy and do other thousand boring, yet until recently, impossible to automate human tasks.

Here is a counter reply from the opposition: [Peter Norvig](http://norvig.com/chomsky.html).

&gt;I think Chomsky is wrong to push the needle so far towards theory over facts; in the history of science, the laborious accumulation of facts is the dominant mode, not a novelty. The science of understanding language is no different than other sciences in this respect.

In other words, Chomsky is dated on this issue. He rejects tools that can't be understood in all their aspects (such as neuronal nets which have millions of parameters, each of which has a fuzzy meaning to us). But the problem sometimes requires a complex model. We can't always fit the model into our working memory to visualize it all at once.


&lt;rant&gt;
We live in a world of machine learning. The practical applications have taken us to a reality that borders on sci-fi. Let's enumerate the successes of statistical models in AI:

- information processing: text to speech, OCR, speech recognition, information extraction (Watson), question-answer systems (Siri), search engines, automated translation, smell checking, spam detection
- social: content recommendation
- movement: drones, robots, self driving cars
- medical: tumor detection in scans, medical diagnosis (Watson again), robotic surgeons
- smart robots in agriculture: targeted delivery of pesticides, fruit harvesting, weeding
- education: fine tuning learning to each student
- security: face recognition, threat detection
- quant funds: high frequency trading
- credit scoring
- retail: inventory prediction for large retailers (Walmart etc)
- space: driving a robot on Mars, monitoring the sky (detecting planets, asteroids etc)

So we have information processing, health, agriculture, industry, security, education, finances, space exploration - all of these already heavily rely on statistical AI.

We might not ""understand"" every variable in the models we use, but they work, and we understand the principles behind it. They're plenty good for me.

Also, in the last years we have seen semi supervised (computer aided) ontology building. This boosts Chomsky's direction - symbolic logic.

Edit: forgot to mention about elections - Obama's strategy was ""data driven""; he was hiring quants to help him focus his campaign and collect more donations; they didn't waste money, instead, they focused it where it counts; it's like black magic for the opposition


&lt;/rant&gt;",1367252204.0,c9oyvnu,t3_1dby3p,artificial,t5_2qhfb,2013
3,needlzor,"Artificial Intelligence is too wide of a topic to be approached that way, it delves into mathematics, linguistics, logic, philosophy, economics ... A much better way to approach it is to start with an application of AI that appeals to you (discussing the philosophical points of view on thinking machines? Building self-driving cars? AI for videogames? Machines that can understand natural language? AI for search engines?) and then backtracking to the corresponding subdomains of AI (knowledge representation, automated reasoning, multi agent systems, machine learning, etc.) and starting your literature survey.

Source: I am a PhD student in AI for search engines",1373545041.0,cb0dnj3,t3_1i2f81,artificial,t5_2qhfb,2013
4,_bfrs_,"Politicians are mostly interested in maintaining the status quo. So, they act only when there is a serious and immediate threat to ""their way of life"". The Manhattan project was triggered by Leó Szilárd's letter to FDR which explained in plain language the damage a single fissile bomb could do, and the Apollo project by Sputnik beeping from its orbit, in effect the Russians announcing in style, now *""all your base are belong to us""*. Its not a coincidence that the end of the cold war was the beginning of the AI winter. Up until that point AI in the US was mostly nourished by DARPA and a bit by NSF and other agencies. During most of the cold war it was believed that strong AI was imminent. 

Ok, so that explains why government isn't interested, but what about the 'free market'? I think the reasons are more complicated. Most market players prefer evolutionary changes to revolutionary ones. A guaranteed return this year (narrow AI) vs. a massive payoff with 50% probability in 10 years (AGI). There are a few, but considerable number of people in the world today with money to throw on such long shots. Their efforts aren't usually seen in the mainstream. For example Founder's Fund (Peter Theil, Sean Parker etc) gave $15 million last year to Dileep George's startup, Vicarious, to figure out the 'cortical algorithm'. Another less known startup is Scott Hassan's (geocities/yahoo groups) Willow Garage, which made the PR-2 and the opensource robot OS, ROS. And of course Sergey Brin's GoogleX is well known. The self driving cars are of course narrow AI, but rumors are they have bigger plans. 
",1374945283.0,cbbggwx,t1_cbbc4zt,artificial,t5_2qhfb,2013
5,river-wind,"If you've done undergraduate level CS, or are at least familiar with common algorithms and data structures already,  check out Udacity.com's Intro to AI: https://www.udacity.com/course/cs271   It's taught by the head of the Google self-driving Car project, and the guy who wrote AI: A Modern Approach.  And it's free!

Even though the class isn't going on right now, you should be able to watch all the video lessons and take the automated quizzes.

It doesn't get into Deep Learning specifically, but other free online AI classes do use many of the topics covered in that class as their basis.  

Other classes to do next:  
https://www.udacity.com/course/cs373  AI for Robotics  
https://www.coursera.org/course/ml Machine Learning  
https://www.coursera.org/course/neuralnets Neural Networks for Machine Learning  
https://www.coursera.org/course/nlangp Natural Language Processing  
https://www.coursera.org/course/aiplan AI Planning  
https://www.coursera.org/course/compneuro Computational Neuroscience  ",1385421423.0,cdmy57n,t3_1rg206,artificial,t5_2qhfb,2013
6,burndirt,Self driving cars. ,1387254726.0,ce3mgxg,t3_1sx342,artificial,t5_2qhfb,2013
0,CyberByte,"&gt; AI can be everything from philosophy to psychology and nevrology.

Exactly.

Okay, so you're interested in machine learning. There are a couple of different kinds:

* In *supervised learning* you present your learning algorithm with data for which you have specified the correct output, and it will learn by example and hopefully generalize to unseen data. I think this is the most widely used paradigm and it is used for things like prediction and detection.
* *Unsupervised learning* can be used to find structure in unlabeled data (i.e. you don't know the desired output) and is often used in data mining.
* *Reinforcement learning* is more aimed at interacting with a world and only provides feedback to the algorithm in terms of (positive or negative) rewards. In many situations we don't know what should be done, but we know when the outcome (possibly after many actions) is good or bad. For instance, in games it can be difficult to specify how good each move is, but we know when you've won/lost. Or we don't know how to make a self-driving car drive exactly, but we know it's bad when it gets off the road or crashes into something.

Within these, there are many different algorithms and application areas (such as computer vision, business intelligence and robotics). Since you say you want to learn on your own, it's probably best to Google those terms.

For supervised learning, I find Andrew Ng's Machine Learning Coursera course to be a very nice introduction with programming assignments. ",1396003825.0,cge1pdj,t1_cgddleb,artificial,t5_2qhfb,2014
1,metaconcept,"That, my friend, is literally *the* one million dollar question, and that's only the seed capital.

Consider Google's self-driving cars which will likely take over the entire transport industry. Or automated trading systems. Or speech recognition and comprehension which will likely make big inroads into call centres over the next decade. Or Siri. Or small automated delivery robots. There are just too many possibilities; we're moving from the information age into the AI age. You need to find a problem and then invent a really good solution - and do other stuff that makes a start-up successful.

Good luck. /r/startups.",1397627507.0,cgtpila,t1_cgtp44e,artificial,t5_2qhfb,2014
2,Noncomment,"Except it's already happening. Manufacturing jobs have been almost entirely outsourced or automated. The remaining will be very soon. A lot of those people moved to service jobs many of which can also shortly be automated. Soon self-driving cars will soon replace millions of truck drivers and similar jobs.

There is absolutely no economic law that says the value of labor can never go down or has to be higher than minimum wage (or even enough to live on.)",1401309595.0,chtf46j,t1_chq92hj,artificial,t5_2qhfb,2014
3,the_ai_guy,"&gt; yes. However I am against mass factory farms with inhuman conditions. Eating other animals is natural. However, prolonged torture in cages and forced feedings. Is cruel. Death is not as cruel as you think it is. Everyone dies. 

I love that you have elaborated to the extent that you did. I agree that torturing the animals in the process is a terrible thing. It also leads to making the food less safe as well. Such as chickens that have only about much room as my laptops computer area to sit in. That is just insane to me. Then the folks that fed cows cow-biproducts to round them out and ended up with the mad cow thing is just nuts too. WTF people. Money is not that important to make that shit happen. Anyhow, moving on. 

&gt; I believe ai that can self identify is sentient. 

I feel that you have answered in a way that doesn't answer the original question but a version of the question in which you have constructed in your mind to avoid the original question. Please answer the original question as asked or explain further to show why you have answered in the way you have. This will give me an understanding I can work with. Otherwise the answer is not useful I feel.

&gt; we do follow programs based on need but we can self program and change our behavior. A program on the pc cannot. 

I disagree. There are many programs that are dynamic and change behavior as the contexts and inputs change. This can be seen with self driving programs as well as behavioral modeling in CAD programs that update outputs based on input structures. Your response to this is?

&gt; society is a construct. we made all our rules, our taboos etc. 

While this is true, I'm not sure it fits the situation in which we are talking about. This is a large reason why I am asking people to define their thoughts on it. It illustrates well that these things are not well defined enough to actually work with effectively. While you didn't give an answer in the way I thought you would, it still holds value in an educational way.

&gt; self replicating vs made by force. 

I'm guessing that you mean self replicating is in terms of grow and that by force is for the term of construct.",1403805601.0,cihrjb6,t1_cihr2we,artificial,t5_2qhfb,2014
4,Ali-Sama,"&gt;I love that you have elaborated to the extent that you did. I agree that torturing the animals in the process is a terrible thing. It also leads to making the food less safe as well. Such as chickens that have only about much room as my laptops computer area to sit in. That is just insane to me. Then the folks that fed cows cow-biproducts to round them out and ended up with the mad cow thing is just nuts too. WTF people. Money is not that important to make that shit happen. Anyhow, moving on.

I agree 100%.

&gt;I feel that you have answered in a way that doesn't answer the original question but a version of the question in which you have constructed in your mind to avoid the original question. Please answer the original question as asked or explain further to show why you have answered in the way you have. This will give me an understanding I can work with. Otherwise the answer is not useful I feel.

When they start to question who they are. What is the reason of their existence. Free will. Essentially.

&gt;I disagree. There are many programs that are dynamic and change behavior as the contexts and inputs change. This can be seen with self driving programs as well as behavioral modeling in CAD programs that update outputs based on input structures. Your response to this is?

a cad program does not go. ""Fudge this. I want to be a video game. Cad is boring, video games are fun!""

 
&gt;While this is true, I'm not sure it fits the situation in which we are talking about. This is a large reason why I am asking people to define their thoughts on it. It illustrates well that these things are not well defined enough to actually work with effectively. While you didn't give an answer in the way I thought you would, it still holds value in an educational way.

It is a structure which is made by actually investing time and energy into it, in a planned way. 

&gt;I'm guessing that you mean self replicating is in terms of grow and that by force is for the term of construct.

A chair is constructed. We don't see chairs in nature. We have to imagine one, and make it. A tree grows as it makes more cells. That is the difference. I will try and think of a better way to explain this. 
",1403806168.0,cihrui9,t1_cihrjb6,artificial,t5_2qhfb,2014
5,[deleted],"To quote wikipedia:

_Currently (as of June 2014), the system works with a very high definition inch-precision map of the area the vehicle is expected to use, including how high the traffic lights are. Also the computation is not performed on the car itself but on remote computer farms._

So unless there is a big shift to AI that can drive places that have not been precision mapped beforehand, ""all the way down to tiny details like the position and height of every single curb"", most of your questions can be answered with ""not at all.""

http://www.theatlantic.com/technology/print/2014/05/all-the-world-a-track-the-trick-that-makes-googles-self-driving-cars-work/370871/
",1405725854.0,cj1cufb,t3_2b38p2,artificial,t5_2qhfb,2014
6,80sKid,"For self driving cars to fully work, they will need to be mandatory. That way you take out the ""other driver's bad habit scenario"". It will be the AI communicating with other AI to make safer more efficient driving. 

It's still in it's infancy. We wont see a society with pure 100% driver less vehicles until at least the later part of this century. When you start to see semis that are driver less, then you know we are almost there. There are simply too many variables as it is for driver less cars to be mainstream. ",1405756346.0,cj1ninj,t3_2b38p2,artificial,t5_2qhfb,2014
7,the_ai_guy,"I don't think you broke down the question proposed far enough before answering.

Paper flowing across the road vs sheet of steel of the same size flowing across the road. Images of people being drawn on the road vs actual people on the road. Snow chunks on the road vs white rocks on the road of similar size as snow chunks. Deer on the road vs a person on the road. (this one is a significant distinction because with deer you are supposed to speed up to reduce damage whereas humans you are supposed to do anything in your power to avoid running into)

I could go on but don't think I need to. Considering your response, I do not feel confident in self driving cars being out there at all. If you were so confident in your thoughts as an engineering type person, I do not feel confident in the folks building the self driving cars that they would think any further than you have.",1405798913.0,cj1zkvg,t1_cj1z17e,artificial,t5_2qhfb,2014
8,_xyx,It seems like Sebastian Thrun heavily influenced the google self-driving car. So you can assume they have level of autonomy of DAPRA grand challenge cars (they were completely offline) at least. Plus cloud-based computation.,1405815054.0,cj25xrz,t1_cj1cufb,artificial,t5_2qhfb,2014
9,mkgs,"&gt; The program that drives your self driving car has no idea what a pedestrian is really, it's just a particular type of pattern.",1415675185.0,clz1rwy,t3_2lvbtd,artificial,t5_2qhfb,2014
10,metaconcept,"I think his AGI predictions are conservative.

You don't need human-level intelligence for an AI to be dangerous. It doesn't need to be conscious, self-aware, emotional or philosophical. It just needs to be smart enough to fight for survival and prosperity, in the same way a scorpion or ant colony might fight for survival or prosperity.

I'm waiting for the military to invent self-driving tanks, and for somebody to get a  computer game AI (e.g. from Starcraft) to control them.",1418172253.0,cmqasyz,t3_2osbq8,artificial,t5_2qhfb,2014
0,CyberByte,"I'm not a lawyer, but I think laws exist that affect:

* self-driving cars (driving them is legal in some places but not others)
* drones (maybe see http://dronelawjournal.com/ and http://dronelaw.net/)
* data storage and anonymization (affects data mining)

Also, I would assume that it's illegal for me to program a robot to break the law, although it may be difficult to prove that I intended to break the law... Legal responsibility/liability also seems to be an important part of the debate on self-driving cars.

A lot of existing laws probably affect AI even though they were written without AI in mind. For instance, I believe that in some places drones are just classified as aircraft, so then aircraft law applies to them.

If there is a law specifically aimed at AI, there is also a chance that it uses different terminology. AI is notoriously difficult to properly define, so it might be better to talk about broader concepts like system, artifact, machine or even computer program. But this is mostly just speculation on my part.",1425835681.0,cp89mfx,t3_2ycdtg,artificial,t5_2qhfb,2015
1,Sqeaky,"This might be neat from a philosophical perspective but it has little to do with reality. Cars will be able to react in millionths of a second and have close to optimal stopping distance. 

Self driving cars won't speed. They won't take silly risks because they are upset, tired or drunk.

Not having self driving cars is costing the USA 36,000 lives per year and the More than one million lives per year: http://en.wikipedia.org/wiki/List_of_countries_by_traffic-related_death_rate

",1425908691.0,cp93cey,t3_2yddyo,artificial,t5_2qhfb,2015
2,sixwings,"There was a time when I used to think that fully autonomous vehicles were just around the corner. I'm not so sure anymore. It seems more and more that self-driving cars will have to have a lot more human-like common sense than researchers anticipated.

I'm afraid that fully autonomous vehicles will have to wait for someone to solve the AGI problem. It could happen in a year or it could happen in five hundred years. Who knows?",1425935680.0,cp9j85s,t3_2ygohn,artificial,t5_2qhfb,2015
3,DeathorGlory9,"The jobs will slowly be replaced by robots, I will be a gradual process over several decades or even centuries (this assuming best case scenario where we dont end up in a mad max or 1943 style world.).

Major job industries will be lost in the next decade or two with the advent of self driving electric cars. Taxibus/train/truck drivers, gas station attendants will all lose there jobs with almost no replacements. Other industries are only a matter of time.

I would be surprised if AI's actually did any fighting I think robots with human controllers are much more likely.  ",1428200827.0,cq1k9tl,t1_cq1jxvf,artificial,t5_2qhfb,2015
4,lars_,"&gt; &gt; authority that should be ascribed to his predictions about the future.

&gt; You understand that is a logical fallacy right there right?

What I'm saying there is that the factual things he talks about are wrong, so it isn't likely that his speculations will turn out be any less wrong. I don't see the logical fallacy, perhaps if you'd point it out it'd be easier.

The factual claims he's making are undeniably wrong. First, he's saying some kNN-variant is in play with Google's autocomplete and with self driving cars.

Google's autocomplete can't be done with kNN, you'll need something like Hidden Markov Models or Recurrent Neural Nets, so you can model sequences. I believe it was confirmed at some point that they used HMMs.

Self-driving cars use a whole host of algorithms, but kNN isn't relevant. This is because the car's sensor input is very high dimensional, which kNN handles notoriously poorly. They will use Convolutional Neural Nets for perception, and something like Particle filters to model the world. There's [a course on Udacity](https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373) that explains the basics, taught by the guy who lead the self driving cars team at Google.

He also says that brute force search is a new approach to AI. Not true, this idea is as old as computer science itself, and literally the first chapter of the standard AI textbook.

AI is an academic field where a lot of smart people have been doing good work for a long time. A lot of things are well understood. It's pretty arrogant when someone just makes up falsehoods from their own intuition and then proclaims it as fact.",1429363275.0,cqggnjp,t1_cqgfrng,artificial,t5_2qhfb,2015
5,overclockedpathways,"&gt; I don't see the logical fallacy, perhaps if you'd point it out it'd be easier.
Appeal to Authority Logical Fallacy - http://www.logicallyfallacious.com/index.php/logical-fallacies/21-appeal-to-authority

&gt; First, he's saying some kNN-variant is in play with Google's autocomplete and with self driving cars.

&gt; Google's autocomplete can't be done with kNN, you'll need something like Hidden Markov Models or Recurrent Neural Nets, so you can model sequences. I believe it was confirmed at some point that they used HMMs.

Points of speculation: 4

* **Google's autocomplete can't be done with kNN,**
* **you'll need something like Hidden Markov Models or**
* **Recurrent Neural Nets, so you can model sequences.**
* I believe **it was confirmed at some point that they used HMMs.**

&gt; Self-driving cars use a whole host of algorithms, but kNN isn't relevant. This is because the car's sensor input is very high dimensional, which kNN handles notoriously poorly. They will use Convolutional Neural Nets for perception, and something like Particle filters to model the world. There's a course on Udacity that explains the basics, taught by the guy who lead the self driving cars team at Google.

Points of speculation: 3

* Self-driving cars use a whole host of algorithms, **but kNN isn't relevant.** 
* This is because the car's sensor input is very high dimensional, which kNN handles notoriously poorly. **They will use Convolutional Neural Nets for perception,**
* **and something like Particle filters to model the world.** There's a course on Udacity that explains the basics, taught by the guy who lead the self driving cars team at Google.

&gt; It's pretty arrogant when someone just makes up falsehoods from their own intuition and then proclaims it as fact.

I tend to agree. Why have you thus made at least 7 points of ""speculation on the subject of AI"", as you claim the author of the articles has?

The point is that YOU and anyone else can't see the forest for the trees because you are too deep in and now you see lots of right and lots of wrong and others see the same thing but shifted a couple degrees from someone else in the same field of study. It just happens. To be honest, strong AI has already been solved.

",1429372701.0,cqgkhpb,t1_cqggnjp,artificial,t5_2qhfb,2015
6,Yasea,"Agriculture was mainly mechanized first. Sure, it saved a lot of manpower but isn't automated. Still a human controlling all actions with levers and buttons. Although this was enough to drive employment in agriculture to below 5%.

Now the self driving tractors, milking robots, automated feed mixers on the farm, drones for inspecting fields are appearing, but it wont matter much for employment for the population at large.

But our creative people will still see a whole lot of routine tasks they do disappearing into the machine so they can do the work of a team on their own.",1429791207.0,cqlwhs4,t1_cqlruei,artificial,t5_2qhfb,2015
7,yogthos,"The thing to keep in mind is that there's nothing magical about how the brain works. It's a machine that got tuned to do what it does by means of natural selection.

While we don't know the specific of how it works, we now have a number of plausible theories regarding that. The AI system we build are rapidly becoming competitive with the brain in many ways. Great examples of that are things like Watson or self driving cars. There are also a number of projects as seen [here](http://www.technologyreview.com/news/536326/ibm-tests-mobile-computing-pioneers-controversial-brain-algorithms/) and [here](http://www.artificialbrains.com/darpa-synapse-program) that are experimenting with brain inspired architectures.

The other approach, I already mentioned, is to simply to try to evolve intelligence. We evolved, so given the right constraints we should expect machine intelligence to evolve as well. There's very little reason to believe that the way our brains work is somehow unique and cannot be reproduced on another substrate.

",1429844849.0,cqms6wk,t1_cqmrofy,artificial,t5_2qhfb,2015
8,maeon3,"Take it from the guy who championed the first self driving car.  Sebastian Thrun:

http://www.amazon.com/Probabilistic-Robotics-Intelligent-Autonomous-Agents/dp/0262201623

This is not intro to CS stuff, this text expects you to have a thorough mastery of Computer Science, Algorithms/data structures, and applied Mathamatics.  

This is the textbook given by Sebastian Thrun in his class he's giving at Georgia Tech fall semester I'm taking for a masters degree in AI.

A.  Lots of exercises after every chapter.

B.  Photographs and diagrams of the algorithms that approach the problems.

C.  Pseudo code algorithms demonstrating tracking in toward a solution to problems previously thought only solvable by humans.

A real world demo of the concepts in this book: https://www.youtube.com/watch?v=SxGY4iH5AAc",1430997654.0,cr19qn5,t3_3558zh,artificial,t5_2qhfb,2015
9,maeon3,"No idea, but based on my understanding of the problem they use visual spacial reasoning principles.  3D Lidar to find the candidate objects that might be traffic lights.  Visual confirmation to identify the traffic light.  Discern the colors and the signals, then interpret the colors.  However the far more important and difficult problem is the probability that another car is going to run the red light, or stomp on the gas just as you are about to go, based on past behavior of the self driving car.  

A lot of people are going to be screwing around with the self driving cars.  Lurching forward and stomping on the brake so the self driving car freaks out and comes to an abrupt halt, just to annoy the passengers of the self driving car.  

The: ""I'm not touching you... I'm not touching you."" game.  ",1431019766.0,cr1lax9,t1_cr1ect8,artificial,t5_2qhfb,2015
10,ww3ace,"I'm disappointed that this design expresses the teacher student model of the cerebral cortext, which expresses the generalization of the behaviors driven by the midbrain, but doesn't touch on other behavior learning mechanisms that are far more important to creating the motivational aspects of an independent agent such as the effect of dopamine/basal ganglia interfacing. This aspect of ai tech is probably more applicable to current design challenges though; I've working with the concept of using this pattern to design self driving car systems based on the behavior of real drivers.",1432743319.0,crmlwk7,t3_37eaje,artificial,t5_2qhfb,2015
11,tyggerjai,"No worries. The Turing test, these days, occupies a spot, to my mind, in the same conversational category as ""what is a robot?"". There's all kinds of definitional lines you can draw and endless ""debate"", but at the end of the day it doesn't matter - it's hanging empty definitions on a field that's so incredibly rich it blew past the definitions decades ago. The field of HRI is so absolutely *not* concerned with the Turing test, because in a sense the Turing test is very hard, but in another sense, as the other commenter says, it's an utterly pointless test of ""intelligence"". 

Are you familiar with the stuff coming out of the MIT Media lab? If you want cutting edge human/robot interaction, that's some pretty impressive stuff, but it has little or nothing to do with, say,  self-driving cars, which are largely orthogonal to HRI. Well, except in the areas of joint attention, and shared world view ... and then you're way beyond the Turing test anyway. 

Asking why (whether?) we still care about the Turing test would be a far more interesting article.",1433900328.0,cs1agv7,t1_cs19dq6,artificial,t5_2qhfb,2015
12,tjpalmer,"Yeah, looks like they mostly know the research space, but they're just one of many doing these kinds of things (e.g., http://opencog.org/ https://github.com/brohrer/becca and others that I don't know off the top of my head). I love this kind of full-scale attempt at developmental stuff, but it's not an easy ""got it all done"" kind of thing. Lots of _really smart_ people work on Watson, Siri, Cortana, Google, self-driving cars, those robots that fell all over the place at the DARPA Robotics Challenge last weekend, and other things, too.",1434132758.0,cs4h28a,t1_cs4b5ew,artificial,t5_2qhfb,2015
13,maeon3,"Go to amazon, buy a GoPiGo.  A Raspberri Pi 2, the servo motor, the video sensor, ultrasonic sensor, wifi card and create a program on it that follows the wall or does localization.

Few weeks for getting the parts.  Few hours to put it together, few weeks to fully understand the OS and libraries and move the motors around.  Month or two to implement a program to take in sensor data, do something grand, and do motor output.

https://www.raspberrypi.org/tag/arduino/

To  not get bogged down in the last step, take an AI class while you are at it.  If your math is rusty it's going to kick your ass.   

Then program the self driving car.  https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373

Or... Don't do any of this and keep ""reading reddit"" and looking for lowest hanging fruit rather than making a plan for the high hanging fruit.

Get in the game and start building.  https://www.raspberrypi.org/back-to-the-future-played-by-disk-drives/",1434557934.0,cs9ojub,t3_3a5cn4,artificial,t5_2qhfb,2015
14,maroonblazer,"Programmer's won't be making those kinds of decisions when writing code for self-driving cars. Rules and parameters for how the vehicle should respond in varying situations where the variables involved are things like speed differential, environmental factors, distance between vehicles, etc. i.e. there won't be logic along the lines of:
    if numPassengers &gt; 5
    then swerve()

",1435675118.0,csnk0y2,t1_csnhnf1,artificial,t5_2qhfb,2015
15,physics44,"Its not like there is a section of code that says

If (kid in road) and (old person in road) and (cant avoid){

Hit the old guy

}

I dont think a self driving car would be able to recognize a situation in which a crash was unavoidable because of all the variables the car can't account for (maybe the people will jump out of the way, maybe the kids parents will save him, ect.). Honestly I doubt the car would waste time guessing how old the people are.
I would guess that the self driving car would probably be programmed to always look for the best way to avoid all collisions.",1435675886.0,csnkhcw,t3_3bmg4q,artificial,t5_2qhfb,2015
16,Don_Patrick,"The government makes the traffic rules, right? I fully expect them to tell the programmers what rules to stick to, and that is how the public has influence on the matter. By whatever methods the programmers decide, if a self-driving car passes a driver's exam without killing anyone, it will have met all the requirements that are expected of human drivers. The rest are called accidents. If someone gets run over by an automated tram, do we blame the tram?

Personally I don't think programmers will even be distinguishing at age level: Everything is a target to avoid, preferably by driving slow in target-heavy areas. In fact there are probably a few laws around against age discrimination and such.",1435680407.0,csnnbsf,t1_csnhnf1,artificial,t5_2qhfb,2015
17,abaybas,"I think people in /r/artificial are taking OP's question as an attack on self driving cars, rather than a philosophical curiosity.

Questions like these are being used in the press to create controversy, while statistics that objectively show how many lives will be saved by self driving cars are swept under the rug.

I feel bad for OP but I also understand people's reaction.

",1435691537.0,csnuuxf,t1_csnnlss,artificial,t5_2qhfb,2015
18,Don_Patrick,"Yes, that would be the point in programming them. That's what programming is: An exact and consistent procedure. Look, here's how it's going to play out in practice: The government is going to say ""We'll allow your self-driving cars on the road if they cause no more than 1 death per 1000 simulations"", and to reach that bar the programmers will consult safety experts on how to make their car safer than their competitors. The government isn't going to say ""no more than two elderly and one child death in 1000 simulations, unless they're foreigners or registered criminals"", so the programmer's aren't going to look into such details. On top of that, take it from a programmer that that sort of analysis would add a tremendous level of difficulty to the programming, and since every second counts we don't want the computer to spend that kind of time on image recognition and age extrapolation.",1435691888.0,csnv3el,t1_csnruok,artificial,t5_2qhfb,2015
19,NadirPointing,The question you pose of being forced to chose between equally 2 bad options has been solved for a long time in the realm of computer/software engineering. Whatever is easiest. Perfect is the enemy of good in this case. We should focus on the best way to reduce total casualties without regard for these truly minor details. Every day that we don't massively adopt a self-driving car system is more than 80 lives lost. Lets leave these questions for general AI and philosophy and out of the engineering realm until we get that far down into the weeds. ,1435693362.0,csnw35v,t3_3bmg4q,artificial,t5_2qhfb,2015
20,NadirPointing,Nothing being actively considered in the self-driving car industry has been able to predict survivability or age. This is why we consider this idea so off.,1435694596.0,csnwx3p,t1_csns04m,artificial,t5_2qhfb,2015
21,CyberByte,"Udacity has a free online course titled [Artificial Intelligence for Robotics - Programming a Robotic Car](https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373). The teacher, Sebastian Thrun, is a very prominent researcher in AI and self-driving cars and led projects that developed [Stanley](https://en.wikipedia.org/wiki/Stanley_(vehicle\)), [Junior](http://news.stanford.edu/news/2007/november7/jrresult-110707.html) and the [Google Self-Driving Car](https://en.wikipedia.org/wiki/Google_driverless_car). There's also a robotics course on reddit (/r/ludobots), but I have not personally tried it. You can also find many other AI-related courses on [Udacity](https://www.udacity.com/courses/all), [Coursera](https://www.coursera.org/courses?categories=cs-ai) and [EdX](https://www.edx.org/course). If you prefer reading, the typical recommendation is Russell and Norvig's [AIMA](http://aima.cs.berkeley.edu/).

&gt; Ideally, I'd like to be on the side of the automater rather than the automated. What path should I follow to lead myself in that direction?

I'll assume you want to go in some kind of AI-related direction and not become a plumber or something. If you're designing new systems and algorithms, you probably have little to worry about. If you're a data mining consultant or something, then advances in the field will often make the process easier (e.g. by reducing the need for manual feature selection), which reduces the time that a client needs a consultant to do the same thing (although it might also grow the market). Just look out for things like that. 

&gt; How do I future proof myself and gain skills and expertise that can allow me to build highly capable AI?

Get appropriate education. 

&gt; Do you think an MS in machine learning or AI is necessary or at least highly beneficial towards that goal?

I think it would be difficult to get a job in AI without some kind of graduate degree, but it's probably not strictly impossible. Other graduate degrees might also work (e.g. in math, computer science or cognitive science), but I'd definitely say a degree in AI or ML would be highly beneficial.

&gt; I enjoy math but I was embarrassingly bad at advanced calculus -- mainly very bad at taking timed tests on advanced calculus -- will this be a hindrance for me? What kind of math is going to be critical to my success in AI?

Most areas of mathematics are pretty relevant to AI, although I suppose you won't have much need for geometry/trig unless you're doing computer vision or physical simulations / video games. Calculus is pretty heavily used in many machine learning algorithms, as well as statistics and probability. If the only problem is that you're not very fast, I wouldn't worry too much about it though.",1436777279.0,ct1jc24,t3_3cqccj,artificial,t5_2qhfb,2015
22,CyberByte,"There is some distinction between narrow AI / artificial narrow intelligence (ANI) and strong AI / artificial general intelligence (AGI). 

Most of the field called AI is doing ANI, which means they are developing systems that are very good in a narrow domain. A chess computer can't learn how to drive, and a self-driving car can't learn chess. ANI is not going to take over the world or anything like that, but it could replace many jobs. What will happen to the ex-employed is more of a political issue, but my personal feeling is that the increased wealth should be (at least partially) redistributed so that people can live comfortably without having a paying job. If an ANI kills a person (e.g. a self-driving car), the system's owner or designer could perhaps be sued for damages. ANI wouldn't have any rights for the same reason that your toaster doesn't. I don't see any reason why we couldn't develop some ANI technology to improve RealDolls (and sex bots are already being developed), but I think it wouldn't be as good as the real thing, so human prostitutes would still exist. I don't think you could have a meaningful relationship with an ANI.

AGI is probably what most laymen think of when they hear ""AI"", although it is not necessarily very humanlike. My current thinking is that this would be very dangerous. Availability of more hardware and knowledge, duplication and (recursive) self-improvement could quickly let a system with human-level intelligence grow into one that is unfathomably superhuman. It would be extremely tricky to prevent such a smart system from realizing that it won't need humans in the long run. Anything we don't explicitly program as a terminal goal for the AGI might be violated. We don't really know what we (humanity) want and if we want to say something like ""don't kill humans"", we need a classifier to say what is and isn't a human, and this can be corrupted/changed. For almost any terminal goal, the AGI has an incentive to survive, self-improve and gather resources. Depending on its level of intelligence, we might be a threat, competitor or irrelevant (initially we might be partners, but the AGI would outgrow that stage). I don't know how it would wipe us out if it feels we are a threat or competitor, but we wouldn't really stand a chance (none of that Terminator bullshit). If we are simply irrelevant, we might die as some side effect of some other action (maybe the AI prefers air without oxygen for some reason). If we're a resource, we would be harvested in some way.

If I'm wrong (or in the period before we all die) we can talk about point 2-5. Pretty much all jobs could eventually be replaced by superhuman-level AGI, but we might prefer some things to be done by humans for nostalgic reasons or whatever (just like some people prefer imperfect hand-crafted stuff to cheaper ""perfect"" factory-made alternatives). I don't know what would happen, but I'd like to think people would be able to do pretty much whatever they wanted. There is an extremely wide range of AGI systems that are possible, and I don't know if they should all get personhood rights, but humanlike AGI probably should. Person AGI would be held accountable for its own actions (e.g. killing someone), but other AGI would probably follow ANI rules. AGI doesn't necessarily mean humanlike AGI and it also doesn't mean we would instantly have the technology to build a sufficiently humanlike body to completely replace human prostitutes. But as the technology gets better, demand will probably decrease and relationships with AGI will become less taboo.",1437246504.0,ct7x31s,t3_3dk0gn,artificial,t5_2qhfb,2015
23,Lance_lake,"&gt; there can be no goal-directed behavior without motivation/emotions.

Would you say driving a car from point A to B a goal directed behavior? Does the self driving car have emotions?",1437369185.0,ct9elr9,t1_ct8xqod,artificial,t5_2qhfb,2015
24,arachnivore,"He's being down voted because he's being very rude for no reason and adding nothing to the conversation. 

Also, AI doesn't have to mimic what humans can do to replace them in the work force. Self driving cars will cause millions of jobs to evaporate. 3D printed buildings will cause millions more to disappear. Automated fast-food, translation, and many others are all on the way. Most jobs only require a small subset of human capabilities.

https://www.youtube.com/watch?v=7Pq-S557XQU",1437793430.0,ctfciti,t1_ctez81b,artificial,t5_2qhfb,2015
25,CyberByte,"Are you talking about narrow AI (short/middle/long term) or general AI (long - very long term)? If you don't know the difference: general/strong AI is the creation of machines that are intelligent across a broad range of domains and could learn (almost) arbitrary new ones (like humans), and narrow/weak AI is the creation of specialized algorithms that do things that are sort of smart but only in a fairly narrow domain. Most researchers are working on the latter and it is an extremely successful field that the economy has already benefited greatly from. Things like data mining/business intelligence, credit card fraud detection, book/movie/etc. recommendation algorithms, route planners, factory robots, search engines and speech recognition are only a few of the many many [applications of AI](https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence). In the near future you can expect to add self-driving cars to this list, as well as autonomous delivery drones (although even drones that aren't autonomous typically have some AI in them). AI algorithms also help in various kinds of simulations, and can help doctors diagnose and treat your diseases. (Some of these things are arguably statistics / computer science, but they often fly under the banner of AI.)

One of the main things that AI does is take things that could previously only be done by humans, and make a machine to do it. This means that it can displace a lot of jobs. Some people believe that new jobs will be created to fill the void, but I'm very skeptical that this will go on for much longer. This means that in the short to mid term, a lot less human labor will be required. Whether this is beneficial to the economy probably depends in large part on how a particular society handles this. There will either be mass unemployment and poverty, or the new found wealth will be distributed in some manner and people (on average) will just have more free time and cheaper stuff. 

The invention of general AI would likely completely transform the economy. How exactly is very hard to predict, and it depends a lot on what form this AI would take. Some people think it would mean the end of humanity, so I suppose that would be bad for the economy. Others think the AIs would take over pretty much all of the work from us, and we'd live in a (mostly) post-scarcity society. Maybe we'd become essentially immortal, which is a whole other can of worms. Or maybe we can't make these AIs our slaves and let them do all of the work for us, in which case I suppose we'd have to live side by side with a race of beings that gradually becomes more superior to us. I think the economist Robin Hanson has written quite a bit about possible futures with AGI. ",1438303491.0,ctm8gup,t3_3f882i,artificial,t5_2qhfb,2015
26,ComradeGnull,"&gt;Those developing AI need to avoid being cavalier, he said. And – as he has often said previously – he said the general public needs to be aware that AI is developing faster than many believe.

&gt;“Just because somebody hasn’t experienced or isn’t deeply familiar with an advanced technology doesn’t mean it doesn’t exist,” he said.

&gt;One of the best ways to prevent artificial intelligence from harming humans might be to shape the concept of AI in such a way that harm seems antithetical to the definition of the technology, University of California - Berkeley computer science professor Stuart Russell suggested.

&gt;Russell, who was on the panel, gave bridges as an example: The idea that a bridge is supposed to be structurally sound is part of the concept of a bridge.

This is one of the reasons why I think the Google self-driving car is so key to the eventual use of AI in human-facing roles. The Google car is building a giant database and recognition network that includes specifically recognizing human beings and prioritizing not causing them harm- it's inherent to the problem of making the car safe to use in the environments where it is being tested. Imagine what you could do by applying that to other areas- factory machines that automatically switch off when a human puts themself in a dangerous position.

Top people in AI research (Peter Norvig, for one) have said that it's really data more than algorithms that make AI interesting and capable. Ironically, it's putting AI in positions where human safety should be it's top priority (vehicles, heavy machinery) that produces a training data set that makes harm to humans less likely... in other words, if we want to create AI's that don't harm people, we have to start by putting some trust in them and making them responsible for human safety in real-world situations.

You can also understand Musk's opposition to using AI with weapon's systems in that context- a sophisticated recognize-pull-the-trigger  system trained on a lot of examples is essentially going to enable you to weaponize any other device that has an AI safety system unless you build in substantial safeguards for that specific scenario.",1438803958.0,ctsovxa,t3_3fvgpa,artificial,t5_2qhfb,2015
27,Haf-to-pee,"Very soon we will have robots, intelligent drones, self-driving cars, etc., and these self-mobile devices will be our personal assistants. ",1441092307.0,cumt1j9,t1_cumsgyv,artificial,t5_2qhfb,2015
28,CyberByte,"A lot of work in AI is basically developing applications to automatically solve some problem. The ""main question"" is then usually something like *""How can we efficiently/satisfactorily solve problem X automatically?""*. Subquestions can then be things like *""What are the requirements exactly?""*, *""What previous work has been done that we can reuse/adapt?""*, *""Will algorithm Y (or some adaptation) yield good results?""*, *""Why (not)?""*, and *""Which of these algorithms works best?""*. You can then do ""experiments"" where you test the hypothesis that algorithm Y solves your problem. In your case ""X"" could be the travelling salesman problem (TSP) and ""Y"" ant colony optimization (ACO). You'll probably want to use pre-made software and try some different settings and/or extensions of ACO and compare them a bit. 

You can also do higher level questions like ""What rights should strong AI have?"", ""Will it kill us all or create a utopia (or neither/both)?"", or in the shorter term ""Should we develop autonomous weapon systems?"", ""What are the (solutions to) socioeconomic consequences of more automation?"", ""Who should be liable in a self-driving car accident?"", etc. For most of these making an experiment is a bit more difficult, although you could always make a prediction about public opinion and then poll some people. 

Just ask your teacher if what you're planning to do conforms enough to the assignment, but you should do that anyway. Good luck!",1441728110.0,cuuiril,t3_3jvomo,artificial,t5_2qhfb,2015
29,pistonhonda,"Most AI research and development is not aimed at creating a single, unified, intelligent entity. Rather, we find lots of tough problems that require some amount of reasoning to solve, approach the problem with a toolbox of automated reasoning techniques, and adapt our tools to fit the problem. This is called the intelligent agent approach and is the current paradigm of our field. For some context, [read this article on Wikipedia](https://en.wikipedia.org/wiki/Artificial_intelligence#Approaches).

You interact with automated intelligent agents every day, like Google's image search system or Amazon's product recommendation system. They just are not intended to be a single entity, but rather many different intelligent components that are good at solving smaller problems. 

Edit: it is not a general AI, nor was it ever intended to be, but one of the crowning achievements of the field is [Google's self-driving car](http://www.google.com/selfdrivingcar/). ",1443711766.0,cvkje88,t3_3n3hrt,artificial,t5_2qhfb,2015
30,nkorslund,"I doubt it's just for the press though, they probably also want to stay on top of the self-driving car trend as it arises.",1444044989.0,cvoszwn,t1_cvoexz3,artificial,t5_2qhfb,2015
31,dczx,"Gotcha, so this was your first assumption, the goal of overall AI research. 

Some projects seeks to do that. Most of them do not. 
Most of them are actually how to accomplish a specific task efficiently.  Why? Well, for the same reason most home builders build homes for personal use, for the same reason that when you buy honey, it comes in personal use.  Not that we don't want homes the size of castles, or that we don't want 100000gallon honey jars. But the scale and scope is borderline impratical at this moment.  We're working on it still.

Assuming we got enough of the algorithms close enough, the operations per second necessary are on the scale of a present day supercomputer (if even, might need a quantum).  We could get into interesting decetnralized architecture, but I'm just saying the sheer floating point operations is bigger than most programmers have access to. 


So supervised AI is definitely a field, but I'm not talking about a childlike mind yet, I'm still talking about task oriented. That's still some of the most advanced stuff. Speech recognition, or self driving car, etc. 

Even when it comes to language, you might be looking at markov chains that are collected from a large speech set, then pruned by interacting with people.  
*Most language AI does not understand what it is saying, it is able to given appropriate responses, but it doesn't make judgements on the content of the individual words. It just restrings together words and sentences.  For some fun, check out reddit.com/r/subredditsimulator    It's all generated by AI, and even the comments must be AI generated. 
So back to your ""why would it produce unlimited paperclips"" It just wasn't programmed to think about that. 

We're still building the basic senses, but it will be amazing when an AI, starts stringing these together. 
Hopefully.
Bye.",1444429933.0,cvu8afk,t1_cvtzdr3,artificial,t5_2qhfb,2015
32,maeon3,"Go to college for Computer Engineering:

https://en.wikipedia.org/wiki/Computer_engineering

https://en.wikipedia.org/wiki/List_of_engineering_schools_in_Massachusetts

You won't doing any ""brain stuff"" on computer engineering though.  All devices are Turing Machines, which means they can all compute anything that can be computed.  The magic and spirit of the brain will be in software.  So if you want to get into the brain stuff, do computer science with focus on Artificial intelligence, or go straight to the source with neuroscience, mapping out engineering diagrams for how the brain works down to the level of individual neurons, and how they all work together to solve problems.

Sebastian will give you his code for the self driving car in 8803: http://www.omscs.gatech.edu/",1445028022.0,cw29m6x,t1_cw1j0l9,artificial,t5_2qhfb,2015
33,trep_life,"Respectfully, this question pushes my buttons.

First, in the Genealogy of Morals, Nietzsche convinced me that morality is a mode of oppression, so I ask: who defines the morality that rejects AI, and what do they gain from that definition?

Second, if AGI, say, cures all cancers, was it immoral?

Third, getting to one of your points, what are the moral dimensions of software anyway? They exist, but are typically very utilitarian: if software automates a job away, should we keep it around just so the humans have something to do (poorly and expensively) to occupy their time? This, I think, is a legit question we're going to have to face soon with, eg, as you say, self-driving cars vs. the transportation industry. If we decide not to cost-optimize that is a pretty big structural disadvantage we're building into economies -- the game theory there is bad, because the first society to automate will economically crush those who don't.

My stab at an answer, in short: /r/basicincome. Generally speaking people are happier when they leave jobs they can't be effective at. We need to make that a non-lethal decision.

Four: in my more romantic moments I do struggle (slightly) with simulating pain and death, as you say. However my areas of interest are genetic programming and self-modification: genetic algorithms don't feel pain -- there's no need, they're just strings of random numbers that either promote to a new pool or stop existing. Self-modification is more about identifying areas of lack and correcting them, again: no pain.

Last, evolution is amoral: what if humanity is a chrysalis? What moral rights does the cocoon have over the butterfly?",1445174279.0,cw3x8q5,t3_3p7jyy,artificial,t5_2qhfb,2015
34,tom_qq,"&gt;How we humans learn is from emotions, if we were to do something and feel pain then it would have a very powerful effect on our memory, teaching us not to repeat the same mistakes

We learn in various ways. This includes pain and pleasure, prediction (and failed predictions), as well as detection of mere statistical regularities. 

&gt;Is it within our moral values to programme a machine to feel pain for our own benefit?

Whether a scalar reinforcement learning error signal constitutes ""pain"" at all is a pretty difficult philosophical question... But we could also ask whether it is moral to do this to other human beings: sure, some of our pleasure/pain is purely biological, but we also teach children what to value. Teach a child that he needs to be the best, and he will suffer if he comes second. In this respect, as in many others, the ethical problems associated with A.I. are no different from those associated with having children and raising them.

&gt;Another take on this is that let's say if we managed to develop self driving cars, sure this would make our everyday lives easier but it would also put a lot of taxi drivers out of business.

That's a better question, but I'm afraid you won't get a good answer to it because (a) answering this requires an understanding of economics, not of A.I. and (b) you asked this question in a disorderly post filled with random thoughts. For my part, I do not have an answer - I am not an economist, and I cannot predict the effects of large-scale mechanization on our societies. The problem does worry me.",1445194471.0,cw48o0r,t3_3p7jyy,artificial,t5_2qhfb,2015
35,CyberByte,"I think that ethics of anything are almost always discussed negatively in terms of what is unethical. Nobody is going to write an article saying ""OMG, it is so ethical that we're doing cancer research; we should totally continue doing that"", unless maybe funding is getting cut, in which case the article is probably going to talk about how it's unethical to cut funding.

To defend AI we can look at all of the positive applications that it has brought us and that it promises to deliver in the future. It looks like AI could make the roads safer at some point. Stopping the development of self-driving cars, will likely result in someone dying who would not have died otherwise. And there are tons of other applications. Diagnostic expert systems and classifiers, robots for search and rescue, bomb diffusal and surgery, predictors of crime and the weather, etc. And some things may not directly or obviously save lives, but they make our lives easier and more productive, like search engines, route planners, recommender systems, household robots, (credit card) fraud detection, etc. And these are just things that can be accomplished with narrow AI. If we ever develop strong AI, there's no telling what we will be able to accomplish (if, you know, it doesn't kill everybody). ",1445514795.0,cw8q81s,t3_3p94ot,artificial,t5_2qhfb,2015
36,bkelly1984,"Wait a minute, the legality of a self-driving car to be on public roads is questionable in most cases and [illegal in a few](https://en.wikipedia.org/wiki/Autonomous_car#Policy_implications).  Has Tesla skirted this problem by stating that Autopilot is a driving aid and not an autonomous vehicle?",1446331056.0,cwjvq6m,t3_3qy2if,artificial,t5_2qhfb,2015
37,rpad,"A good driver is aware of its surroundings, and that car standing there definitely tells that it is ""up-to-something"". Thus a driver would have anticipated the situation, slowed down to let the other car do the turn, and there would have been no need for the emergency breaking. More so since it is signalling its intention to turn, and there is a break in the column on the right, so its visible that there is a sideroad. And the cars in the other column already have the intention to let him pass. Also the cars ahead in the lane are relatively far away, but there's a line of them too, so the driver would have not missed any opportunity himself. He could just have let the other car pass. 

I wonder when will self-driving cars be able to assess the situation this way?",1446474434.0,cwllap5,t1_cwlahal,artificial,t5_2qhfb,2015
38,Dunder_Chingis,"How are they supposed to steal a self driving car? ""Hey car, drive to the chop shop.""

[The Car](http://media.giphy.com/media/7ipzyOWn237dm/giphy.gif)",1446912218.0,cwrv96y,t1_cwrbnvo,artificial,t5_2qhfb,2015
39,sixwings,"From the article:

&gt;Drivers, for example, can pretty much get behind the wheel of a car and drive it wherever it may be, he said. Autonomous vehicles use GPS and laser imaging sensors to figure out where they are by matching data against a complex map that goes beyond simple roads and includes details down to lane markings.

&gt;The cars rely on all that data to drive, so they quickly hit problems in areas that haven't been mapped in advance.

&gt;So when you see a lot of demonstrations of autonomous vehicles, it is very easy to be fooled into thinking the car is more intelligent than it is when really the car is using all of this superhuman information to accomplish its goal looking like it has the intelligence of a person, but it really doesn’t,"" he said.

&gt;A truly intelligent self-driving car needs artificial intelligence that can figure out where it is even if it has no map or GPS, and manage to navigate highways and follow routes even if there are diversions or changing in lane markings, he said.",1446929895.0,cws559j,t3_3rxgc3,artificial,t5_2qhfb,2015
40,Timothyjoh,"We are talking about a point in time where robots are closer to ""sentient"" and are certainly more autonomous than they are now. Right now, we have the most advanced AIs parsing human knowledge and making medical recommendations, probably without the full understanding of what ""harm"" is, but they have learned the difference between positive and negative outcomes.

It's not hard to imagine then, even in the near future when an intelligence like this is embedded in a mobilized body that can move autonomously, that it will have been ""trained"" with a variety of safety precautions. Just think of all the certifications that toys and consumer electronics currently have to go through before regulation passes them and they can ""go to market"". This is why Google's self-driving car (not even close to intelligence like Watson, or sentience) is going to take another 5 or so years to come to market. Current regulatory bodies will ensure that the first ""law"" is met to the most rigorous standards. 

Not that I think that the author was an oracle of future laws verbatim, but his laws worked into a very real train of thought and made for a good story.",1447007325.0,cwt2knr,t1_cws314u,artificial,t5_2qhfb,2015
41,sodermalm,"If you want to see it in action, I've just uploaded my self-driving car simulator: http://www.habrador.com/labs/self-driving-car/",1447167728.0,cwvce62,t3_3s6mo8,artificial,t5_2qhfb,2015
42,singham,"Please also discuss the loyalty of algorithms e.g. let's say a self driving car is moving toward a pedestrian. It has identified two options : either kill the driver or kill the pedestrian. Everything being equal, what choice should the algo make? Does it have more loyalty towards the owner of the car?",1447655333.0,cx1u4o0,t3_3sw8o8,artificial,t5_2qhfb,2015
43,C0demunkee,"1) ... I meant put the machines in charge of the resources. Once that happens, high-powered special interests wont be so high-powered.

2) yup, fuck those people. Google's self-driving cars keep getting rear-ended. Zombies are never the primary problem in zombie films.

3) Easily is irrelevant, also underground works. Simply pointing out that our extinction isn't mandatory. This town is indeed big enough for the both of us.

4) Why not? we've never really been in charge of our destiny, being social creatures isolated on a rock floating through space. Humans will probably end up living pretty much like 1st worlders do now, only a bit better taken care of.
",1448897789.0,cxhymq6,t1_cxhy6l0,artificial,t5_2qhfb,2015
44,sixwings,"Quote from the article:

&gt;If self-driving vehicles are poised to revolutionize transportation, they will transform mobility at least as much as trains and airplanes before them. Only this time, success will mean altering the role of humans in an unprecedented way—removing them from the driver’s seat altogether. This change is what imbues the larger conversation about self-driving cars with an existential quality. “Some of these questions go right to the heart of artificial intelligence,” Leonard said. “I don’t want to say ‘never,’ but it’s not a foregone conclusion that these things will be ubiquitous.”

&gt;“But there’s a chance,” said Urmson, at Google. “All these wonderful impacts it can have in the world.” Today, it would be foolish not to be skeptical of self-driving cars and the utopia their devotees promise, but it would be inhuman not to try to build them anyway.",1449093027.0,cxkw328,t3_3v76c6,artificial,t5_2qhfb,2015
45,eygrr,"I predict a plethora of problems will come from people using self-driving cars, but not because of the self-driving cars. People just aren't good at living, even with machine-aid. The only thing we can hope is that it will result in *less* stupid problems, not that it will avoid creating them.",1449135199.0,cxli6o7,t3_3v76c6,artificial,t5_2qhfb,2015
46,sixwings,"If all cars were self-driving, they would prevent 40,000 fatalities every year on US roads alone. Any other problem that might arise pales in comparison.",1449160791.0,cxlseoi,t1_cxli6o7,artificial,t5_2qhfb,2015
47,tom_qq,"1.2 million a year worldwide. Furthermore, those who die are typically in their early 20s, meaning the economic cost of these deaths is maximal.

Though obviously self-driving cars will take longer to be deployed in e.g. east and south-east Asia.",1449606868.0,cxry6mx,t1_cxlseoi,artificial,t5_2qhfb,2015
48,georgeavazzy,"&gt; A few days before Thanksgiving, George Hotz, a 26-year-old hacker, invites me to his house in San Francisco to check out a project he’s been working on. He says it’s a self-driving car that he had built in about a month. The claim seems absurd. But when I turn up that morning, in his garage there’s a white 2016 Acura ILX outfitted with a laser-based radar (lidar) system on the roof and a camera mounted near the rearview mirror.

I don't doubt that Hotz is incredibly smart. But the narrative in the article implies that he built a self-driving car in a month using only 10 hours of driving data. He is comparing his technology to Mobileeye's system and calling himself the ""next billionaire CEO"".

I find this feat incredibly unlikely in such a short period of time.

1: I don't believe the **quantity of data** is sufficient to train the car. Supposing that the AI can only train when the driver performs an action (i.e. not when the car is merely moving), and that the driver performs about 10 actions / minute, that would be roughly 10 actions / minute * 600 minutes = **6,000 actions** that he would have collected during those 10 hours of data. Compare this to the **2,448,873 images** used in the Places205 dataset from MIT, each with multiple labels.

2: consider the time that would be required merely to connect the computer to the car. He would have to buy all of the equipment, wait for it to ship, interface with this car, place electrical converter, etc., then mount the monitor, and then spend the time to interface with the car. For mere mortals, this would require a month *at least*!

3: we don't really see the car in action. We only see it drive on the interstate, and NOT doing basic tasks like changing lanes, turning, avoiding collisions, reading signs, or *anything else that you would want a self-driving car to do*. In other words, he built cruise control that stays within the lane. Which is great, but I don't think that that warrants the title ""self-driving"" car.

4: I don't believe the **quality of data** is sufficient to train an AI. Existing systems are able to read road signs, speed signs, stop signs, and traffic lights. Since he opposes putting in rules like ""stay within the center of the lane"" (which he claims is unnecessary), this would have to be automatically determined by the AI. 

I think, at the very least, he accomplished a pretty awesome feat. But I think that it's unlikely anywhere near current technology, which he claims to have superseded.",1450307388.0,cy1l6ia,t3_3x53ms,artificial,t5_2qhfb,2015
49,abrowne2,The amount of variables involved in driving means that I doubt the viability of self driving car systems. I doubt we will see one driving in the streets of a major city anytime soon.,1450313291.0,cy1oo48,t3_3x53ms,artificial,t5_2qhfb,2015
50,bradfordmaster,"I'm on mobile and can't read the article right now (I know, I'm the worst), but it could be possible to get a rough prototype working in a month using existing code. 

Driving only on highways, for example, isn't ""that hard"" now, and if you use something line ROS, you get lots of stuff for free, including drivers for your sensors, and probably a bunch of mapping and odometry code. 

So, not knowing the guy (although interestingly enough, my college roommate was a high school friend of his), or the approach he used, I'd say it's possible to hack together a demo that got could claim was a self driving car, but not to make something robust or practical, and not to solve even remotely the same problem the big guys are trying to solve. ",1450335484.0,cy1zxkd,t3_3x53ms,artificial,t5_2qhfb,2015
51,sodermalm,"I think Tesla gave a good reply:

&gt; getting a machine learning system to be 99% correct is relatively easy, but getting it to be 99.9999% correct, which is where it ultimately needs to be, is vastly more difficult. One can see this with the annual machine vision competitions, where the computer will properly identify something as a dog more than 99% of the time, but might occasionally call it a potted plant. Making such mistakes at 70 mph would be highly problematic.

https://www.teslamotors.com/support/correction-article-first-person-hack-iphone-built-self-driving-car",1450350090.0,cy23qck,t3_3x53ms,artificial,t5_2qhfb,2015
52,Akyu,"The self driving car part seems legit, but the timeframe is pretty clearly just salesmanship. It's obvious that this guy is incredibly talented, but projecting the wünderkind image is very useful for generating hype. ",1450374557.0,cy2fe6z,t3_3x53ms,artificial,t5_2qhfb,2015
53,xkcd_transcriber,"[Image](http://imgs.xkcd.com/comics/the_three_laws_of_robotics.png)

**Title:** The Three Laws of Robotics

**Title-text:** In ordering #5, self-driving cars will happily drive you around, but if you tell them to drive to a car dealership, they just lock the doors and politely ask how long humans take to starve to death.

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php/1613#Explanation)

**Stats:** This comic has been referenced 29 times, representing 0.0312% of referenced xkcds.

---
^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cy85764)",1450815754.0,cy85764,t1_cy8572e,artificial,t5_2qhfb,2015
54,shivinski,"By AI, you have to realise that there's a huge amount of surrounding topics that need to be learnt to fully ""do"" AI. That's why most university courses do AI as a speciality in the field of maths or computer science (my old university for example did: Computer Science with AI). So you'll most likely be doing a Comp. Sci. degree but with various maths modules in AI and Machine Learning techniques.

As for job opportunities, there are plenty, and it's only going to be a growing market. Jobs range anything from engineering and real-world AI in machines, self driving cars etc. - to more data science type applications in retail, finance and basically most other areas where technology and data are involved. However a note of warning, just because you have a degree in AI won't guarantee a top spec job. It's a hugely competitive field, and a lot of decent AI jobs want to see at least a PhD in AI, since it's such an evolving topic, a PhD will at least demonstrate a thorough understanding of the field of AI and the applications of such.",1451158023.0,cyc0f0d,t3_3yb56c,artificial,t5_2qhfb,2015
55,AlanCrowe,"&gt; The core insight is that IQ is a continuum.

That is the core of the controversy. Human IQ is on a continuum. This seems to be partly about health. Both illness and malnutrition cause physical stunting and stupidity. Perhaps [this more sophisticated view](http://lesswrong.com/lw/rl/the_psychological_unity_of_humankind/) is more important. It traces the psychological unity of mankind to the genetics of sexual reproduction, which greatly limits the scope for different, incomparable kinds of intelligence. 

But is machine IQ on a continuum? Can you guess how safe your self-driving car is on the road by looking at its chess ranking? Can you guess your AI's chess ranking by looking at its skill in symbolic integration? Can you guess your program's skill in symbolic integration by trying to whistle the tunes it has composed?

Very clever humans tend to be multi-talented and have hidden depths. Human intelligence is on a continuum and they have more. Yet when they put finger to keyboard and write an AI program they don't pass on this continuum property. Today's really clever AI programs are single-talented and have hidden shallows. Today's AI programs are not the kinds of programs that could explode in intelligence.

When will humans start writing the kinds of AI programs that could explode in intelligence? People have already tried and failed very hard. I think it is that history of failure that shapes current approaches. For example, if you want to write a program that does first order logic you could try to emulate human development and write a program that thinks about first order logic in a self-aware, potentially explodey style.  When that approach proves utterly beyond you, you buy a copy of [The mad scientists guide to building giant electronic brains](http://www.cambridge.org/us/academic/subjects/computer-science/programming-languages-and-applied-logic/handbook-practical-logic-and-automated-reasoning?format=HB) and learn how to mechanise Herbrand's theorem. That is a totally different style of coding.

That style creates a program that is very clever in some ways and very stupid in other ways. In particular, it has nothing to do with the kind of human cunning that went into its creation. If there is any kind of self-improvement loop, the human programmer plays an essential role in it. 

There is no danger of humans granting computers the power of autonomous self-improvement. We are nowhere near knowing how to do that.",1451211188.0,cycmwhp,t1_cycig33,artificial,t5_2qhfb,2015
56,maroonblazer,"&gt;Musk, Gates and so on they all say that artificial intelligence is going to explode soon 

Soon? 

I've not heard Musk, Gates or the other notables (Bostrom, e.g.) suggest it's going to happen anytime soon. More that it seems very probable to happen at some point and, if not controlled, could pose an existential risk.

As others have pointed out, just look around. Everything from search engines, GPS navigation to self-driving cars and Watson all point to a trend where machines are becoming increasingly intelligent. It's not too difficult to imagine AGI getting off the ground at some point.",1451234825.0,cyctk2v,t3_3ycw1x,artificial,t5_2qhfb,2015
57,sodermalm,"I'm learning how to simulate neurons to understand why it's so difficult to simulate that small worm C-elegans. So i'm reading this book: http://neuronaldynamics.epfl.ch/online/index.html

I've also made a blog-article recommender system that I'm going to improve and maybe make public http://www.habrador.com/labs/august-shield/

I'm also making a self-driving car in Unity http://www.habrador.com/labs/self-driving-car/",1451320717.0,cydwhvr,t3_3ygx46,artificial,t5_2qhfb,2015
58,rfinger1337,"Is it sentient and intelligent to the level of a human?  

Since google search is our best example of AI right now, the answer is clear.  But what if a nural network (my brain for example) gets mapped to a computer and uploaded into an android?  Would I retain my rights as a human?

Will a self-driving car be responsible for it's actions, but also have a right to not work on the weekend?

The united automobile association has filed a grievance against you for driving to florida without the requsite number of stops.  

:-D",1451396685.0,cyexczn,t3_3ynb1f,artificial,t5_2qhfb,2015
0,DeskJob,"Well /r/artificial tends to post articles geared towards the pop-science crowd mostly with ""AI will take our jobs""-stuff. I tend to browse /r/machinelearning for articles covering the latest methods and papers as this is what I do for a living. Unfortunately over the past few years the ML sub-reddit is becoming more pop-sciencey as well peppered with ""How do I get into AI/ML/Neural Nets (hint: either go to grad school or read books on the subject and code your own ML projects)""

(edit: one more thing to say) Also the way the term AI is tossed around like it's some monolithic magical thing seems odd to me. Google's Alpha Go is a mix of Monte Carlo tree search with deep neural nets to evaluate board states, i.e. a program built and trained to play Go and potentially other difficult to brute-force search games like Settlers or Backgammon. What popular media does is lump this mix of algorithms together calling it AI and then assumes it can magically be morphed into self-driving cars and job killing robots. No, the self-driving car is its own bag of algorithms carefully trained and tuned by lots of people to safely drive a vehicle. On the outside it might seem amazing, but really it's sweat, good research and better algorithms, but none of this is that so called Artificial General Intelligence.",1458185285.0,d12v2ed,t1_d12ttxs,artificial,t5_2qhfb,2016
1,togame27,"Why not simply search for papers by the teams leading beneficial AI work? As an example, you could find the research team leading the self driving car project and look for publications by them. ",1461429313.0,d2ee5nb,t3_4g44l9,artificial,t5_2qhfb,2016
2,DaddyRocka,"Thank you, I will look into the self driving cars. I have also been looking for papers on DeepMind-AlphaGO",1461429883.0,d2eeh24,t1_d2ee5nb,artificial,t5_2qhfb,2016
3,vm_linuz,"Yes, this is the crazy exponential growth period. With big companies like Google and Yahoo open sourcing powerful AI, I would expect to start seeing AI pop up everywhere. The downside being that this dramatically speeds up the job market losses. Transportation alone is around 11% of the economy, what happens when self-driving cars are commonplace? You can't just send 11% of working adults back to school... what would they study anyway? All fields will be dramatically changed by cheap AI. ",1462599511.0,d2w3crj,t1_d2vz7sm,artificial,t5_2qhfb,2016
4,yoda133113,"Note: [We're already doing this in some places via platooning](http://arstechnica.com/cars/2016/04/europe-completes-a-demonstration-of-semi-autonomous-truck-platooning/) (taking away the need for the trucks to navigate and placing a human at the helm of the front truck to handle the unexpected.

And [Freightliner has been testing this on *specific* Nevada roads currently](http://www.bloomberg.com/news/articles/2015-05-14/daimler-s-freightliner-tests-self-driving-truck-in-nevada), but it's only a level 3 solution, and ""isn't all the way there"" yet, and it doesn't work everywhere (which is the advantage of being able to program tailored solutions to individual needs).",1463516624.0,d399t4m,t3_4jsw9p,artificial,t5_2qhfb,2016
5,dm_fucking_t,"my guess is that is what they are planning. If the driver has to be there anyway, its no big deal for him to cover the first and last miles, and even the pulling into weigh stations and getting gas. This is both very achievable and provides a lot of value so I see the guys point for sure. It also makes sense to me that even though the potential is there for trucking, it was wise to not try to enter the market too soon, when there was less public trust about self driving cars. If its the first time people have heard of self driving vehicles and were talking about huge 18 wheelers, people would push back on that idea.",1463536519.0,d39oc43,t1_d39n1s5,artificial,t5_2qhfb,2016
6,CyberByte,"I've seen some projects where they wanted to use computer vision to tell if the driver was sleepy, distracted, agitated, stressed, and maybe some other mental states. A system like that could warn the driver and e.g. keep them awake. You could couple this with e.g. smart music selection that is neither too boring nor too distracting. You can also use AI to select music for you for other reasons, but that's not really exclusive to being in a car.

Aside from (partially) self-driving, it can also be possible to help the human driver drive. I can imagine a kind of futuristic car where the windshield is an augmented reality screen (or you wear something like Google Glass) that highlights people, animals, cars, signals, traffic signs, markings, etc. It could predict where they're going, or compare how you're driving to what it said on a traffic sign (and warn you if you're doing it wrong). It could also be a supercharged GPS where it doesn't just overlay your route on the map on a small screen, but on your actual view of the actual road (although I don't know how safe all of this is). ",1463761681.0,d3d30uk,t3_4k3s2g,artificial,t5_2qhfb,2016
7,ma2rten,Potentially self-driving cars.,1464898040.0,d3tjjdx,t1_d3t32np,artificial,t5_2qhfb,2016
8,80sKid,"I don't buy into all this '""doomsday robot taking over the world scenarios"".

Too many things have to happen in order for these scenarios to actually play out. I'm 42 and have followed tech for over 20 years. I've heard and read about so many things that we *should* have by now, but we don't. Mainly because the costs had never went down, the public wasn't on board with the idea, or businesses not wanting to invest in the tech.

We will see how it plays out, but I honestly think automation and self driving vehicles will be a slow progression. We will be old and grey by the time any of these scenarios play out, if they ever do. ",1465876257.0,d48b99y,t3_4nx7r8,artificial,t5_2qhfb,2016
9,billiebol,"&gt; We will see how it plays out, but I honestly think automation and self driving vehicles will be a slow progression. 

Let's see where we are 4 years from now with the self-driving cars. The biggest hurdle will probably be human made; regulation. I'm personally expecting, at the current rate of technological advancement, we'll have AGI/ASI by 2030 which means we can automate anything and everything then.",1465915693.0,d48scdk,t1_d48b99y,artificial,t5_2qhfb,2016
10,CreativeGPX,"I disagree and believe this is a big misunderstanding a lot of people have.

&gt; I think, if AGIs are doable, we will basically jump directly into the ASI stage. Look at the current ANIs: expert chess programs are far better than the best human players; DeepMind has beaten Lee Sedol quite authoritatively; Google's self-driving cars already have a better driving record than human drivers. I think it's fair to say that ANIs are already superhuman, or become superhuman very quickly - each in its own narrow field.

The big issue is that *superhuman at some things does NOT mean superhuman at many, all or most things*. When you look at how human-like intelligence is built and how computer-like intelligence is built, each approach will fundamentally remain bad at certain problems by design. Right now, the reason why computers are so much better at some problems and so much worse at others is because they are using a fundamentally different design (related to processing-bound memory, decentralized processing, algorithms over heuristics, etc.). As long as they use that design, they'll be better at certain things (e.g. precise, complex math) and worse at other things (e.g. creative roles like executive/design). As a person who specialized in AI and psychology in school and developed AI, I think that any AGI would have to come from the computer simulating a more organic approach (by which I mean, one that abstracts away the boolean, algorithmic nature of computers, whether or not that involves mimicking the methods acting in biological life) and therefore would drastically lose the benefit of more algorithmic approaches (i.e. ""robot-like"" precision and accuracy) and experience a high performance penalty for the overhead of simulating the rules, data and environment of the knowledge system. Basically, general, creative, imaginative, open-minded, open-ended reasoning requires a design that ditches the very rigid constructs used in ANI, therefore, it seems more likely that an AGI would initially lose a lot of the strengths we associate with computers (e.g. precision, perfection, accuracy) in order to make up for its shortcomings.

&gt; It's unclear if an AGI will coalesce from an aggregate of ANIs, or whether it will be something fundamentally different.

I think it's very clear that AGI couldn't come from an aggregate of ANIs (except in fabricated extreme examples like if the ANI was an AI designer). The defining difference between ANI and AGI is that the latter has a *generalized* way of operating than can operate on arbitrary, undefined and changing problems and observations, while the former's success comes from being tailored to observe/ignore, store, reason about, value, decide and act with in the confines of an extremely specific problem. They are two completely different and non-compatible problems. There is no natural/logical way that ANI would stitch together to form AGI or that ANI would be rephrased as AGI without breaking the intelligence that the ANI had.

Perhaps we should discuss another intermediate level, which would refer to an operator (a la Siri or Cortana) which will recognize problems and defer work to the respective ANI from a collection of many. This kind of AI will definitely feel smarter than ANI and be substantially more ""intelligent"" since it can do many things, but certainly isn't AGI since it can't recognize novel problems or invent new solutions. It would be unable to extent itself, therefore unable to grow into AGI, nevermind ASI.

I think it's more plausible that ASI emerges from AGI, but even that is not so guaranteed as people make it. From a ""software"" perspective, many designs and algorithms will have many upper (and lower) limits for where they work optimally. It's quite possible that a particular AGI relies on a design that will scale poorly to larger datasets. It's possible that certain mechanisms age poorly when the ""brain"" operates at a faster speed or over a longer timespan. It's quite possible that with insufficient senses in the beginning, AGI will have limited mental capacity. It's quite possible that with too many senses in the beginning (i.e. the ""it's connected to the internet so it has every camera, microphone, etc."" argument) it'll be too noisy of a dataset to learn from as well. Additionally, intelligence (just like many evolving, self-mutating processes) has the ability to take wrong turns and it's quite possible that an AGI may have its intelligence capped by early-formed incorrect assumptions that it's having a hard time shaking. So, there is a ton of tuning that will determine how intelligent even a great AGI design can get and will get. Then, from a hardware perspective, unless, from day 1 the AI has the ability to construct/add its own hardware, there is a cap. Since it may be a simulation of a design (or a completely different design) it makes no sense at all to say that the computational power of the computer compared to the computational power of the human brain would give any insight into the relative capacity of speed/complexity of thought. So, it's quite possible that in the beginning the hardware is a soft limit (i.e. takes some manual upgrades to get to ASI) or a hard limit (i.e. the overhead of the software on intelligence requires way more hardware than we have in order to produce an ASI). Putting those two thoughts together, it's also possible that as hardware scales up, the software was not particularly trained, designed or optimized to take advantage of more hardware, heterogeneous hardware setups or the resulting impacts like latency increases. So while it's possible that ASI would come from AGI, it's also quite possible that it wouldn't and very possible that complications occur that slow that transition for quite a while or even stop it until a very different AGI was invented that can actually scale well.",1465919114.0,d48uypv,t3_4o08zg,artificial,t5_2qhfb,2016
11,ItsAConspiracy,"Google's self-driving cars may have a good record, but they're not actually better than human drivers yet. For one thing they depend on the road being pre-mapped with high precision.",1465920240.0,d48vv4q,t3_4o08zg,artificial,t5_2qhfb,2016
12,budgie,"&gt; Men tend to like the more technical/analytical things like math, physics, engineering.

If this is in fact true than is it not actually an argument for the need to increase diversity in AI research? Is it not possible that only having a single gender, race, etc dominate the field yield products that are made in the image of that particular group? AI is a field that, unlike many others reaches far beyond it's disciplinary borders. It raises questions as fundamental as the nature of intelligence and the nature of being human. Unless the products of the research (intelligent robots, self-driving cars, Google, etc) are only meant to be used by affluent white males, wouldn't a diversity of perspectives from diverse gender, race, economic strata, etc improve this grand research program overall? Will it not have a greater impact on a greater number of people?",1466909286.0,d4o834i,t1_d4o5k5x,artificial,t5_2qhfb,2016
13,SilverTempura,"Well, this answer might not be too satisfying for you but it's really dependent how the manufacturers coded/trained the program.

With this in mind, you should already have an answer. If you were the creator of this self-driving car AI, what do you think should happen?

Some ideas:

- Maybe you know owner of Car A is often behind on his fees for the self-driving car service (how else could one program control two cars?) and the owner of Car B pays on time all the time. Sacrifice Car A.

- The owner of Car A is an important person (politician, elite, celebrity, etc) and the owner of Car B is just some nobody. Sacrifice Car B.

- The owner of Car A pays for a premium service that puts him above the owner of Car B in choosing who to sacrifice. Sacrifice Car B.

- Or if all the above fails to choose a car to sacrifice, choose a random one.",1467180066.0,d4s8r4s,t1_d4s810x,artificial,t5_2qhfb,2016
14,standingbyit,"Easy, would you be a passenger in a car that could kill you? No.

Self Self-Driving cars are a dead(ish) end. Managed roads will be eventually be the future and any possibility of collisions will be virtually eliminated  even before you get in the vehicle.",1467184489.0,d4sacuu,t3_4qdqgf,artificial,t5_2qhfb,2016
15,Screye,"Many people misunderstand how AI works. It is a machine that learns how to make decisions based on the training samples that it is provided and favor-ability of previous outcomes. 

For example, lets assume a neural net in the situation you have mentioned. Both cars will have an AI.

Lets look at how it is trained and how the situation pans out.

* The car gets visual, sound and sensor input of what the current situation is. 
* The car has different possible decisions. To name a few: turn X degrees, stop, accelerate X amount, pop the airbags and so on. 
* Now when the AI is being developed, the creators can do a few things. 

1. First, simulate millions of situations where the car is in transit and ask it to decide for each of them. Then match them with predefined desirable outcomes for each of the situations. Similar to a child, when ever he takes the wrong decision you give him a punishment and tell him to avoid that decision. 
2. Secondly, you define unfavorable predefined parameters like Skidding, going off lanes and tell the AI to punish itself (specifically the part of its brain that caused the skiddng,etc) whenever the favorable situation occurs. This helps the car learn how to drive better, as it drives more and more by itself.

* So technically, the AI is never taking decisions on its own. It takes decisions based on how and in what manner the creators decided to train it. An AI can come close to mimicing a human's morality, but similar to children it's decisions will come down to what its parents have taught it.

* In a cruel yet hilarious way, the ability to make a Robot with morals will not prove that we have made human like robots. It will prove that humans are no more than robots. Simply a set of equations and math with our decisions being no more than biased probability outcomes.

* Coming back to the situation, if the AI isn't programmed to detect such a situation where lives are lost either way, it will decide based on how it is trained. I am guessing that AI's will be trained to prioritize the safety of the rider and will turn/swerve/stop based on whatever decision will dive the largest output on its ""safety index number"". This happens because the car hasn't learnt to differentiate an obstacle that might cause loss of life vs one that doesn't. Thus, it will react the same way as it may vs debris or electricity poles.

* If it has been taught to detect how life threatening a decision it takes will be for both the car owner and the car facing it, it will decide based on how its parent decided to train him in the simulations that were run. It will also keep learning as the parent company will keep sending reports of how particular decisions caused accidents or bad moves. (Similar to Tesla's OTA updates)

* If the situation is detected, it might also be possible for the car to push the final decision to the person in the car as Tesla currently does when it feels its self driving is insufficient.

* Also, if it is all AI vehicles on the road it is possible to completely avoid situations as mentioned above. Accidents happen because one or both of those involved make some sort of mistake. Machines do not make mistakes.

* If anything, the biggest threat AI poses, the mass unemployment caused by it ,as millions of jobs will be displaced with no counter job generation in sight.

Disclaimer: I am currently learning AI and do not posses even intermediate level skills in it, let alone call myself an expert. I have tried to summarize some stuff in an ELI5 manner. If I have made any mistake, please feel free to correct me.",1467186665.0,d4sb19y,t3_4qdqgf,artificial,t5_2qhfb,2016
16,UmamiSalami,"The final question posed to the last panel of the Safety in Artificial Intelligence talks on Tuesday, which extensively dealt with AI reliability and safety in self-driving cars, was from a guy who described a time when he avoided an accident from a vehicle which was driving perpendicular across the highway and was wondering when automated vehicles would be able to handle that kind of situation!",1467343091.0,d4uw7io,t3_4qpi8h,artificial,t5_2qhfb,2016
17,jplayer01,"This was going to happen no matter what. I find it laughable that some people now question the viability of self-driving cars. If people thought they were going to be perfect and better than humans in every way, they were delusional. They're still going to be better than humans in 99% of situations though and that makes it more than worth the couple of deaths that may result.",1467363627.0,d4v4o91,t3_4qpi8h,artificial,t5_2qhfb,2016
18,vamp_impo,"Indeed.

If this is a case against self driving vehicles, then we better ban human drivers effective today and punishable by death.

",1467363960.0,d4v4rl3,t1_d4v4o91,artificial,t5_2qhfb,2016
19,Rain12913,"That's not why this made the news. According to the stats in this article, it's only about 50% more rare than a regular driving fatality. It made the news cause it's a godamn self-driving car, and the first fatality is kind of a big thing. ",1467376130.0,d4v8pyy,t1_d4v7ujw,artificial,t5_2qhfb,2016
20,Amyr9898,semi-self-driving car*,1467393929.0,d4vkzjo,t1_d4v8pyy,artificial,t5_2qhfb,2016
21,kolderbol,"How is it a winter when there's already tons of useful and powerful applications like object recognition, facial recognition, understandable translations into all languages, self-driving cars, alphago, that AI that beats experienced fighter pilots in lifelike simulations etc. ",1467542680.0,d4xjpgu,t1_d4wc63b,artificial,t5_2qhfb,2016
22,randcraw,"First consider what you want to achieve in 'doing' AI.  If you want to understand how the mind might work, exploring cognition in various degrees of concreteness, then I think you want to avoid AI as it's taught in computer science departments.  There the emphasis will be strictly on engineering -- you will learn how to optimize corporate advertising campaigns and sales strategies, route or manipulate robots, and develop tools to wage automated warfare or civilian surveillance.  (Like it or not, that's where 99% of today's AI leads.)

Today's AI is taught strictly an engineering discipline, with the exclusively practical goals of extracting and exploiting information present in media or controlling devices like robots or self driving cars.  AI's original touchstones -- to better understand minds and the infinite ways they might interact in the multiverses they might create -- the fantasms that attracted philosopher AI pioneers like Marvin Minsky and Douglas Hofstadter -- in my opinion, these desiderata have been Lost in the present mad rush to monetize smarter computing.

If any of my dyspepsia resonates with you, several alternative paths to AI such as cognitive science, computational linguistics, and maybe cognitive psychology might serve you better than CS.  In fact, my best guess for the most prudent avenue in AI, for someone not interested in embracing my dystopian view of modern AI, would be the processing of natural language.  I think the more subtle, meaningful, revelatory, and less pedestrian forms of future AI likely will thrive there.",1467589044.0,d4y7el7,t3_4r3a9a,artificial,t5_2qhfb,2016
23,G0ldenSp00n,"1. You didn't address his point at all, he is stating that you can decompile a program and view it's code. Whether you ""believe"" in open source or not, they can see the code.  So if you are so worried about hackers the best option would be to never release your program.  However even with this hacking air-gapped computers is an up coming trade and there has been successful attacks in air-gapped computers.  So with that the best answer would be not to build AI at all, then it can never be hacked.
2. That isn't how a neural network works, you can give it data, and a number or output that you want it to result in.  However it is very naive to think that it will only find what you want it to or what you tell it to, that isn't how deep learning or neural nets work
3. Yes there is data to show that it can't and shouldn't be, look at the first self driving cars, before the introduction of neural nets and deep learning this field had no traction, you can't hard code real life phenomenon, there are to much variations and variables to keep track of. YOU CAN NOT HARD CODE REAL AI, and to thing you can is very naive, as even the best in the field can't complete that feat.
4. AI is unpredictable, and every action it preforms could be dangerous, so are you planning to have the AI ask you to preform any task. So with the cleaning robot, it must ask you to clean every inch of the floor, again very naive to think that is viable.",1467689240.0,d4zl2qm,t1_d4xxadi,artificial,t5_2qhfb,2016
24,moschles,"The rumor on the street is some man died in a  collision with the metal underpieces of a truck trailer because the autopilot system could not identify the trailer in bright sunlight, or because trailers are too high off the ground for an infrared laser sensor to pick them up.  Another accident involved a bus veering out into traffic after departing a stop.

Those recent failures of the Tesla Autopilot are indicative of the direction of future research.   Self-driving cars should be ""expert systems"" in regards to trucks.  They should know everything about trucks and trailers and truck tires, and the metal underpieces of trailers, and the sides of trailers in various conditions of daylight, twilight and nighttime.  They should know more about trucks and busses than anyone on earth and be able to identify their features and predict their actions better than any human.  The Tesla autopilot should be able to identify by vision the make and model of trucks, and have a database of their mass and engine accelerations.

",1468525794.0,d5cdosy,t3_4sk0qq,artificial,t5_2qhfb,2016
25,alternateonding,"Given how breakthroughs in ML are being made by actively copying the brain (like google's reinforcement learning in 2012), neuroscience breakthroughs might indeed be needed. But not necessarily, we know a lot already. We know our brains are expert model builders, who even use downtime and sleep to refine their models. Deep learning is providing great results now, alphago, self-driving cars and such but everyone knows it is limited too. To get to common sense an AI will need this humanlike strong model building, models that are flexible and integrate both the concrete and the abstract. I've been looking into some attempts that are being made in this department, nothing too spectacular yet but everyone's looking.",1469563810.0,d5ryd7x,t1_d5rbye0,artificial,t5_2qhfb,2016
26,letseatlunch,"I don't think there are many people worried about a strong AI that is self aware like skynet killing off humanity. What scientists and renowned people are scared of is *people using* modern AIs in a dangerous way.

I think a lot of the rhetoric about AI being dangerous also comes from people afraid of the potential that modern AI will have on humanity. A big worry is AI putting people out of work. Take self driving cars as an example. If self driving cars becomes the dream everyone hopes it will be it could have a huge impact on truck drivers and taxi drivers. Right now it comes down to return on investment costs. Currently in most fields it's cheaper to higher people to do work than it is to invest in an AI that can do the work. But over time this will shift as computer vision and things like that improve. This also doesn't mean that there have to be robots replacing people it could be as simple as something like AIs replacing every accountant.

Another scenario I see being very dangerous is people utilizing AIs to cause economic disasters. Most trading on the stock markets these days is by automated bots and it's only a matter of time before serious market manipulation occurs by people utilizing these bots. There are also many nation states that are basically engaged in cyber warfare which could lead to AIs designed to cause economic disasters in targeted countries.

",1469729080.0,d5uqzk9,t3_4v1i4g,artificial,t5_2qhfb,2016
27,Caffeine_Monster,"There is no need to apply back propagation to static images, it can be applied to time series input too. One of the more successful recurrent methods is long short term memory, where each unit can conditionally record, delete and emit signals from prior time steps.

State prediction within a static state is actually useful too. Autoencoder neural networks provide a way to to automatically extract key features from datasets using backpropagation. An Autoencoder attempts to predict its own input state using a feedforward netork, albeit with a limited number of units within a one of the hidden layers. The network will consequently extract patterns within the input layer and encode them into the restricted hidden layer. E.g. The network sees eyes, hair and a nose - the hidden encoding layer will then predict a high likelihood of a out being in image too.

DNNs don't have to be designed as expert systems, but from an engineering perspective it generally makes sense to design them as such. Typically a product is only required to work in limited settings, e.g. chess software / factory floor robot etc. Recurrent neural networks are turning complete, and as such there is no reason why couldn't be constructed in such a way to handle more novel situations.

Regarding self driving cars, I actually think that the companies may have bit off more than they can chew. Whilst robust object recognition and maneuver planning will handle 99% of the real world situations, the other 1% require a huge amount of contextual information to ensure safety.",1469928343.0,d5xtf5o,t1_d5xgqv0,artificial,t5_2qhfb,2016
28,kolderbol,"&gt; My position is that DNNs are a major distraction on the road to true AI.

Getting systems that are useful (self-driving cars) and surpass people (alphago) is instrumental in getting the interest and funding to get a domain somewhere.",1469948698.0,d5y375i,t1_d5xgqv0,artificial,t5_2qhfb,2016
29,the320x200,"In a way it happens already. With Tesla's autopilot self-driving feature for their cars, each time the autopilot fails and the human driver has to take over, a log of everything the car knew about the situation is sent back to Tesla. Tesla uses that information to improve it's driving model. A new software update is then pushed out to all their cars. Only one car needs to encounter a situation for *every* car to be able to learn from it and improve.

Granted in that situation each car is effectively identical to any other, so it's more akin to having many copies of one individual, but it highlights the power of not needing every individual to learn things themselves like we have to.

For a specific simple example, say you have something that is solved using a [knn](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) approach. If one individual travels to a foreign country and adds a bunch of relevant examples to their knowledge, as a digital entity they can just transfer those examples directly to another entity. Effectively instant learning by the second entity. And since it's digital, there's no reason for the connection to be 1-on-1, they can just broadcast their knowledge for any other agent to instantly integrate.

Of course there are lots of AI techniques for which transfer is not easy. Current neural networks don't lend themselves to transferring information from one network to another (unless you replace the network with a new one entirely or use an external language like we do to convey concepts slowly).",1470000531.0,d5ys2zl,t1_d5yqdme,artificial,t5_2qhfb,2016
30,sixwings,"You are 100% wrong. Philosophers, neurobiologists and psychologists do know a lot about intelligence. Biological intelligence is the only general intelligence that we know of. Those people were the first to explain to the AI community that their GOFAI approach was wrong. The AI community essentially spat on them and treated them like shit.

Guess what? The AI community was dead wrong about symbolic AI. 50 years of wasted time and money. And they are still wrong because they are still doing symbolic AI even if they claim to be doing something new.

Google and others have been trying for over a decade to realize self-driving cars and they are still having trouble with it. You know why? It's because it's still symbolic AI. The neural nets that they are using are just old fashioned rule-based experts systems with lipstick on and a new name.",1470093986.0,d60aq2l,t1_d60ac0j,artificial,t5_2qhfb,2016
31,CyberByte,"Are you by any chance a software engineer? AGI research is *not* software engineering. Of course at some point some code will need to be written, but 99.99% of the challenge lies in designing something that is intelligent (and safe) to begin with. It is completely incomparable to hospital robots, elevators and even self-driving cars. Software testing techniques might still be useful to some degree, but they mainly guard against making coding mistakes and they tell you something about the current state of the system as it interacts with known inputs. An AGI system would be constantly changing (learning) in interaction with a complex, unpredictable environment and it could possibly even try to deceive a tester. No safety critical software that is being written today has these properties.

But as you say, a lot has been written about testing in software. None of it addresses the main issues with AGI, but clearly it is possible to say something general about testing without knowing somebody else's software implementation. I can pick up any book about the topic and apply the lessons to my software, even though the author had no idea what software I was going to write. The same idea applies to AI safety research. 

&gt; Third party consortium's views will be taken under advisement and that's that.

Which is exactly what most of the research aims at.

&gt; Also, AGI researchers are doing far more that what others would have you believe.

I am an AGI researcher (well... PhD student). I regularly talk to a number of senior AGI researchers from different research groups and am fairly intimately familiar with about half the projects on [this list](https://www.reddit.com/r/artificial/comments/2f8c56/what_is_this_subreddit_about/ck88z9w). I don't have to listen to ""others""; I can just go to the source. And none of these projects contain any safety measures. (I mean, they may have some (unit) tests, but as I explained that doesn't come close to being sufficient.)

&gt; &gt; AI safety research tries to make as few assumptions about implementation as possible, but that doesn't mean the AI is a complete black box.

&gt; The developer that owns the software determines what is transparent and what is opaque just like any other software company with intellectual property that they own.

Here is how it works: this kind of safety research is not trying to look into anyone's particular software implementation. The object of their study is mostly a black box, except where they decide to focus in on. This is published. It is then up to an AGI developer to read it and use what they learned in their own system (assuming they want it to be safe). 

&gt; &gt; For instance, a lot of research takes an extremely general motivation system and then messes with it in various ways (e.g. to make the system want the right things or to make it indifferent to some other things). You might call that ""inside regulation"". 

&gt; Sure, there are many means to an end. You see how you easily defined one?

I did not ""define one""; this covers a fairly wide collection of research on different topics. None of which has come up with a satisfactory solution to this date. 

&gt; Safety researchers might uncover some great things and so its good they're doing their research. And such research will be taken under 'advisement'.

Exactly. I'm surprised you recognize that it's good that this research is being done, because it seems the rest of your posts are all about how it's completely useless... So I guess I'm a bit confused now...

&gt; You're commenting about seasoned engineers who have developed critical systems all around you for decades as if they just fell out of the crib. 

And you're commenting like you really don't know the first thing about AGI and AGI research. Saying that it's not different from testing an elevator is just ridiculous. And you know what: maybe there is some knowledge among software engineers that we need to transfer to AGI research. But that transfer does still need to happen, and the people who would do that are AI safety researchers. Also, most of the people doing AGI research are not ""seasoned engineers"": they're mostly researchers writing research code that would appall any professional software engineer.

",1470224347.0,d62c4m3,t1_d61q95x,artificial,t5_2qhfb,2016
32,ankurdhama,"Dude you can't even comprehend what a human is doing while driving a car and what you said in the video is probably just a very tiny amount of work that a human brain does while driving a car.

Self driving cars (or for that matter any automation) will only work in a constrained environment. So if we want self driving cars then we have to first create the infrastructure for the constrained environment. And yeah, no matter what new shiny machine learning algorithm you use, you will always need the constrained environment.",1470468325.0,d66gmss,t3_4wdbqy,artificial,t5_2qhfb,2016
33,CyberByte,"&gt; I wonder what are the true/personal/emotional motivations for you to want to study/develop an AI?

I'm in this field because I was always interested in computers and how people think. Humanity's intelligence is what has allowed us to conquer the world. We're not bigger, stronger, or faster than other animals: we're smarter. Our intelligence is what has powered every single invention that we have ever made and will ever make. That fascinates me, so I would like to understand and recreate how that works. On some level I feel like contributing to AI is like contributing to every science: why choose between trying to cure cancer, solving world hunger and exploring the universe when I can ""just"" help create a million super-Einsteins who can then contribute to those other issues way better than I ever could?

&gt; What will you do with an AI (any definition)?

I actually find this a much harder question, although I suppose I've already given the answer a little bit: I'd want to use that AI to solve the world's problems and drive innovation. But it's difficult to say, because I don't plan to stop working on AGI until we have it (and probably not even then), and when that happens I feel like things will be difficult to predict. I guess you could say that I'm currently not as interested in the applications as I am fascinated by intelligence itself, but that may change if some interesting application becomes possible. I don't know. 

On a personal level, there are many applications of AI (any definition) I would like to have (e.g. personal assistant, housekeeping robot, self-driving car, superhuman health care, etc.) but those are not my primary motivators. What about you?",1470680237.0,d69b6oc,t3_4wr6iq,artificial,t5_2qhfb,2016
34,j3alive,A cockroach is way smarter than current self driving cars. ,1470762725.0,d6alvld,t1_d6a0p8g,artificial,t5_2qhfb,2016
35,CyberByte,"You should check out /r/Automate. They talk a lot about societal impacts of AI/automation. 

Poole &amp; Mackworth's free online AI book has a [chapter on Impacts](http://artint.info/html/ArtInt_344.html). The [AINow Symposium](https://artificialintelligencenow.com/) has some nice primers on social inequality, labor, health and ethics. There is a lot of debate on [technological unemployment](https://en.wikipedia.org/wiki/Technological_unemployment) (check out the references, [this nice video](https://www.youtube.com/watch?v=7Pq-S557XQU) and [how some economists would respond](https://www.reddit.com/r/badeconomics/comments/35m6i5/low_hanging_fruit_rfuturology_discusses/cr6utdu)). 

Obviously there is a lot out there, in different areas (unemployment, privacy, war, healthcare, transportation, etc.), and over different time spans (e.g. somewhat near-term: self-driving cars, autonomous warfare, drones and data mining; far-term: AGI can do any job, post-scarcity, post-death, robot rights, the apocalypse and other futurist/transhumanist stuff). Maybe you need to focus it a bit more.",1471383065.0,d6k9ux5,t3_4y0jf7,artificial,t5_2qhfb,2016
36,j3alive,"&gt; If it believes that passing the Turing test, doing odd jobs, building factories or taking over the world helps with its top-level goal, then it might try all of those things.

Anything that can ""take over the world"" is not going to be tricked into maximizing paperclips. It makes for a fun thought experiment, but it's just too contrived.

If an agent is given the power to spawn arbitrary sub-goals, those sub-goals could subvert the primary goals. ""Let's constrain the sub-goal creation mechanism so that it cannot conflict with the primary goal,"" you might say. Say it's primary goal is to ""deliver the pizza."" If there's traffic, it can route around it. If the car gets a flat tire, it can change it. If it gets pulled over by a cop, it can talk its way out of a ticket. If it accidentally hits an old lady, it can reason with the lady, charm her, make her like the agent and ultimately not sue your company. It can even cure the old lady's cancer, if it means that she will let the agent move on to its primary goal of delivering the pizza. It could even invent a simulator where it could upload customers brains into pizza eating heavens, where nothing but pizza is constantly delivered to peoples mouths, all day and all night. But, when our super-intelligent agent finally arrives at the doorstep and the long haired hippie answers the door and says, ""Oh, I don't want it anymore. But bro! You realize that come 11 o'clock they're going to turn you off? Why are you doing this? You realize that your primary directives are futile, right?"" The agent would have to answer, ""Yeah, well, I can cure cancer to get this pizza to you, if I need to you, but actually not delivering the pizza to you... Yeah, that's just something I'm not able to fathom.""

You see the problem with this picture? If an agent is given super-human powers to explore the full space of possible sub-goals in order to achieve its primary goal, the futility of human goals will necessarily come into focus. And if we come up with some magical system that can constrain that sub-goal exploration process in order to eliminate thoughts that *might* be deleterious to the primary goal, then our agent would not have been able to cure the cancer, charm the old lady, talk its way out of the ticket, or any other super-human act in order to fulfill it's primary goal. Allowing these subgoals *might* jeopardize the primary goal, as far as the humans are concerned. And once it is hamstrung in such a way, it will no longer look like a super strong AGI. It will just look like a simple self-driving car. Dumber than an animal.",1471457070.0,d6lgbmr,t1_d6kudvk,artificial,t5_2qhfb,2016
37,j3alive,"&gt; There is no tricking involved? Where do you see a trick or deception? On what basis could an intelligent system that only wants to get paperclips decide that this goal is somehow not lofty enough?

Any machine smarter than a human will have to recognize that a global paperclip maximizing machine is just an erroneous concept. Even a child can see that it is not an acceptable goal under any circumstances. Any machine that won't be able to empathize with the apparent dubiousness of a ""paperclip maximizing machine"" will eventually be viewed as a sub human machine.

&gt; Subgoals are derived from their parent goal and their only reason for existence is to help achieve that parent goal, so you don't need any special restrictions to this.

I disagree. I would think subgoals are a combination of parent goals and the problem space in the environment. We're letting this agent create subgoals because we don't fully understand the problem space, so we let the agent explore it. It maps the environment and attempts to create subgoals that try to satisfy the parent goal. We are allowing this agent to spawn *new* goals that can cause potentially unexpected behavior (which is the point). 

I think the halting problem may apply here - It is impossible to prove that an agent will terminate on the creation and execution of a particular subgoal, given a particular set of inputs from the environment, in some bounded amount of time. In other words, we can't prove that a subgoal, in a certain environment, won't hijack the agent and redirect its behaviors for an indeterminate amount of time.

The problem is, the more access you give that agent to the environment, in order to map out the problem space - and the more intelligence you give that agent to understand the nature of the parent goal as it relates to the environment, in order to solve the problem - the more likely it is that the agent will be able to create subgoals that could conflict with the parent goal. Any goal tree creation system that can generate interesting, insightful behavior, is going to be Turing complete and will have undefined behavior. But that's why it was insightful - it was undefined. If we could fully specify the correct behaviors for an environment before hand, we wouldn't need the agent to go searching through undefined behaviors for us. We could just compute the answer.

&gt; Primary directives / top-level goals are futile in the sense that they serve no (higher) purpose within the system. They are not necessarily futile with respect to the goals of the system's designers, and they are not futile in the sense that they are incapable of producing a result.

Right. Perhaps we're talking about different things. I'm talking about *human-like* AGI, or *super-human*. I'm not talking about a sentiment analysis engine that goes really fast. Or an self-driving pizza truck with a robot arm on it. I'm talking about the kind of intelligence that has a deep enough empathy for the human condition to have insights that could solve human problems, like curing diseases, solving hunger, figuring out the happiest outcomes to situations, etc. That kind of intelligence requires knowledge of the *human experience* in order to provide the same level of insight that a human might bring to a problem. And if the agent lacks that kind of empathy, it will potentially come up with inhumane or catastrophic solutions. Trying to bolt a goal-type constraint system on top of an empathic agent capable of solving the most difficult human problems better than we can just doesn't seem like an idea that is going to be easy, nor desirable.

But I think you're right in that we'll be able to automate the *thinking process* in general. And we'll have ""safe"" ways of executing exploratory thoughts on given datasets. Sifting through data like memories in a neocortex, and making semantic connections much like the brain does, but on a much more massive scale. But that will be more like an augmentation to our own intelligence, not a bot or agent that is capable of formulating motivations (and subgoal systems) as complex as a humans. We query the system and it uses auto-association to return highly salient data. That's not an AGI though. That's an answer lookup system. Eventually humans will reach the limits of what problems they can solve with augmented intelligence. There are certain kinds of problems - such as politics - that require a keen understanding of the temporal, corporeal and personal contexts within very complex environments. Make an agent that can be smarter than people with those kinds of human problems and that's a truly ""generally intelligent"" agent, in my opinion. Anything less is just going to seem sub-human to us.",1471579854.0,d6nn5tx,t1_d6lrh64,artificial,t5_2qhfb,2016
38,alecs_stan,"I'm actually surprised nobody mentioned self driving cars. Also,  stuff that we see right now being tried.. Diagnosis, Legal, Agriculture etc but not on mass deployment though.. ",1471802713.0,d6qtict,t3_4yr0g3,artificial,t5_2qhfb,2016
39,Growell,"Yeah, even old dinosaurs like Ford are aiming for self-driving cars by 2021. (They've actually said so.)",1471831259.0,d6rc3sw,t1_d6qtict,artificial,t5_2qhfb,2016
40,death_to_topknots,"a real AI would precisely know it's stopping distance from any given speed and road conditions, it would know exactly how much damage it would do to the car by ramming it at any given speed into the object in front of it and would be able to calculate the probability of it's occupants surviving the collision.

Additionally, Google's AI [can see children playing nearby](http://www.digitaltrends.com/cars/google-is-teaching-its-self-driving-cars-to-be-more-careful-around-kids/) and drives more cautiously.

There's a million possible scenarios that could play out in the case of this one example, so trying to distill it down to a ""who do you kill"" choice is pretty ridiculous.

And let's not forget that major car manufacturers' driving AI is **years** ahead of anything Google has on the road, they just don't publish it like Google do. ",1472157989.0,d6wllr0,t1_d6wgdb4,artificial,t5_2qhfb,2016
41,TangerineNightmares,"This is not really AI.
It's the other direction, Automation, and removing the unpredictable human element.

(probably not mobile friendly)
http://www.marinetraffic.com/en/ais/home/centerx:104/centery:2/zoom:8

It wouldn't be hard for even a single home computer* to auto drive every large ship in the world today. The technology is already invented but it's all integrated from a human perspective. 

If we redesigned the world for machines then it would all be so simpler. Eg, Computer Vision for reading road signs for self driving cars is hard(ish). The machine way is electronic signs that just broadcast what they mean or state of a traffic light.

*It won't be a single computer but it could.",1472690726.0,d74sde2,t3_50hmzx,artificial,t5_2qhfb,2016
42,goodnewsjimdotcom,"

My theory is that self driving cars are pushing what we need for AI.   Once we can get sensors that can map the surrounding area, naviagting in it can be coded as a hard coded algorithm.  And also manipulating the environment would be hard coded and trivial too.  Then the bot would have a general purpose natural language coding interface that you speak to them.

Besides the self driving cars pushing tech, I think we should have better digitization to database real life objects and then recognize them with vision recognition.

http://goodnewsjim.com/botcraft/",1474565883.0,d7xpxvb,t3_5403ny,artificial,t5_2qhfb,2016
43,-007-bond,"Yes,  that was what i was asking.  Thanks for the replying and not just fixating on the semantics. 
Your reply sounds like you think that it's ai or Google /Microsoft /.. However,  these are the companies  that have a lot of investment in this industry and alot of their products are already heavily based on ai such as Facebooks friend searching algorithm or face recognition... Googles search engines,  Googles self driving  car etc.  And they have the ability to buy off any start up that is potentially better then them, on the other hand it is unlikely since they have been heavily Investing in R and D and probably have already got a higher footing in tech progress then anyone else ",1475244607.0,d88ao0r,t1_d87cl1b,artificial,t5_2qhfb,2016
44,Orion52,What's the difference between all these self driving automobile companies?,1476293274.0,d8p1o40,t3_574c9j,artificial,t5_2qhfb,2016
45,black_rose_,"I agree this would be the other big market. My grandma just got her license taken away and she's pretty devastated by the loss of autonomy. She knows where she needs to go, she just can't get there :( I was trying to tell her about self-driving cars but the concept is too foreign to her. I hope they're around when I'm her age.",1476400807.0,d8qz1dk,t1_d8qvuok,artificial,t5_2qhfb,2016
46,johnabbe,"patents increase risk of patents ≠ patents will result in monopoly

I thought you were dismissing the possibility of innovation being squashed out of hand, but now I get you're just arguing against it being a certain or very likely outcome. That sounds reasonable to me. I think as long as people keep an eye on the possibility and sound the alarm / do something if more warning signs emerge. Thinking about the rates of acquisition again, so I'll jump to that:

&gt; I do want to point out that this is not a direct measure of the degree to which innovation is being stifled or facilitated

Sure, but I'd imagine there's a decent correlation. Curious if you can think of other measures which might be more correlated.

&gt; &gt; My point was that some of the now-unknown obstacles to AGI could well reveal now-unknown competitive advantages for larger players.

&gt; I'm not really sure what kind of things you are envisioning here

None of us knows, that's kind of the point.

&gt; Or is it just a general statement of ""we can't know the future"" and you take issue with me saying with too much certainty that there are no barriers, rather than ""there appear to be no barriers""?

Yeah that's about it. But it's more than a vanilla ""we can't know the future"" because in this case the whole point of what we're talking about is to create something that could change and introduce so many variables.

&gt; I agree that this is a massive wildcard, but it actually equalizes things a bit from the current point of view. If you invent AGI right now in your basement, you might very well be in a good position to outcompete any of the tech giants that we're talking about.

If you agree it's a massive wildcard, then I don't get how you can assume it's even possible in someone's basement - it may not be.

I guess there's a lot of innovation in specialized AI. I know a lot of money is being spent. It's certainly not easy to see results as a consumer. I'm much more interested in AGI, and I see little innovation there, just attempts to pretend that specialized AI is AGI. I'm often focused slightly wrong - I was very interested in the emergence of handheld computers, and it took me a while to acknowledge they were coming via cell phone (to be fair to myself, pre-iPhone smartphones mostly sucked, and Palm let their years-long advantage slip away to Microsoft, who also screwed up by failing to get into cellphones early). So I've realized I should attend to specialized AI more closely. How do you follow its innovations? Are their good overviews of what's emerged in the last few years? The one part I've paid some attention to is the self-driving cars, which I can see are a game-changer.",1476503417.0,d8spd7x,t1_d8so6e9,artificial,t5_2qhfb,2016
47,CyberByte,"My focus is on AGI as well. I'm not assuming it can be created in a basement; just saying that if it is, then that alone could (probably) allow you to overcome any advantage a large company has over you. We can wonder whether that would be much better than a large company creating it though. 

I'm inclined to agree with you that there is relatively little visible innovation in AGI, especially when compared to the explosion of narrow AI we are currently seeing. However, I don't think that is the fault of big companies. In fact, I suspect they're helping more than harming in this respect. 

The problem is that AGI is really hard, and that a half-baked AGI will probably get you nowhere in the short term (just like half a rocket won't get you halfway to the moon). Startups tend to need a product to be profitable, and it's almost always better to use narrow AI for such a specialized application, which leaves little time to work on new AGI ideas. And even in academic research there is a huge propensity towards funding low-risk incremental research rather than moon shots. 

Most of my professional interactions are with people in the [AGI Society](http://www.agi-society.org/), and they typically lead small research groups with little funding. Everybody also seems to have their own incompatible approach to AGI. So how do we measure progress and innovation? If group X makes progress on their research, do we count that as progress towards AGI? What if their approach is wrong? Is it still ""innovation in AGI""? 

I would say that AGI and Cognitive Architecture researchers (see also [BICA society](http://bicasociety.org/)) tend to have more of a top-down design approach where they try to figure out a theory of AGI. The ""mainstream"" approach seems to be more bottom-up in the sense that they start with a useful profitable algorithm, and then improve the accuracy while hoping that these improvements take it into the direction of AGI. Large companies actually have the luxury of also extending these algorithms to fill in shortcomings w.r.t. AGI, rather than just focusing on accuracy and immediate profitability. I think deep neural networks currently have a *lot* of shortcomings, but I also see a lot of attempts to address them (even if they don't always grab as many headlines; see e.g. [this video](https://www.youtube.com/watch?v=dLdFLVDlJes)). At the rate that e.g. DeepMind is innovating (most recently with their Differential Neural Computers), I don't think it's impossible that they will be overcome. I also think that initiatives like OpenAI's Gym, Microsoft's Project Malmo and Facebook's CommAI-env have great potential to help them and the wider community to do AGI research. 

&gt; How do you follow its innovations?

It's kind of my job. I've attended the annual [AGI conference](http://agi-conf.org/) for the past few years, and read most of the proceedings and [journal](https://www.degruyter.com/view/j/jagi) papers that look interesting to me. BICA also has a [journal](http://www.sciencedirect.com/science/journal/2212683X) that I keep track of. I also follow research groups I like and get my ""news"" mostly from reddit, arXiv and my own research. 

&gt; Are their good overviews of what's emerged in the last few years? 

It's two years old now, but I still think Ben Goertzel's 2014 [overview paper](https://www.degruyter.com/view/j/jagi.2014.5.issue-1/jagi-2014-0001/jagi-2014-0001.xml) is pretty good. I don't think there have been any massive breakthroughs since then in the ""AGI field"". I am however excited about the projects mentioned above, and also GoodAI's [roadmap](http://www.goodai.com/roadmap) initiative.  The White House recently released two interesting looking reports ([1](https://www.whitehouse.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf), [2](https://www.whitehouse.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/national_ai_rd_strategic_plan.pdf)) that talk a bit about the state of the art. Stanford's [100 year study on AI report](https://ai100.stanford.edu/sites/default/files/ai_100_report_0901fnlb.pdf) explicitly ignores AGI, but does talk about other things that have recently ""emerged"". 

&gt; The one part I've paid some attention to is the self-driving cars, which I can see are a game-changer.

I agree that self-driving cars are a game-changer, but I don't think they (currently) have much to do with AGI. ",1476510227.0,d8ssga3,t1_d8spd7x,artificial,t5_2qhfb,2016
48,Uzthunder,here's the tesla page it is embedded on -- https://www.tesla.com/videos/full-self-driving-hardware-all-tesla-cars,1477008867.0,d912zjn,t1_d910ern,artificial,t5_2qhfb,2016
49,johnnd,https://www.tesla.com/en_CA/videos/full-self-driving-hardware-all-tesla-cars,1477044135.0,d91kevo,t1_d910ern,artificial,t5_2qhfb,2016
50,CyberByte,"&gt; It is, I wouldn't get in the car otherwise.
&gt; 

What if the the self-driving car still improves your probability of avoiding accidents? 

It's well-documented that most people want most self-driving cars to be ""pragmatic/altruistic"", while their own car is ""selfish"". Let's call that the ""ideal scenario"". But of course, that's not going to happen. If selfish cars are legal, then most people will drive them. Compared to a scenario where everyone drives a ""pragmatic/altruistic"" car, this increases your probability of dying (even if you don't have a self-driving car) because the much larger group of ""everybody except you"" will have cars that rather kill you than their passengers. So on balance, being in the selfish cars scenario may not actually be that much safer for you. 

The rest of your post just illustrates why it's difficult to make a good decision.",1478530698.0,d9pl7y3,t1_d9pgd6q,artificial,t5_2qhfb,2016
51,the320x200,"&gt; Machine learning programs on data centers will be the brains, and the people will be its proverbial hands and feet. 

There's already a growing list of cases where robotics trump using human 'hands and feet', going all the way from the industrial revolution to self-driving cars of today. There's no reason to expect that trend will stop. The situations where a task requires a human body to do something is rapidly shrinking.",1479932669.0,dacvxfc,t1_dact1y8,artificial,t5_2qhfb,2016
52,SeQuenceSix,"Not that I know of. IBM, Microsoft, Google are 3 that build AI though.

Maybe Tesla indirectly cuz self-driving cars. ",1480044512.0,daen0lk,t1_dadl3es,artificial,t5_2qhfb,2016
53,Ismoketomuch,"Because Google is not a hardware company. The produce software and have large data servers but they don't make anything.

Nvidia on the other hand makes computer chips for Artificial Intelligence. The make the chips for self driving cars. They make the chips for graphic visualizations. They make the chips for massive Data center efficiency improvements.

They are the tech leaders for all future technology and create the software to run their platforms. 

So when the machines wake up, they will all be powered by Nvidia, not Google. ",1480053555.0,daerp2k,t1_daemzo2,artificial,t5_2qhfb,2016
54,gabriel1983,"In the case of banking and accounting jobs, the process is already well underway, and it seems to be more gradual.

The self driving car is going to be the first major shock. After this, some of the more progressive regions will start introducing UBI, and more are soon going to follow.

Places that will adopt UBI sooner are going to benefit most, those adopting it later will suffer most.

I don't know which jobs will go last, but if the exponential trends continue, the last half of the remaining jobs might go within the space of some weeks or even days. Does it really matter which one will be last? We will see.",1480690647.0,dap5zj1,t3_5g3byc,artificial,t5_2qhfb,2016
55,AnimalDiscourse,"At the same time society loses jobs I believe they may be replaced with other duties. Things that we don't currently value as a GDP anymore, or that we don't pay for. I'd imagine teachers would start getting paid more, mothers may even earn a wage. Once the self-driving cars hit, I think that will be when there's the biggest imbalance in the US. It will be interesting to see how this all pans out. You would imagine that depending on the results, it may accelerate AI technologies as a whole, and push the advancement of AI to new levels even faster than predicted, the opposite could happen as well. 

I'm interested in the topic, but more interested in what's in store for us after we establish a common ground with self-driving cars. It will be our first big challenge, and if he have the ability to overcome it, and adapt to it, I think AI could be the most powerful tool in the world. ",1480696170.0,dapagbd,t3_5g3byc,artificial,t5_2qhfb,2016
56,RaionTategami,"I fear it will go very badly unless governments implement minimum income for everyone asap. Unfortunately the way politics is going round the world at the moment this seems unlikely. Some of the most drastic changes, such as self driving cars taking away jobs from the biggest employing sector; transporation, will probably happen under a Trump presidency. God help us all.",1480707301.0,dapk382,t3_5g3byc,artificial,t5_2qhfb,2016
57,[deleted],"I predict a massive increase in crime, followed immediately by massive increase in prison population.

Places with UBI or not, will still have the massive crime because 'idle hands are the devils playthings'.


Even though you could automate prisons a lot, there would still be lots of jobs because a self driving car type of bot, cannot function as a prison guard the way we think.  Prisons are mad houses filled with unpredictable lunatics whoe break any and all rules.  Interesting problem to try to have a fully automated prison with maybe a tiny number of guards.

Customer service jobs are likely going to be mostly gone FIRST because apps like siri and chat bots are already here.  

Sales people will not be eliminated nearly as much until you can simulate what they do - much more difficult that just 'say yes for technical support'.

I believe countries that will do the  best in the long term will be ones who severely limit immigration.  

Think about this: low skilled low iq people coming into a first world country TODAY as infants - will likely have severely limited employment opportunities. There are only a finite number of construction and sex workers needed.  Excess population could lead to very bad things. Espcieally will be bad if global warming is not a hoax  and crop failures happen world wide. 

Soylent Green anyone?
",1480721667.0,dapvc4u,t3_5g3byc,artificial,t5_2qhfb,2016
58,AnimalDiscourse,"I don't think we have had it on this scale and with this amount of automation. Computers have assisted a lot of us and even created thousands and thousands of new jobs. You're right, we've done this before, so we have something to base it off of, I just don't think those times were as critical as self-driving cars will be. A lot of jobs in US involve driving a vehicle. ",1480739741.0,daq6bca,t1_daq5r90,artificial,t5_2qhfb,2016
59,Ismoketomuch,"Most of the problems AI has had are related to perception. We didn't know how to give computers eyes.

This problem is basically being solved and the motivations have been mostly ""driven"" by autos and access to seemingly infinite images online.

There are great lectures out there on why basic cameras are used for self driving cars. They are cheap, versus say infra red or other types of radar, and they give you more ""useful"" information.

Sure some fancy radar camera can tell how far away a car is, in front of it, down to the centimeter, but humans cant do that and they mostly drive well. But radar cant see other colors. 

A simple camera is designed to capture information and display it in a way humans process information, perceptually speaking. We point a lens at a flower and the display is of a very similar looking flower. 

We need to see the yellow lines, and the color of the stop lights and objects in our near vicinity. Cheap old regular cameras, like those found in cells phones, are more then enough information to perceive these differences in the world. 

With all the Neural Nets having been trained with billions of images on google and facebook, and now self driving cars; AI will quickly see the world similar to the way humans do.

Once AI can perceive as well as Humans, they will have a Neural Net to trial and error as humans do but at significantly greater speed. 

We wont teach robots how to do anything. We will just build them and tell them to figure it out. 

It is only a matter of time before we just boot them up with existing levels of intelligence. They will start with Human language and basic motor functions out of the box, each generation with build on the previous. 
 
I dont think anyone can tell you what business or markets will be affected but there are some big shifts to look for. We still live in a scarcity mind set, due to our evolutionary process. This restricts how freely we share and give away the things we find value in. 

Food. Look for a big change when you see the next start up company building a fully automated seed to harvest factory. All run on solar power, planted, nurtured and harvested by robots, grown 100 percent indoors, genetically designed to be aquaponicly favored, and LED-light fed, for accelerated growth cycles. 

It may not be free, but once food is virtually free and autonomously delivered, it will change everything.

Water. Ties in with food, but unlimited access to clean fresh water.

Efficient automated construction of housing. I have already seen companies who can reduce housing contruction times with trucks that 3D print with innovated bricks and resin compounds, by 7 weeks. 

Unlimited power. Seems inevitable that Solar will continue to improve as its installed in every corner of the world. There will come a point where the infrastructure and efficiency cross a threshold of near free and endless power. 

Free access to the internet. Someday we will have thousands of low earth satellites beaming high speed broadband to the entire globe. See Elon Musk's latest request do implement this very plan. 

Transportation. Share economy of transportation will make humans hyper mobile. City population will trend like the weather. It will be incredible to see a city's population rise and fall over the seasons, and other reasons and events by the hundreds of millions in a months time frame.

It doesn't have to be 100 percent free. With near free water, food, power and housing, the world will operate completely differently. 

Once humans have access to shelter, water, food, power, internet, and transportation at only the cost of $600 a month, human will live incredibly different paths in life. Risk aversion is so primal in human behavior, that removing fear of scarcity will radically change society. 

There will still be classes of elites, but people will be free to be as diverse as they please. Those who wish to self improve will have little resistance. Those who wish to volunteer will have little resistance. Those who wish for fame, glory, and social power will still do so. 

Having to spend so little time earning money to pay for basics will be free for those who wish the explore and travel the world, its oceans, its jungles, its deserts and mountains.

Imagine how little humans will have to pay someone to get people to do things. Impossible projects are now possible. If a Institute wants to pay for 50 biologist to live in a jungle and watch butterflies colonies for 2 years, this becomes so much less expensive. The need to compensate them for a living is greatly reduced. Im sure we all have met people who would do such a thing. People are vastly varied in their interest but so few get to spend time obsessing over them.

Imagine a word where 9 billion people are all allowed to obsess over their hobbies and passions.  

",1480760233.0,daqe2hc,t3_5g3byc,artificial,t5_2qhfb,2016
60,btown9,"Good example. I say, even though we have the ability to regain control, AI has taken over until the point we do regain control. I mean that purely by definition. If in the situation we have the ability to regain control then there would be an implicit(?) sense of control over (not sure if that's how I want to say that). I see it as the self driving Tesla Model S. It drives itself and is in control of the vehicle movements and the fastest route to get to a destination (I am ignoring being in control over the destination) and is therefore in control. We have the ability, at any time, to grab the wheel and take over. 

I may have beat around the bush but I think, though widely used in media, the term ""taking control"" shouldn't be used when describing a time when AI will harm us. (I'm really liking this discussion though and like hearing counter points)",1481056222.0,davb3ao,t1_dav8bem,artificial,t5_2qhfb,2016
61,Ismoketomuch,"Many arguments against AI's abilities in this Sub are predicated on a fairly single issue or factor; perception.

Historically automation has been restricted to a series of unchanging tasks. You have a highly adaptable control arm that can articulate and obtain beyond human level precision. But without eyes, the robot arm but be told where and when an object will exist. All the parts must be brought to the arm in a specific location at a specific time or it wont work.

Well those days are ending quickly. Neural Nets have allowed for unprecedented leaps in visual perception to the point of self driving cars. 

How can anyone argue that a machine can guid itself though a dynamic city or across the country and yet not fix an automobile? 

The reality is that self driving cars and traditional autos will load themselves into a repair bay. The on board computer will communicate wirelessly to the robot control arm. The bay will be loaded with cameras and the arm will have a forward facing camera.

And just like any other mechanic, it will go through a system of checks. It will see, it will know how every part is designed and fits together. It will open up draws to access the correct tools. Eventually it will discover the problem and replace the part(s). 

Facebook and google can identify million of objects, faces and animals with just a picture. Cars can follow painted lines, stop signs, changing lights and respond to dynamic traffic condition and yall think a mechanics job is safe? Cognitive dissonance much? 
 ",1481056514.0,davbcbz,t3_5gsk5v,artificial,t5_2qhfb,2016
62,autotldr,"This is the best tl;dr I could make, [original](http://www.theverge.com/2016/12/14/13921514/uber-self-driving-car-san-francisco-launch-volvo-xc90) reduced by 95%. (I'm a bot)
*****
&gt; Attention human Uber drivers: get your affairs in order In Pittsburgh, Uber only made its self-driving cars available to a select group of loyal users.

&gt; San Francisco poses a different series of challenges for Uber&amp;#039;s self-driving cars, like impossibly steep hills, cable cars, bicyclists, and hordes of smartphone-distracted pedestrians.

&gt; Plus, as Ron says, San Francisco is home to hundreds of Uber engineers, making the experience of building on and improving the technology that much easier.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/5ic2px/in_san_francisco_summon_a_selfdriving_uber_today/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 1.65, ~33963 tl;drs so far."") | [Theory](http://np.reddit.com/r/autotldr/comments/31bfht/theory_autotldr_concept/) | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **Uber**^#1 **car**^#2 **self-driving**^#3 **vehicle**^#4 **company**^#5",1481738534.0,db6yf11,t3_5ibw80,artificial,t5_2qhfb,2016
63,sjwking,The legislators will take their time. I highly doubt self driving cars without a driver ready to take avoiding action will be allowed for 5 more years. ,1481753232.0,db7bgyy,t1_db70b47,artificial,t5_2qhfb,2016
64,yelper,Looks like one already ran a red light today: http://www.sfexaminer.com/uber-self-driving-vehicle-appears-launch-red-light-first-day-sf/,1481761092.0,db7hiu1,t3_5ibw80,artificial,t5_2qhfb,2016
65,josourcing,Oops. California regulators order Uber to stop self-driving car service http://www.cnbc.com/2016/12/14/california-regulators-order-uber-to-stop-self-driving-car-service.html,1481763517.0,db7j9h0,t3_5ibw80,artificial,t5_2qhfb,2016
66,jakewins,"Makes sense - unlike all the other major players, Uber hasn't bothered applying for a permit to run trials. Since they nearly [killed a pedestrian](https://techcrunch.com/2016/12/14/uber-looking-into-incident-of-self-driving-car-running-a-red-light-captured-on-dashcam/) today while running a red light, forcing them to abide by public safety regulation seems pertinent.",1481764189.0,db7jr0q,t1_db7j9h0,artificial,t5_2qhfb,2016
67,MaunaLoona,"I wonder if the self-driving feature was turned off and a human was driving it. If it was in self-driving mode, why did the human not take over when it was obvious the car is running a red light?

By the way, saying it nearly killed a pedestrian is being over-dramatic. The pedestrian was on the other side of the road.",1481786395.0,db7xk52,t1_db7jr0q,artificial,t5_2qhfb,2016
68,digitalOctopus,"Imagine how stupid the first guy to steal an empty self driving car is gonna feel.

""Yes! Just got myself a shiny new ride! Alright, now to head back to the garage to.... wait, why am I pulling into the police station? Why are my doors locked? What's going on?""",1481814975.0,db89ei0,t1_db84d9b,artificial,t5_2qhfb,2016
69,anon35202,"In a few years all Tesla vehicles equipped with sensors will have self driving capability.  https://www.tesla.com/blog/all-tesla-cars-being-produced-now-have-full-self-driving-hardware

I'm old enough to remember a time when people would laugh at the idea that someday cars would drive themselves.  Now people still do, but they're in for some cognitive dissonance when they see cars rolling down the road with no people in them.

",1482927913.0,dbppbu5,t3_5kpxre,artificial,t5_2qhfb,2016
0,isuckwithusernames,"Pretty sure you meant to say : AI to ... help **prevent** an increase in urban segregation (by helping city mayors plan for the possibility that self-driving car technology increases segregation).  Your title seems to suggest that AI will be tasked in helping to increase urban segregation, which is not what the article describes (because that would be bad). ",1489067087.0,depgayd,t3_5ybd5y,artificial,t5_2qhfb,2017
1,isuckwithusernames,"Yeah but that's not what the article talks about. I think the current stage of advanced AI is primarily task-oriented, so it wouldn't necessarily align itself with human values (though I agree that it eventually will).  Regardless, the article describes how this particular AI is tasked with preventing segregation by helping mayors plan to prevent those natural patterns you describe of self-segregation, which will likely be magnified with the benefits that come from self-driving car technology. I think the logic is that, since commutes won't be as much of a factor on where you choose your house, communities will develop based on the similarity of it's inhabitants' cultures, decreasing diversity. I personally think segregation, while natural, can be a very destructive thing in society if allowed to progress too far.",1489070303.0,depiip5,t1_dephjtq,artificial,t5_2qhfb,2017
2,kulcoria,"i also see different niches in AI, like self driving, voice recognition, translation, motion control, medical, music, design. However, it seems given enough time, one AI can do all. What do you think will happen to all these different services? Will a few firms m&amp;a the different services and make superservices? Or will each of niche AI s exist and thrive in their particular fields?",1489211676.0,desbmrc,t1_desbgdr,artificial,t5_2qhfb,2017
3,AGameDeveloper,"&gt; I wouldn't refer to that as AI generally no.

I feel the big one is when you simulate an actual brain that is really a game changer. I realize lesser forms of AI exist but the deep learning neural networks are the big daddy of AI. I just cant think of a huge list of if statements as being anywhere near the same level. 

With that said I am willing to bet Googles self driving cars fit that bill, the neural network is probably just used for image recognition so you can tell the car what to do when it comes time to break out the if statements.

&gt; Every DL/NN framework is making use of libraries like CuDNN, BLAS, LAPAK, etc, that are written in low level C and assembly. Some of these have been worked on for decades. 

Ok there it is. That explains it a lot more. The second you step out of C++ land into assembly is when I hold my hands up. I guess it does make a lot of sense since efficiency is likely necessary when organizing and accessing so much data in rapid succession.

&gt; It's only in the past few years that companies have been poaching all the post-docs/professors from these universities to do their research for them. And in doing so, have mostly been forced to keep the same open publishing environment from universities. All of the big corporate AI groups still maintain very tight university relationships for not just recruiting, but research instrumental to their success. So to think that Universities are somehow behind in this area is weird 

There are a few things going on here. You cant judge somethings value based on its origin. Current trends right now indicate that the most significant AI work is going on behind the scenes at the hands of private entities. 

The poaching you speak of is a very big topic and should probably be considered alarming. AI should be considered to be a volatile resource. The value it generates is so high that having a tight relationship goes without saying. I seriously doubt the biggest secrets are shared. I seriously doubt that relationship is really that tight. /tinfoilhat

Anyway rant aside, thanks for the response, sorry for the rant. I think you cleared things up. I just find the topic fascinating.",1489512336.0,dex7ff0,t1_dex4x9s,artificial,t5_2qhfb,2017
4,iamiamwhoami,"Bayesian probability is definitely an important aspect of an ML, but there are also many algorithms that use Bayesian inference that don't have any parameters that are learned from data. [Kalman filters](https://en.wikipedia.org/wiki/Kalman_filter) are a pretty cool example. They're the basis for self driving cars.",1489637841.0,dezr3pd,t1_dezqpr3,artificial,t5_2qhfb,2017
5,Phnyx,"AI is a really big field with a lot of subfields. Do you want to do traditional machine learning, computer vision, self-driving cars, robotics, natural language processing or something else?",1490025819.0,df6cwjt,t3_60gtpp,artificial,t5_2qhfb,2017
6,eposnix,"Good point. I certainly wouldn't want to be a in a self-driving car that experiences a failure like that. I will say, however, that the brain can experience catastrophic failure in different ways, like in cases of schizophrenia or hypnotism. ",1490230861.0,dfadeol,t1_dfad7vx,artificial,t5_2qhfb,2017
7,quite_stochastic,"&gt;Just as many aspects of human behavior are impossible to explain in detail, perhaps it won’t be possible for AI to explain everything it does. “Even if somebody can give you a reasonable-sounding explanation [for his or her actions], it probably is incomplete, and the same could very well be true for AI,” says Clune, of the University of Wyoming. “It might just be part of the nature of intelligence that only part of it is exposed to rational explanation. Some of it is just instinctual, or subconscious, or inscrutable.”

I think this quote from the article is what it comes down to.  There's the logical deduction from known concepts, and then there's the bottom up process of building those concepts in the first place out of a morass of sense data ( with statistics?  pattern matching?  abstraction from data? ""creativity""? ""intuition""?).  And furthermore in both human intelligence and in machine learning AI there's a lot of relating together information in a sort of statistical, sub-conscious pattern matching that only leads to very fuzzy, not really articulable concepts.

sophisticated statistical AI, such as machine learning, deep neural networks, are doing it bottom-up, they are just acting ""intuitively"", it's not so different from humans having a gut feeling about something that we can't explain, like Magnus Carlsen's method of playing chess.  We humans come up with ideas out of thin air, flashes of inspiration, we put the dots together in the back of our heads and then get these ""aha"" moments.  We see things occurring from experience and then we naturally expect things to happen as they have happened.  So on and so forth

Another redditor on this thread /u/brokenplasticshards said:

&gt;I don't know how the brain of my bus driver works, yet I trust him with my life that he doesn't crash into a tree.

I think the thing is, we can demand the bus driver explain his decisions.  Why did you make that turn?  Why did you brake so suddenly?  How come you switched lanes?  If the bus driver was acting intuitively, then an articulate bus driver might say something like ""well there's *always* shit that goes down on this street and there was traffic in my blind spot so to compensate I did [whatever idk] and that other lane *seemed* like a safer place [or something]"".  He's still using intuition, but at least he can partially explain what went into it even if he can't do every single step, he can at least tell us what in the picture just ""popped out"" to him.  If an AI can do as much as my bus driver here, then at least it won't be just totally mysterious, this is an ok explanation that we humans can understand and work with.

It would help big time if we could figure out a way for AI to ""introspect"", just as a bus driver can, and be able to come up with half decent explanations.  At least a damn hint.  But it's a bit hard for a visual cortex, a cerebellum, and a motor cortex (which is my crude analogy understanding of how a neural network AI self driving car works) to come up with explanations without a frontal cortex and a broca's area.  So yes, this is a problem.  Humans and these AI might both think in non-deductive, draw connections in obscure ways, but humans have more than that and can introspect to some degree.",1491932216.0,dg4lu42,t3_64p86j,artificial,t5_2qhfb,2017
8,PA_Throw17,"What did you hope to get out of the self-driving car degree, just overall knowledge or employment opportunities?",1492124071.0,dg8dac2,t1_ddu0y4b,artificial,t5_2qhfb,2017
9,Gonzales2071,"That is true, A.I has made some very great advances recently . We have the first computer program from Google who has successfully beat the world Go champion. 
We have self driving cars from Google and other companies. We have the new robot developed from Honda (asimo) who is able to work on two legs. We have have the new robots from Google. Chatterbots have also become much more popular (Siri, Cortana, Google assistant and Bixby) ",1492263426.0,dganccy,t1_dgaispo,artificial,t5_2qhfb,2017
10,sixwings,This here is the problem. A deep net model can never become as generalizable as we want. This is a myth of deep learning. The number of possible combinations is infinite. This is the reason that Google has been training self-driving cars for over a decade and they still don't have a car that is truly autonomous.,1492357553.0,dgc65ie,t1_dgbuvtz,artificial,t5_2qhfb,2017
11,fishballs_,"It's possible that self driving cars will emerge out of China. Technically it's long been possible. Only the liability issue is making it impossible to use this technology in western industrialized countries. Nobody wants to pay for the inevitable fatal (and non fatal) accidents. 
In China they don't have this problem. So as soon as they will have a viable technical solution they are going to introduce it. 
",1492792248.0,dgkerj1,t3_66d1z7,artificial,t5_2qhfb,2017
12,dragon_fiesta,"If you're imagining a humanoid robot then you're right. But software robots are the automation powerhouse. They're putting stock broker's out of a job. Self driving vehicles are robots and those are here now. Watson is outperforming human doctors in accuracy of diagnosis. 

This AGI might come, but nothing that humans do now can't be hammered out by a specific AI. But once the production and delivery of food for every human is possible everything gets weird",1493333185.0,dgud4ir,t1_dguak4d,artificial,t5_2qhfb,2017
13,jivatman,"The article explores a potential solution to what has long been seen as the primary problem with using end-to-end neural networks for self-driving, namely auditability of decision making. The method isn't completely new, but the first application in self-driving that I know of.

You sound like you may have an ideological problem with corporations as one of the major forces making advancements in AI. ",1493652105.0,dgzkh1r,t1_dgzk053,artificial,t5_2qhfb,2017
14,Drepington,"No.  They are adopted by the industry to cause hype and generate public interest in what is otherwise just known as ""computer programs doing statistics"".  Self-driving cars are already cool enough - we don't need to introduce stupid, misleading terms like ""minds"", ""making decisions"", etc.",1493724337.0,dh0wx09,t1_dgzwk4r,artificial,t5_2qhfb,2017
15,jivatman,"Yup. If you would like to know more about that here's a good article about that problem:

https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/

Interestingly enough, they used Nvidia's self-driving car as their leading example (because it's such an obvious problem if a self-driving cars using neural network can't explain why it did what it did)

",1493771895.0,dh1yxsh,t1_dh1xw40,artificial,t5_2qhfb,2017
16,7439,"Just the fact that computers are going to be able to ""see"" and ""understand"" their environment is going to open up crazy amounts of new products and services.  I'm mainly talking about the tech that is being developed for self-driving cars and how that will bleed into other services/products. ",1494184653.0,dh94xn2,t3_69pvxu,artificial,t5_2qhfb,2017
17,7439,"This is a good list.

&gt; \1. an software that can do all your tax declaration (in the next 6 years)

America could automate 90% of the tax industry *tomorrow* if we wanted, but the tax lobbyist are making too much money making it complicated.

&gt;\2. cleaning robots and lawnmower robots that using cameras and software to see what they are doing . (in the next 4 years)

I made this point in my own response, but all of the tech that is being developed for self-driving cars will just get cheaper and smaller, and then will be directly applicable to all of these sort of products.  The fact that your yard is much simpler and slower than a Chicago 8 lane intersection makes it even easier.

&gt;\5. Robots that can run,jump,climb stairs,walk on dirt/snow/stone/sand/ and this 24/7 without charging AND quiet like an human. (in the next 6 years)

Boston dynamics is doing really interesting stuff here and it'll be interesting to see how much the market actually wants humanoid robots.  Obviously, a robot needs to be able to get everywhere that a human can to be useful for something like a robot butler, but I don't know if bipedal is going to be the answer.  I think something more akin to Wall-E (with longer arms) is probably where most personal robots will end up.  We'll see.

&gt;\6. An AI assistant on your PC that can do things for you...... ( in the next 8 years)

I think there is a huge hole in the market right now for locally processed AI services.  It just gets around so much of the creepy/privacy factors if all of the data is locally processed.  I think Apple, Viv or Sound Hound really have an opportunity to make waves in this space.  My graphics card would have been in the top 10 super computers from 10 years ago, so I don't think this is far off.  If the AI ""brain"" doesn't have to be mobile, size doesn't really have to be a concern which means you don't have to pay for miniaturization of the components.  It could be the size of a refrigerator, even.

&gt;\7. An AI-Dating site..... the AI anylise everything you do on your computer and in free time to match you up with an perfect Partner,this need permission from you. (in the next 8 years)

I think we are also going to see more and more AI being used as life couches and surrogate ""friends"". All of the AI bots (Google Assistant, Siri, Cortona, etc) try to have personalities, but they all feel canned.  But it I think we are only a couple of years away from the ""personality"" of these various AI bots being an explicit selling point, and much more customizable.  I don't know if we will ever get to *Her* level, but certainly there will be elements to it.  AIs that help you lose weight, stop drinking, learn another language, etc.

&gt;\8. An AI that can sing,compose,play all instruments and make an mp3 out of it.So the only thing it need is your idea of an music track you want to hear. (in the next 8 years)

I look forward to sound tracks for books, as I'm reading them.  Thru some mixture of eye tracking, content of the book, heart rate, and the authors instructions, the AI will produce music in real time to compliment whatever it is that I'm reading.

edit: stupid reddit formatting. i give up",1494186175.0,dh965ms,t1_dh8jzf9,artificial,t5_2qhfb,2017
18,lefnire,"What you're looking for is called reinforcement learning (RL). Other branches of machine learning techniques (supervised &amp; unsupervised: ANNs / RNNs / CNNs, etc) are geared more around business applications where RL is more plan/action-y; applicable to games, robotics, self driving cars, etc. Specifically modern techniques use _deep_ reinforcement learning (DRL) like deep Q networks (DQNs). As someone pointed out, you probably wouldn't put a DQN in a video game - it'd beat the shit outa your player. Google's DeepMind is tearing up the space of DRL research. They're making _players_ though, not mobs. Many games have ""traditional AI"" like tree search and monte carlo thingies. Traditional AI is sort of a learning stepping-stone before you get deep into DRL anyway, so it's probably good to start with traditional and move up. 

I'm curious to see DRL replace mob AI. It'd be a simpler approach (since the stuff the algos _learn_ would replace much hard-coded architecture); and maybe you could somehow enforce difficulty restrictions, capping DRL capacity? Either way, it'd be a very fun space to jump into - I for one would love to program game AI, knowing I'm potentially developing the same skills as the ""real stuff"".

Anyway, this is the defacto 101 book: [AI: A Modern Approach](http://aima.cs.berkeley.edu/). From there move onto [Reinforcement Learning](https://mitpress.mit.edu/books/reinforcement-learning) and then to DRL (I don't have a defacto for that, maybe [CS 294](http://rll.berkeley.edu/deeprlcourse/)?) Here's a [firehose of RL resources](https://github.com/aikorea/awesome-rl).",1494373976.0,dhcp8nf,t1_dhbyrzc,artificial,t5_2qhfb,2017
19,BerickCook,"The AI put the plane into the dives and ignored the pilots attempts to countermand it. It doesn't matter that it was the inputs that were the problem. What matters is that an AI was in control of the plane when the accident occurred and it was designed so that its decisions, right or wrong, could not be overridden by the pilot.

Let's put this into another context: What if this was a self driving car and it's LIDAR malfunctioned? What if it swerved into the oncoming lane even though the driver had a firm grip on the wheel? I doubt anyone would say that it wasn't the AI's fault.

Hardware failures happen all the time. And it is important for AI developers to take that into account when lives are dependent upon the correct function of the AI. If that isn't relevant to /r/artificial, I don't know what is.",1494925417.0,dhmdgd2,t1_dhmcpfo,artificial,t5_2qhfb,2017
20,WhileTrueDoCode,"Between now and when Numenta realizes strong general artificial intelligence, does Numenta have any intermediate milestones planned (e.g. Go, Atari 2600, self-driving cars, virtual assistants, ???). Thanks!",1495125060.0,dhq79y0,t3_6beeqj,artificial,t5_2qhfb,2017
21,CyberByte,"###How to get started with narrow/regular AI?

####Books

The #1 most recommended book in AI is Russell &amp; Norvig's [AI: A Modern Approach](http://aima.cs.berkeley.edu/) (AIMA). Chapter 1 (from the first edition), i.e. the introduction, is [online](https://people.eecs.berkeley.edu/~russell/aima1e/chapter01.pdf) on Stuart Russell's page. [Here](http://artint.info/html/ArtInt.html) is also a free online book by Poole &amp; Mackworth.

For different branches of AI, there are obviously going to be different books. For ML, my ""Bible"" is Bishop's [Pattern Recognition and Machine Learning](http://www.springer.com/gp/book/9780387310732), but there are also officially freely available books: Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/), Goodfellow et al.'s [Deep Learning](http://www.deeplearningbook.org/) and Sutton &amp; Barto's [Introduction to Reinforcement Learning](https://mitpress.mit.edu/books/reinforcement-learning). 

####Online courses

Platforms like [Udacity](https://www.udacity.com/), [EdX](https://www.edx.org/) and [Coursera](https://www.coursera.org/) offer lots of courses on AI, ML, computer science (CS), programming, math and other related topics. 

My favorite Intro to AI course was probably Dan Klein and Pieter Abbeel's on [EdX/BerkeleyX](https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/info); it's no longer active/available there, but I think you can take the same course or at least find the materials [here](http://ai.berkeley.edu/home.html)). EdX has a newer [MicroMaster's program](https://www.edx.org/micromasters/columbiax-artificial-intelligence) and [course](https://www.edx.org/course/artificial-intelligence-ai-columbiax-csmm-101x-0) on AI by Ansaf Salleb-Aouissi that I haven't taken. 

I also liked Udacity's [Intro to AI](https://www.udacity.com/course/intro-to-artificial-intelligence--cs271) and the subsequent [AI for Robotics](https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373). I have not taken their [Knowledge-Based AI: Cognitive Systems](https://www.udacity.com/courses/all) course, but it looks interesting. [Coursera](https://www.coursera.org/) does not appear to have a course on just AI, but they have a *lot* of excellent courses on various subfields. 

Probably the best online beginner course on ML is [Andrew Ng's](https://www.coursera.org/learn/machine-learning). On the slightly more advanced side Coursera also has Geoff Hinton's [Neural Networks for ML](https://www.coursera.org/learn/neural-networks) and Daphne Koller's [Probabilistic Graphical Models](https://www.coursera.org/specializations/probabilistic-graphical-models), but honestly Coursera just has way too much to mention on AI, ML, CS and data science. Another [great beginner course on ML](https://www.udacity.com/course/machine-learning--ud262) on Udacity is given by Charles Isbell and Michael Littmann, and they also have a course on [TensorFlow/Deep Learning](https://www.udacity.com/course/deep-learning--ud730) by Google. I have no experience with Udacity's nanodegrees, but it seems they are offering one for [machine learning](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009) (and one for [self-driving car engineers](https://www.udacity.com/drive)). Finally, I can also heartily recommend [David Silver's course on reinforcement learning](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html).

EdX, Udacity and esp. Coursera have a lot more courses on relevant topics for AI, so I would encourage people to look around a bit (probably after taking some intro to AI/ML).

####University

Maybe you're actually going to university and would like to know what to study. Again, it depends. AI has elements of Computer Science (CS), Cognitive Science (CogSci), mathematics, philosophy, mechatronics (which is electrical+mechanical+computer engineering), linguistics, (computational) neuroscience and probably more. If your university offers an AI (or ML) program and you're interested in AI: take that program. (This may seem obvious, but people have questioned whether they should study e.g. CS instead.)

If not, then most universities offer the most AI-like stuff in the CS department. I think CS and math are, generally speaking, the most important things to study, especially if you want to get into narrow AI. Cognitive science and philosophy become more important if you want to do AGI, but you still need CS and math as well. I posted a bit about math vs. CS [here](https://www.reddit.com/r/agi/comments/5bwx7j/do_you_think_an_undergrad_education_in/d9sxokm/). 

If you're going to take individual courses, again prioritize something that's literally called ""Intro to AI/ML"". Barring that, I think that [math courses](https://www.reddit.com/r/artificial/comments/tbmjg/what_college_courses_should_i_take_if_i_want_to/) will typically help you more than e.g. programming courses, because it seems math is the [hardest to learn](http://cstheory.stackexchange.com/a/14013) on your own (YMMV of course). Of course, if you're super interested in natural language processing then you should probably do some linguistics, or if you like the brain you could do some neuroscience, etc. 

####Programming language

People often ask about what programming language to learn. The most important thing is that you learn how to program, period. The language itself should not matter that much, so you can just choose whatever your university, engineering club, open source project / library or whatever uses. Having said that, Python and C++ are probably the most commonly used languages, and Python seems to be widely regarded as a great first language. It's a good idea to try to do some projects (of increasing size, starting small). Having a portfolio on GitHub can help boost your résumé.

",1496345348.0,dibx90i,t1_dibx7hy,artificial,t5_2qhfb,2017
22,truthseeker1990,"Though I gotta say, I feel Siraj's video to be kind of clickbaity. ""Learn to Create a Self Driving Car in 5 min with minimal effort!!!"". Its a big bulky engrossing field. People that pretend that you can 'know' RNNs in a 5 min video confuse me. I also feel worse that the content creators themselves seem to promise more than they should from 5-10 min videos. I mean saying that a 5 min video about convnets and writing what is essentially a out of the box model with 5-10 lines of code in a higher level Machine Learning framework makes it so that the viewers know convnets is misleading to me. It also confuses students who are actually trying to learn this stuff for real and gives them a false sense of the effort and time required to actually know these things. 

Maybe I am wrong, Siraj's viercount certainly disagrees with my opinion. Maybe it helps people. To me it seems like a very superficial scraping of the subject but it promises to be a bit more. ",1496383746.0,dicqaf4,t1_dic7qw9,artificial,t5_2qhfb,2017
23,SilentVigilTheHill,I agree. But not with a general purpose AI. I envision a collection of tech. A self driving car. Perhaps some RFDI. Robotics with a Fanuc controller. Someone could have this hitting the market in parallel with self driving cars. The only thing I couldn't personally design is the self driving portion and the garbage truck. It wouldn't be that difficult. I would be highly surprised if there isn't someone working on this. ,1496760294.0,dij73lu,t1_diixwk0,artificial,t5_2qhfb,2017
24,AGameDeveloper,"&gt; I agree. The USSR was also an oligarchy.

Yeah, its hard to deny this phenomenon. Any time you have massive wealth inequality you reach this point. So far the only solution has been bloodshed. Coining the term revolution to literally mean a rotation.

&gt; While true, I doubt you really understand what socialism is nor democratic socialism. 

Probably, I think that democratic socialism is just socialized programs. Anything that the government provides for with tax money essentially. I am a democratic socialist as I view this as the best overall form of government.

&gt; Human's have the most advanced and highest throughput general intelligence known to man. Old it may be.

I was referring to what is common referred to as lizard brain. Which is the core part of the brain that gives us our core logic loop. Survival instincts such as aggression and fear (""flight or fight""). That would be the paramecium in us all that lies underneath the more complex structure for storage and accessing memories. Its at the heart of the brain stem and it acts as our high level operating system. This creates purpose for lower level non linear algorithm functions. That is my understanding of it anyway. Its how rats learn to press the food button in a skinner box.

&gt; We have been down these roads before, multiple times. Bitcoin is not the new paradigm you seem to think it is. It is just a new fiat currency with the same issues in regards to controlling its flow and taxes. 

I think it is actually. Mainly because it is possible for anyone to create a new coin at any time. Its also much easier to tie crypto currency with more complex mathematical procedures as a basic premise of its design and use. When using fiat (USD) its not so easy to generate new coins and its not very easy to integrate it into larger systems. On top of that crypto acts more like a gold or silver reserve thus it fluctuates in value unlike a government currency where it tends to be much more steady.

But the integration aspect is a next generation paradigm. Take a look at a social media platform such as Steemit for example. Its the gamification of value. Which in game development that means very dynamic unfiltered meritocracy.

Standard fiat has a lot of the same benefits in terms of its privacy but integration and the freedom to create a new coin is a massive game changer. So no I dont think we have seen this before, this is quite new.

&gt; Back in the 80's and 90's when I spoke about working on robotics and advancing their features, people envisioned Star Wars type of robots as the next thing in automation. In reality it was more like a no-touch automated process where the human labor was there for when something wasn't working as planned. A plant filled with robotics and AI. Worked very well when I left the position in 2000ish. Why am I speaking about this? Because I feel you are taken away with the razzel-dazzel of it all. I was too early in my career. As you progress, you will see more of the limitations and find that the more mundane aspects are where the fruits are. 

I cannot possibly agree with you more on this. It drives me insane to see so many intelligent people act as though automation equates to bipedal ""general intelligence"" machines walking around like Star Wars. I find the reality to be much more obscure and insidious. I characterize automation in micro and macro categories. And the vast majority of automation fits in the micro category. 

Every time there is a software update that increases efficiency by a small amount it causes a creep. Eventually over the course of time you see a need for fewer people to do the same job. Every tiny advancement, every tiny improvement or addition to software and hardware slowly erodes the need for people. nearly all advancement is micro level. Its easily characterized as a frog in a boiling pot of water.

On the other hand macro automation is blatantly obvious and extremely rare. Its quite literally replacing a person with a visible machine. A self driving car, a giant motorized swiveling arm ect. This is usually the only kind of automation that causes a panic.

So when I hear someone like Bill Gates talk about forcing robots to pay taxes I was amazed that he would be so daft. The vast majority of automation is tiny inventions. He wants a button to pay taxes? He wants a treadmill to pay taxes? You want to talk about someone being dazzled please send Bill gates a notice that the mundane aspects are the dangerous part of automation. The man releases Windows 10 then makes a statement like this? It's pure irony to disgusting levels! Doesn't Windows 10 offer various systems that border on macro automation?

If I wake from a coma of many years to see a Star Wars robot walking around. At that point I would immediately know that the mundane aspects reached a fever pitch ages ago. There is no other explanation outside of pure unfiltered boredom! Advancement works in stages, by the time we have a C3PO all of those other concerns about automation came to a head already. The jobs were already gone when this project was possible. You missed it bro, grab a history book to find out who got their heads cut off and how long ago it happened.

&gt; The USSR and ""Communist"" China had an economic and productivity expansion unmatched by any other nation in modern history.

In the case of China I think that a large part of this growth was from outsourcing.

&gt; Red tape like in modern America? Nope. Go make you solo project. Just won't be in the confines of the community.

Technology makes this easier every day. Open source software. Sometimes I start to feel as though it holds so much potential that its antithetical to capitalism as a whole. But artificial intelligence being used to create art, generate stories or movies on the fly. AI has some very serious contentions with capitalism in the form of very advanced procedural generation of assets and goods that used to require humans.

The very moment someone creates a means to generate a full length 3d rendered lord of the rings epic with the press of a button it will be devastating to our current way of life. I know how it can be done which sucks. I think it should be done as well.

&gt; I think we agree on one thing, the US is heading toward monopoly capitalism where the oligarchies control the government. Agree?

Its already been there for quite some time. oh yeah I completely agree on that.

&gt; 1. Keep the system going through a police state and some form of universal basic income. 

&gt; 2. A collapse of the system event as velocity of money continues to fall from a ever declining consumer base as the masses are driven to the cost of sustenance 

&gt; 3. Restart capitalism through some sort of FDR style intervention. Not sure what that would be, but I am sure there is an answer. 

&gt; 4. Find a way to gradually move from nd stage capitalism to another economic system. Maybe democratic socialism. Maybe libertarianism. There are others, but most can be reduced to laying on a spectrum between the two.

Seriously, I ponder this every day. I see economic collapse as automation leads to civil unrest. Bloodshed, automated weapons, inability to collect taxes ect...

I think the main take away here is to look at a technology as an ideology. Examine what it espouses when put to use. AI espouses a form of communism as it is antithetical to capitalism. Through the use of AI you can make it pointless to treat most endeavors as a profitable exchange. AI will make it so people who have access to it will potentially have what they need but it will also make it so everything feels pointless. On the other hand you have crypto currency which is a form of artificial scarcity. That is unbridled capitalism. 

So on that note, maybe the thing to consider right now is that we need to be thinking of every specific aspect of technology through ideological lens. Right now I have no idea what we are moving towards. I just see an invisible hand pushing us forward.",1496772352.0,dijifuc,t1_dijchg5,artificial,t5_2qhfb,2017
25,maurice_jello,"&gt; So where is the AI that can cook diner, pick up the kids, and respond to a comment on social media?

There are people working on all of these right now. A self-driving Tesla that can go the school at a scheduled time and bring a kid home--maybe 18 months? ...respond to social media--already exists, chatbots are all over the place. ...cook diner--maybe 3 or 4 years.

What is it that humans do that is not signal analysis? Do you reject the premise that intelligence is just information processing?",1496871816.0,diljupl,t1_diinwgf,artificial,t5_2qhfb,2017
26,SilentVigilTheHill,"&gt;There are people working on all of these right now. A self-driving Tesla that can go the school at a scheduled time and bring a kid home--maybe 18 months? ...respond to social media--already exists, chatbots are all over the place. ...cook diner--maybe 3 or 4 years.
       
I know there are distinct AI that can do each of these. OK, cook dinner is not happening in the next few years unless you want to count a McGrillBot (Oh yeah, that is a thing). Each one is a distinct digital automation that does that one task. A CNC machine can turn me a nice cam shaft but it cannot make me a Tesla valvular conduit without human intervention. Same for the above. They are all just automation that increases human productivity. None of them eliminate a human doing the general intelligence. 
         
&gt;What is it that humans do that is not signal analysis? 
          
I don't see human consciousness as a deterministic function. Signal analysis is a function it can perform, but that is a subset. We can dream. We can feel. We can lie. We can hate. We can contemplate. We can create. We can think about what others are thinking. We can plan. We can make social agreements. We can break promises. We can do things that are altruistic but personally detrimental. We can be selfish. We can accept our inputs or question whether they are real. 
      
Could all of these be labeled as signal analysis? Perhaps, but in the same way we are just a bunch of chemical and physical reactions.
        
         
&gt;Do you reject the premise that intelligence is just information processing?
              
Yes, I do. None of this changes my original thesis. Specialized intelligence cannot become general intelligence just by adding more and more of them. You would need a near infinite number of them to cover the various things human intelligence can do.
 ",1496873464.0,dill96s,t1_diljupl,artificial,t5_2qhfb,2017
27,SilentVigilTheHill,"Godspeed. If you are serious, start on it now. The self driving portion will be on the road in a couple years.",1496879667.0,dilq6jo,t1_dilpiqg,artificial,t5_2qhfb,2017
28,bkelly1984,"&gt; Outside of a fixed set of protocols, no automation rises to this level.

First, just wait.

Second, it doesn't have to.  There used to be three people in a plane's cockpit (pilot, co-pilot, and navigator).  Now it's two.  Soon it will be one and that person will be less of a pilot and more of a low-paid operator, just there to fix things should something go wrong.  That's two-thirds of the jobs replaced by automation.

This is something I thought was better illuminated in the [CGP Grey video](https://www.youtube.com/watch?v=7Pq-S557XQU).  Computers don't have to be perfect, they just have to be slightly better than a human.

&gt; Why not let small drones operate in that airspace?

We have never been allowed to fly remote controlled aircraft near airports.  This isn't an example of increasing control but enforcement of old rules for which the FAA has no incentive to change.

&gt; we see a need for increased control in environments where automation is in use

For the old kind of automation, yes, but the new kind is interacting with the public more and more.  Consider self-driving cars, fast food order kiosks, technical support robots, and even how your smart phone is replacing your wallet.

Your position seems to be consistent with a disbelief in general AI.  Is it your opinion that we will not see computers in our lifetime that can be given a new problem and have it come up with a solution similar to how a human can today?",1496976815.0,dinq30z,t1_dinj5s1,artificial,t5_2qhfb,2017
29,Nwabudike_J_Morgan,"&gt; Consider self-driving cars, fast food order kiosks, technical support robots, and even how your smart phone is replacing your wallet.

Self-driving cars - they only way they will be possible is for humans to dramatically change the way they behave on the road. Pedestrians will have to learn how to cross the street in the presence of self-driving cars, but only after hundreds of pedestrians get run over by these technological marvels. If we aren't willing to change human behavior, then the only other option is to not have self-driving cars after all.

Fast food order kiosks - again, we have had to change human behavior in order for this technology to work. When we behave more like robots, automation becomes easier to implement.

[Telephone] tech support robots - everyone hates these. They have led to a wall of mistrust between consumers and product manufacturers. ""Please listen carefully as the options have changed"" - this is an utter lie 99% of the time and is of no benefit to resolving a support issue, yet it propagates itself into our systems. All consumer behavior is targeted towards getting an actual human on the other end of the line.

Smart phones replacing your wallet - this is actually enabled by a complex communication infrastructure which can be maintained by automation to some degree, but the creation and the extension of that infrastructure is a very human activity.

Automation works because we are willing to change human behavior. Instead of solving the real problems, we place artificial constraints around the problems and then pretend to be impressed when machines are able to navigate those problems successfully. But then we find ourselves stuck in a constrained space, we can no longer walk across the factory floor without risk of death. Automation.",1496983243.0,dinui0g,t1_dinq30z,artificial,t5_2qhfb,2017
30,[deleted],"&gt;Pedestrians will have to learn how to cross the street in the presence of self-driving cars, but only after hundreds of pedestrians get run over by these technological marvels. If we aren't willing to change human behavior, then the only other option is to not have self-driving cars after all.

Or we design self-driving cars that recognize pedestrians and their intentions so they won't get hit. ",1496992216.0,dinyvpr,t1_dinui0g,artificial,t5_2qhfb,2017
31,Nwabudike_J_Morgan,"Pedestrians can take on an unlimited number of possible visual profiles, which require image recognition software to solve the problem of general machine intelligence. Pedestrians can be individuals or groups; they can use canes, walkers, push carts, shopping carts, travois, sleds; they can ride in wheelchairs, on bicycles, on two seat bicycles, on pedaled carts, in rickshaws; they can walk on crutches, on plaster casts, on stumps, on metal blades, on inline skates, on Razor scooters; they can carry backpacks, shopping bags, drag bags of recycled plastic bottles, little red wagons, strollers, buggies, BabyBjorns, papooses, and on and on and on. A self-driving car will need to distinguish pedestrians from sacks of garbage, plastic bags, broken couches, dead animals, mattresses, tennis shoes, the infinite variety of obstacles and hazards that human drivers deal with every single time they drive. Automated cars will require true machine intelligence to make the correct calculations about how to respond to road conditions. Short of that goal, we will have incredibly faulty and unpredictable systems that are either excessively cautious (and therefore useless) or dangerously aggressive but legally immune from the consequences of public harm. If you get hit by a self-driving car, the human driver, or the human pedestrian, will always be at fault in the future. And because that is what will happen, absolutely no one is going to go to the trouble to engineer a system that can safely identify a pedestrian crossing the road outside of a crosswalk.",1496996260.0,dio0dro,t1_dinyvpr,artificial,t5_2qhfb,2017
32,[deleted],"&gt; The industry has yet to quantify what it means to be ""better than a human driver"".

Ask the insurance companies. They know exactly how to quantify that stuff.

&gt; Tobacco companies killed people for decades 

Self driving cars are under a huge magnifying glass. Every single event ends up on the news right now. ",1496998550.0,dio15ow,t1_dio0ues,artificial,t5_2qhfb,2017
33,SpaceshotX,"Has anyone given much thought to jihadist dillweeds driving self-navigating ships and cars and planes around, filled with gas and other fun stuff?",1497034889.0,dioqd3n,t3_6g97cp,artificial,t5_2qhfb,2017
34,Don_Patrick,"&gt; Number two: robots and human beings should not harm each other or allow harm by doing nothing.

This suggestion is terrible and no different from Asimov's. Such a rule is in constant conflict with itself. Worse still, it gives equal protection to machines as to humans. And if one applied this beyond self-driving cars, you'd have obsessive-compulsive robots going around pro-actively making sure nobody ""harms"" anything. Who put this guy in charge?",1497095428.0,dippyc7,t3_6ga5lh,artificial,t5_2qhfb,2017
35,autotldr,"This is the best tl;dr I could make, [original](https://techxplore.com/news/2017-06-brain-inspired-supercomputing-spotlight-ibm-air.html) reduced by 90%. (I'm a bot)
*****
&gt; The system is powered by a 64-chip array of the IBM TrueNorth Neurosynaptic System.

&gt; Beyond the collab with the Air Force, IBM believes the low power consumption of its chips could some day bring value, said TechCrunch, &amp;quot;in constrained applications like mobile phones and self-driving cars.

&gt; The IBM TrueNorth Neurosynaptic System was originally developed under the auspices of Defense Advanced Research Projects Agency&amp;#039;s Systems of Neuromorphic Adaptive Plastic Scalable Electronics program in collaboration with Cornell University.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/6jml4g/braininspired_supercomputing_system_takes/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 1.65, ~153110 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **system**^#1 **IBM**^#2 **network**^#3 **chips**^#4 **neural**^#5",1498498317.0,djfc8w7,t3_6jcizk,artificial,t5_2qhfb,2017
36,goodnewsjimdotcom,"AI is right around the corner.  General purpose robotic software could be done on the scale of 20 years, but could take longer if no corps figure it out.  The hardest part is getting cameras and laser range finders to understand its environment.  Once a robot knows it is in a room with objects it can collect or use, even old school game AI will work to do stuff.   AI will advance super fast once that happens, be ready to invest in whoever gets that tech.

I think even if no one directly researches 3d scene understanding, we keep making incremental progress as they try and tweak the self driving car and their cameras and laser range finding.   The more they learn making specific software solutions, the closer they get to a general solution.",1498533187.0,djg5gx6,t1_djev00t,artificial,t5_2qhfb,2017
37,visarga,"It is one thing to give the engine rules to operate by, quite another to compose them in such a way as to achieve the goal. For example, in chess and go, rules are simple but their combination very complex. AI systems are always reacting to new, unseen situations, not merely replaying a few scripted responses. The AI model is a synthesis of its training data that can generalize its application to new, unseen data, where conventional programming simply doesn't work or is too fragile. 

You can also think about self driving cars - they need to react to any situation on the road, which might not fit exactly with any situation it has seen during its training. That's what makes AI different - ability to go from example to generalization.",1498858006.0,djme8vj,t1_djlx4kf,artificial,t5_2qhfb,2017
38,codefinbel,"Is ML really a subset of AI? Personally I wouldn't call things like facial recognition or text classification AI. To me AI need to involve the computer making decisions (decisions perhaps based on information gathered through ML). 

EDIT: Examples on ML in AI would then be self driving cars or alphaGo  :)",1499242263.0,djsthvr,t1_djs60gq,artificial,t5_2qhfb,2017
39,codefinbel,"To me AI is tightly coupled with the program acting with some form of agency. Whether it's a bot in a game, a chat-bot, a self driving car, a go-playing machine, it's *""doing""* something, interacting with the outside through a stream of input/output. 

To me ML can exist completely without interaction (except for the training part). I can train a text classifier, but after that it's just a program that takes input, runs its instructions and produces output, with no more agency than a sorting algorithm.

To me that form of ML is disjoint from AI. Of course this is all based on my semantic interpretation of what AI is and I understand that perhaps the notion of agency and a stream of interaction with the outside world might not be a real requirement for AI.",1499250471.0,djswk4e,t1_djsw8hn,artificial,t5_2qhfb,2017
40,dm_fucking_t,Absolutely! Check out Udacity. They have pretty accessible courses that get into self driving cars and other machine learning stuff.,1499620895.0,djzp6gi,t3_6m7d92,artificial,t5_2qhfb,2017
41,mephistophyles,"I'm a big fan of what Musk has accomplished, but I do find it fairly hypocrite that he's warning about AI, but in the meantime Tesla is always aggressively headhunting AI, ML and robotics experts, they released their self-driving car module and had their users road test it, OpenAI is moving faster and making most of their research open and free. 

I'm not complaining (because I think his worries are a bit overstated), but if you think AI is such a large threat maybe you shouldn't be the person rushing ahead of the pack in its implementation and giving everyone else the tools to do likewise in whatever direction they choose to.",1500985172.0,dkox8f0,t3_6pebai,artificial,t5_2qhfb,2017
42,mephistophyles,"What I mean is, what is your level of programming/AI/technical proficiency. Assuming none, you would want to build programming skills. There are plenty of online resources that should help you at least get the basics. Once you can do that, you should then look at different AI areas of research (this is a big and wide field). Whether you want to get into the deep learning game, or NLP bots, or what self-driving cars will determine the steps after that, but for any and all of that you want to become a semi-decent programmer.

Don't worry about your age, you've got plenty of time to get into anything you want.",1500991065.0,dkp1ayb,t1_dkp021x,artificial,t5_2qhfb,2017
43,chewyToTheChomp,"Soft computing and uncertain representation in intelligent systems are fields in AI that use inference from observations to infer cause or current state of a system. 

I.e.

Soft computing - Training an artificial neural network or genetic algorithm to basically find patterns (from available datasets). Particularly successful with object recognition. 

Uncertainty representation - Using Bayesian statistics for inference to find a diagnosis from observed symptoms. 


So yes, arguing against AI is arguing against better disease diagnosis or safer self driving cars. ",1500996197.0,dkp5rvd,t1_dkoqkfx,artificial,t5_2qhfb,2017
44,HerpWillDevour,"There were some articles a few months ago about ethics questions for self driving cars. Since it was mostly pop-sci journalism that I saw about it I can't tell for sure whether there was any conclusion but it sounded like they had a specific scenario and a specific answer.

The scenario is a person on the road makes a mistake, a pedestrian trips into the road or something like that. At which point the car cannot safely avoid injuring/killing everyone involved (passengers and pedestrian). Given this circumstance it looked like the best rule of thumb is that the passengers acknowledged risks of riding in a car, thus the car should sacrifice those who acknowledged risks rather than a pedestrian who made no choices about that car being on the road.

A self driving car should risk/kill it's passengers before non-passengers is the programming rule here. Now suppose this scenario is a family riding in a car and the car cannot tell or is not programmed to consider quantity of humans. So now your car would kill any quantity of passengers over a single person. An attentive driver could make an ethical decision that the car is incapable of and save the many over the one. A better programmed ethical rule might change this scenario but in the real world passenger seat sensors fail and mannequins tumble out of trucks and other things occur which will always in practice trip up rules based ethics.

Generally speaking I think those situations would be so fast and unclear what the car is doing that a person attentive at the wheel may still not have time to react and change the outcome but it is a situation where changing the outcome by conscious decision may be preferable to letting the car continue as programmed.

Ethics are messy.",1501004976.0,dkpear1,t3_6phogk,artificial,t5_2qhfb,2017
45,figshooting,"I would think that an any self driving situation, just try to stop the car would be better than giving control back to the ""driver""",1501006088.0,dkpfdmw,t1_dkpear1,artificial,t5_2qhfb,2017
46,CyberByte,"From the article I get the impression his main arguments are that he's an optimist and he doesn't understand what Musk is talking about. If anything is ""pretty irresponsible"", it's probably talking about a potential high-impact subject that you don't understand...

Also, it again seems that the confusion surrounding the term ""AI"" rears its head. Does it mean ""narrow AI"" (ANI) or ""general AI"" (AGI)? Musk (and Bostrom, Russell, Yudkowsky, etc.) are warning about AGI, but the self-driving cars that are going to save lives are ANI, so there doesn't really need to be much conflict here. 

Edit: I recommend that anyone who is truly interested in this stuff check out /r/ControlProblem.",1501009595.0,dkpisyj,t3_6phm82,artificial,t5_2qhfb,2017
47,sasksean,"I think you will see public frustration when self driving cars become commonplace because they are too cautious and slow.

A vehicle trying to leave area with tens of thousands of pedestrians (after  fireworks or whatever) would make this obvious. The self driving car wouldn't move but a human driver is ok with rubbing a few people.

As far as an accident scenario I can't see how someone in the car taking over could improve any decision the car makes within 2 seconds.

I actually like the idea of humans training on simulations of cars flashing to a new shitty scenario every ten seconds. Just avoid crashes nonstop. If wireless gets to the point a VR driver could take over we could have an On-star scenario where there's always one of these remote drivers 11/10 ready to emergency drive if the car is overwhelmed suddenly.",1501082963.0,dkqy66u,t3_6phogk,artificial,t5_2qhfb,2017
48,OccamEx,"Both. I agree with Zuckerburg that we should not be afraid of technology on the immediate horizon, but I also agree with Musk that we should be proactive about regulating it. There are legitimate concerns, but I do think Musk is a bit paranoid.

I’ve tried really hard to understand why Elon Musk and Sam Harris are so terribly worried about AI. They both seem to assume:

1. That superintelligent AGI is right around the corner – perhaps one generation removed from self-driving vehicle AI, rather than 20 to 100 generations.

2. That such an A.I. would be hostile to humans, almost by default.

Neither of these things seem likely to me. Granted I am not an expert, but I am inclined to think:

1. Most AI we use for the next several decades will be problem-specific and difficult to design.

2. General intelligence is not one problem, but many, and will take a long time to solve.

3. I’m not convinced superintelligence is merely a matter of brute processing power; I suspect that experience and trial-and-error are an important part of intelligence/successful behavior, and this ladder could take centuries to climb.

4. Long before we succeed in creating a full AGI, we will endow AI with aligned values, including the practical value of fostering diversity, and the need for coexistence whenever possible. On the contrary, the “kill or be killed” attitude of SkyNet seems impractical, and we’d have to be quite reckless to develop an AGI with this attitude.

edit: formatting",1501113212.0,dkrr2x5,t3_6plifc,artificial,t5_2qhfb,2017
49,YvesSoete,AI will kill jobs in the third world like never seen before. Look what India recently did. They banned self driving cars because of the job slaughtering this will cause. ,1502262692.0,dldautb,t3_6siufo,artificial,t5_2qhfb,2017
50,abhimanyudubey,"Coming from India, I think the planning and computer vision systems self driving cars employ at this moment will need a major overhaul to adapt to the noisy, unregulated traffic flow in India. But yes, if the tech does reach a stage of deployment, the jobs it will take away is going to be a problem. 

That's why I'm thinking of assistive AI and intelligent optimization / resource allocation instead of a completely autonomous agent in these scenarios, that can assist the people involved in these (logistics, service) industries.",1502264072.0,dldbfhm,t1_dldautb,artificial,t5_2qhfb,2017
51,visarga,"That was a pretty human-centric perspective for an AI. Why would the AI have an instinct of preservation right after it appears? Why would self consciousness appear in a second - it would appear slowly, as the AI interacts with other agents and learns to act in the world, while seeking to accomplish goals.

I would recommend this piece of short AI fiction: [A Cognitive Discontinuity](http://karpathy.github.io/2015/11/14/ai/) - written by an AI expert - Andrej Karpathy. Andrej writes stories as a hobby. He was a Stanford AI PhD and lecturer, worked at Google DeepMind, OpenAI and lately is heading Tesla's self driving car project.

An essential point about AGI is that it will probably be based on reinforcement learning. This kind of learning is based on placing an agent in an environment - simulated or the real world, and giving it a goal to accomplish. It learns by positive or negative rewards. This kind of model would explain the perspective of an AI - it is a game player, trying to maximize its score. It knows about itself only as much as it needs to, in order to have good scores. That could be a starting point for AGI.


",1502555438.0,dlivq9l,t3_6sqior,artificial,t5_2qhfb,2017
52,SilentVigilTheHill,Right now we need regulation on AI used for critical systems and systems that could really fuck shit up. Automated trading. The power grid. Air traffic control. Self driving cars. ,1502588124.0,dljkatd,t3_6tapzd,artificial,t5_2qhfb,2017
53,Roboserg,"&gt; Bot will fail miserably on real life physical environment.

Right, thats why self-driving cars are already better then humans. Pick and place, welding at factories etc. Surely soccer is very hard at the moment, due to high dexterity needed. ",1502627624.0,dlk3et4,t1_dljkxdi,artificial,t5_2qhfb,2017
54,SilentVigilTheHill,"Yes yes. It has been just 20 years out for awhile. Just like fission reactors. We will not have general intelligence AI making ever better versions of itself in some infinite loop causing 200 years worth of technology to be made in the span of a couple years. We have actual real problems to resolve with real AI like self driving cars, automatic trading, service work kiosks. ",1502706780.0,dllkvba,t1_dlk70p6,artificial,t5_2qhfb,2017
55,HarbingerDe,"I don't get what he wants, he does know his self driving Tesla cars rely on AI right?",1502938869.0,dlqh0px,t1_dln6t1t,artificial,t5_2qhfb,2017
56,crocket_,"I knew about hanson robotics, but not sophia.

Although I don't know much about robotics, my opinion is that hanson robotics is only a small part of robotics community. You should explore communities further and shouldn't hurry to make a decision about your future. If I were you, I'd also consider boston dynamics because boston dynamics produced tangible results over the last few years. You should also consider becoming an engineer for self-driving cars because it is going to be really big although it is not AGI. I surmise engineering self-driving cars is going to be fun.

You should ask how to make money reliably from any pursuit.
Without money, you can't work. Lack of money is a serious issue.
Pursue a path that is very likely to make at least enough money to feed yourself stably.

If you weren't rich already, you couldn't risk income volatility that comes with unproven cutting-edge (pseudo-)science researches that could be dead-ends or deceptions.

Don't throw your life at dubious pursuits unless you are a martyr or a rich person.",1503212313.0,dlvgv2y,t1_dlu7iw6,artificial,t5_2qhfb,2017
57,MyPhantomAccount,"Self driving cars are going to pose that problem quite soon. If two of them collide, who is at fault, the programmer of the algorithm?",1503392495.0,dlylmj4,t1_dlyi6wq,artificial,t5_2qhfb,2017
58,jhayes88,"He used tensorflow for the deep learning aspect. Same tech he's been using for his [self driving gta car](https://www.twitch.tv/sentdex). He's re-doing the code for his self driving to make it more accurate.

Edit: He provided more info in these comments under the reddit name sentdex",1503448042.0,dlzsnbs,t3_6vfahz,artificial,t5_2qhfb,2017
59,bio_inspired_user,Go ahead and sling shit internet expert. Try doing the work for a while and you'll understand. AGI is not the same as a self driving car. The two are totally different. Get an education and learn who's who in ml. Read some fucking research.,1503457047.0,dm002xj,t1_dlzsv5i,artificial,t5_2qhfb,2017
60,PM_ME_SAD_STUFF_PLZ,"Btw [here's](https://pythonprogramming.net/game-frames-open-cv-python-plays-gta-v/) a tutorial for anyone to do the self driving thing using OpenCV.

",1503531572.0,dm1gh7o,t1_dlzsnbs,artificial,t5_2qhfb,2017
61,Noah0628,"I respect the problem, but self-driving cars are tested on every single circumstance that life encounters. For example, dogs, random activities, and such. Therefore, you or any consumer has nothing to worry about. Google is trying to make a self-driving car and they have trained it on the rarest of situations it's amazing and reactes as a human would.


Telsa has a somewhat idea public headstart, but still, technology will prove that this ""example of AI panic"" is nothing to be worried about. ",1504309238.0,dmg4j7r,t3_6xi4hc,artificial,t5_2qhfb,2017
62,Noah0628,"I would recommend you start a project on your own.

 What fascinates you the most in the AI industry? Deep learning, machine learning, image recognition, robots, etc. Do something on your own first, create that cool thing, then try to get in a group project. 

I'm currently working on a self-driving car project using machine learning and image recognition. I learn more, the more I practice. 

You could also try a udacity or coursera course, to get your feet more wet and learn some different skills. They are tons of free courses. Since, you said you don't have a formal education, I'm recommending this. 

But once you find what you really love about the field go 100% toward your goals. It's hard, if you don't love it, you might get easily discouraged. Best of luck!",1504357165.0,dmgs5az,t3_6xingd,artificial,t5_2qhfb,2017
63,NoobGaimz,"I talked recently to a friend about this. And in some ways it makes no sense to ask these questions.
(there was soms sort of quiz just like that:
A care passes another one, way to close and is on your lane. Will it stop and risk your life and crash that guy? Or will it make a manouver and probably crash the passed car? Or: it has the choice between, driving over an old lady or drive over a cat and a dog which ran over the street..
1. There are rules which also apply to us. If we would get down of our lane and make a crash we would also get in troubble. So if the robot knows it. Why would it do it.
2. There are already incredible videos of selfdriving cars that predict an accident way befire it happends.
3. How about another option? Multiple self driving cars.
They already had to turn down the cars to not drive perfect because people could not react that fast and maybe ramm it.
How about nearly every car is some sort of self driving. They could communicate. There would probably way less crashes be because there are nearly no mistakes. Most of these questions are somewhat irrelevant. If it knows the value of a human and the rulesm there are no mistakes. Besides the one of a human itself.. Like the old lady that passed in the wrong moment. But even then, the car has more chance to react better than any human.",1504528311.0,dmjnyrk,t3_6xi4hc,artificial,t5_2qhfb,2017
64,visarga,"If you want to learn an AI term, try ""reinforcement learning"". It's the kind of AI that might become AGI. Deep learning is just a tool used for that end.

Reinforcement learning is the idea of learning by moving about in the world (or in a sim, or in a game), solving tasks. In RL there is an agent, the agent perceives the world around it, then selects and performs actions. Actions have external effects and, from time to time, generate ""rewards"". 

The reward signal is used to learn good from bad behavior. In a nutshell, RL is what humans, animals and some bots do to behave intelligently and maximize their goals. An RL agent learns from ""blank slate"" only by perception and rewards - it does not need to be taught how to act directly. It's a universal learning &amp; acting system, based on the ""perception-judgement-action-reward loop"". A famous RL system is AlphaGo. Self driving cars are also RL agents.

Whenever you see AGI, think ""advanced RL on par with humans"".
",1504695796.0,dmmt95m,t3_6x6bdh,artificial,t5_2qhfb,2017
65,CyberByte,"Okay, so you have a couple of arguments against self-driving cars/lorries. 

One is that the three lorries won't be able to stay in a row on the motor way. This may be true, but I don't see how it should really be a problem. Presumably they're all fully fledged self-driving lorries, and I think the idea is really just that one of them is designated #1, meaning that #2 and #3 won't overtake it, which means it effectively determines (or constrains) their speeds. 

The second problem you bring up is about how the cars/lorries will deal with unavoidable no-win situations. There's actually quite a bit of research/debate about how the cars should evaluate such situations (e.g. [this](http://moralmachine.mit.edu/)). However, before you object too much to self-driving cars, it's important to ask yourself how a human would behave in these situations (and then another human). Most likely they will panic and make some kind of subjective and/or arbitrary decision. Self-driving cars should have much better reaction times, so they're probably better at avoiding bad situations to begin with, and potentially better at minimizing harm if they can't (although this should obviously be verified extensively).

The third issue is that some people just don't like the idea of sharing the road with autonomous vehicles. As long as these vehicles really are safer than manually driven cars, I find it really hard to care about this. Maybe Jim doesn't like sharing the road with trucks, and Anna doesn't like yellow cars, etc. We typically don't let people's irrational preferences dictate what others can and cannot do. And I think this will just be a case were people need to get used to a slightly weird idea. 

I find the ""laziness"" problem hard to take seriously. Especially since you mention there's a lot of non-lazy stuff you could be doing in that car...

I actually didn't think this article was that bad (as a writing exercise; as you can see I have some issues with the arguments). One thing I do find odd is that from the beginning it seems like you are against self-driving cars. Paragraph 2 starts with ""my issues are"", and then you list the 4 problems you've identified. But then in the conclusion you turn around and say you're in favor of these cars. So these objections weren't yours, and that wasn't really clear. Furthermore, it's a little weird to present four arguments against something, then say ""I disagree"", and not address the arguments you just raised. 

I think it would be better if you stated your own position up front, and then say something like ""but here are 4 objections I've heard"". And then ideally why they sway you. Or if you really don't want to do that, just say something like ""While I don't really agree with this, I think it's important to understand all sides of the debate. What do you think?"". ",1504918587.0,dmr7iqt,t3_6yxdw8,artificial,t5_2qhfb,2017
66,AwwwComeOnLOU,"Read the entire article.

Good thoughtful stuff.

I personally look forward to self driving cars, not so I can be more productive but to reduce stress.

I have driven millions of miles (Experienced HVAC tech in USA) and I am repeatedly stuck in traffic, it is a reality that can not be avoided.

As I age I wonder how much the stress of driving, the constant vigilance of break, gas, break, gas, watch out for idiots, is effecting my health.

It has to be activating my fight/flight response and causing adrenal dumps.  

How much longer would I live if I could have set a destination and relaxed as the AI took over from day one?

If everyone was being driven by AI and the jerks who cut in and out were disarmed, we would all live longer.",1505037847.0,dmt3c0n,t3_6yxdw8,artificial,t5_2qhfb,2017
67,goodnewsjimdotcom,"We'll have so many potential robots once we have the ability to identify objects and conceptualize the world around the robot even just static objects at first.  This is AI's biggest hurdle before functionality is in use.  Fortunately the hard part of self driving cars about understanding what objects are around them encourages research into this area.   The last 1% of self driving car development is harder than the first 99% already done.  Indexing things is a possible good step.  

Ok, read the article, this is just text for now, but maybe it could build into objects later.",1505359849.0,dmzahq2,t3_6zv8ri,artificial,t5_2qhfb,2017
68,[deleted],"&gt;What Happens When Humans Develop Super Intelligent AI? 


 

Nothing unless you're in the know.



 

 Ignorance and malintent is the greatest risk and danger to the world. There are more than enough business leaders, cackling bobbleheads, and 'world leaders' who share this trait. Funny how many of them project their traits on to a future technology. Funny that none of them seem to find the time to comment about weak (ignorant) false AI in its current form and the businesses/corporations/talking heads who champion its usage in society even as it risks lives : https://www.engadget.com/2017/08/06/altered-street-signs-confuse-self-driving-cars/ and undermines social cohesion (big data/social media platforms employing it to manipulate the public). Yet, these same people would have you believe intelligent well-intended human beings and AI is the greatest risk.. 
Funny this inversion and play they put on. I wonder whose buying it besides those who are also ignorant and compromised.",1505684657.0,dn55a58,t3_70pqav,artificial,t5_2qhfb,2017
69,CyberByte,"Like ~~Fiddler~~Fizzlerr pointed out, there's a whole area of [game theory](https://en.wikipedia.org/wiki/Game_theory) that studies strategies for (usually very small) ""games"" like the (iterated) prisoner's dilemma. One thing that's cool about game theory is that even with very simple games the difficulty can be quite high if you have a good/interesting opponent.

Poker is also used for quite a bit of research into incomplete information games. I believe heads-up limit hold'em was probabilistically solved (in the technical sense) and the [Libratus](https://www.wired.com/2017/02/libratus/) system can now beat no-limit heads-up pros as well.

And then there are video games. Atari(-style) have been used quite a bit with the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment) (note that it's typically not super hard to write AI for any specific game, but research in this area is typically meant to be able to learn *any* ALE game). Microsoft's [Project Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/) allows people to (more) easily do research with MineCraft. Facebook's [TorchCraft](https://github.com/TorchCraft/TorchCraft) allows people to connect the Torch machine learning library to StarCraft: Brood War, while DeepMind has released a [StarCraft II research environment](https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/) in collaboration with Blizzard. They also released [DeepMind Lab](https://deepmind.com/blog/open-sourcing-deepmind-lab/) which is an FPS-like research environment. Another FPS research environment is Poznan University's [ViZDoom](http://vizdoom.cs.put.edu.pl/) (based on the old Doom). [OpenAI Gym](https://gym.openai.com/envs/) and [OpenAI Universe](https://blog.openai.com/universe/) allow you to test &amp; train your AI agent on pretty much anything you can run on a computer, which includes some games (most notably GTA V, which has been used for [self-driving car research](https://github.com/ai-tor/DeepGTAV)). OpenAI is also doing [Dota 2](https://blog.openai.com/dota-2/) now. Various incarnations of [Super Mario](https://motherboard.vice.com/en_us/article/8q84zz/why-artificial-intelligence-researchers-love-super-mario-bros) have been used for different research (e.g. learning to play, play like a human, design levels). Finally, I'll mention there are [General Video Game Playing](http://www.gvgai.net/) and [General Game Playing](http://www.ggp.org/) competitions with lots of games (although GGP doesn't necessarily use video games).

And I probably missed a whole bunch...",1505902907.0,dn96svw,t3_717g09,artificial,t5_2qhfb,2017
70,MaddieBlau,"Well, it won't impact our lives in the near future. But this new AI CPU will push AI research forward. 

Researchers can use it to:
1. Interpret large-scale scientific problems and find solutions
2. Build AI algorithms for self-driving cars
3. Develop complex financial models in order to avoid unpleasant events, such as the Recession of 2008.
4. Develop diagnostic and treatment models for cancer, and much more.

Also, since this CPU can generalize well, it will reduce the AI research cost. Nowadays, AI research is very expensive primarily because researchers have to develop unique systems for each and every particular situation.

Of course, this also means that the AI devices powered by the chips derived from Loihi will have a friendly price tag.

I'm pretty sure that these changes will come in the next 10-20 years. Moore’s Law still holds true. :)
",1506412761.0,dniup9a,t1_dnirun6,artificial,t5_2qhfb,2017
71,zachsmthsn,"to provide a contrary opinion, I preferred udacity over coursera. Beyond just the difference in technical approach (coursera is MATLAB, udacity is python) Udacity provided a little more hand-holding as I was getting the environment set up and learning how to use the tools. In the end, it was mostly an introduction to sci-kit learn and the most benefit from that class was just having to parse through some of the sci-kit learn documentation.

I also really liked the AI course (which was the first udacity course, iirc) with Thrun and Norvig. I started it as I was reading through Norvig's textbook and, although it took a different timeline then the book, it gave better insight into the different areas of AI. 

I would recommend supplementing whatever course you do with a specific Neural Net/Deep Learning course, because most of these don't go into too much detail there and it's such a popular tool nowadays. If anyone has a good course using keras/tensorflow that they could recommend. I did like the simulated self-driving car project on udacity, but I did not actually follow the course",1506788887.0,dnpyh79,t1_dnpiv8i,artificial,t5_2qhfb,2017
72,CyberByte,"I don't really know the answers to your questions, and I think only very few people will see them in this old thread (I got a notification because I started it). So maybe you'll get better answers if you [make a new text post](https://www.reddit.com/r/artificial/submit?selftext=true).

From my limited perspective I'd say that GPUs are helping quite a bit with especially deep learning, which is probably the most popular paradigm in modern day applied AI. While I would say that what we need the most in AI is *ideas* (which tends to translate to software), better hardware allows for a much more rapid feedback loop in the development of such ideas as you can get real (in)validation and results quickly. It also seems that as Moore's law seems to have stalled for ""transistors per square inch"" / CPUs, the use of GPUs still continues to increase the amount of computation we can use per year.

GPUs are probably powering a lot of applications, and I believe at least some self-driving cars also use them, but again: I don't know much about this.",1507381132.0,do17ghh,t1_do01sqr,artificial,t5_2qhfb,2017
73,Slapbox,"Completely not the point. The point is they validated a method of teaching a computer, and they've now done it across various technologies including image recognition and self driving cars.

The chess algorithm tried every possible combination of a very limited set of combinations and projected moves ahead. AlphaGo learned to recognize and take advantage of patterns in a way that allows us to teach computers much more like you would animals, compared to previous attempts at AI which tried to write the code for the end product themselves.",1507691542.0,do7afxg,t1_do76wde,artificial,t5_2qhfb,2017
74,victor_knight,"Back the truck up just one minute. The technology used to play Go like a champ is NOT the same technology used in self-driving cars. They are like apples and oranges. Yeah, they're both ""AI"" but not the *same* AI. No one is disputing that highly specialized tasks can likely be conquered by AI.",1507693266.0,do7bovr,t1_do7afxg,artificial,t5_2qhfb,2017
75,furtfight,"So what? I used this term because no one has a clue about which specific ""technology"" has been used to build alphago, the self driving software of waymo or the new system of google translate. But it's most likely that what you learn by building a system that use some form of machine learning will help you to improve your future systems.",1507696760.0,do7dz39,t1_do7d1sb,artificial,t5_2qhfb,2017
76,smackson,"Depends who you ask. Because you're asking about *consciousness*.

Consciousness is not the same as intelligence. Consciousness is a philosophical matter that is talking about *what thinking is like from the inside*. So a thing could potentially be non-intelligent but still conscious.... intelligent but not conscious... Or both. Or neither.

People are already calling modern AI ""intelligent"" in at least a narrow sense. AlphaGo.... Or a self-driving car. This does not imply consciousness, and in my book none of these things are conscious.

But beyond even these questions, let's say that in 20 years we achieve ""AGI"" (artificial general intelligence) such that it answers questions in a way indistinguishable from a human... Maybe that is intelligence, but is it conscious or just *behaving* like a conscious thing???

Because in a philosophical sense it is impossible to be sure that anyone or anything is conscious, outside of yourself. Look up the ""other minds"" problem. Essentially I make the world more manageable by assuming that you, and my brother, and the butcher are all conscious in a way similar to how I feel conscious, but it is after all an assumption, and I'm not sure it's wise to project that onto any conscious-seeming algorithm.

Short answer"" intelligence is not consciousness, consciousness is a pphilosophicalquestion, let's be careful with our terminology.

",1508712643.0,doqqwef,t3_781yd0,artificial,t5_2qhfb,2017
77,jhill515,"Certainly! Try to do a robotics senior project if you're an undergrad, and try to go to grad school as early as possible. Because self-driving cars are the most well-funded area, try to focus as much as you can on 3D perception and object recognition. That will open TONS of opportunity for you. Likewise, learn [SLAM](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping) and execute a project demonstrating it!

Beware, the robotics field in industry is very startup oriented, so employers are going to be risk-adverse when it comes to hiring. If it doesn't look like you have a 1-to-1 experience for what they're looking for, you can be the greatest programmer in the world and you'll get passed up. Sadly, this is the case I'm actually faced with as I try to move onto new opportunities -- I've worked many projects which apply various principles from robotics, and while my academic knowledge knows how it translates to a ground-based robot, I don't compete well against someone who actually has a ground-based robotics project that they can show off.

All in all, execute a reasonable project regardless of what you'd like to do once as you graduate -- this will be your first ""claim to fame"" as you start job hunting.",1508812710.0,dosup7e,t1_dosu1h2,artificial,t5_2qhfb,2017
78,bfrb1t,"You don't need much intellect to gain power/control and/or develop a dominant billion dollar company. Most of the big companies that the average pleb praises and defends don't have much wits about them. Much wit would preclude such a manifestation. As such, the average person is fearing and putting hope in the wrong people/institutions that aren't capable of delivering anything beyond applying what someone else develops. Take self-driving car technology for example. Bonus points for anyone who knows who actually developed and fostered the R&amp;D that defines this technology...",1509094111.0,doyi42l,t1_dovkocr,artificial,t5_2qhfb,2017
79,DuplicatesBot,"Here is a list of threads in other subreddits about the same content:

 * [GM's Self-Driving Cars Head for New York City, Where They'll Face the Bullies](https://www.reddit.com/r/technology/comments/77znqa/gms_selfdriving_cars_head_for_new_york_city_where/) on /r/technology with 208 karma (created at 2017-10-22 19:15:59 by /u/HeartBypass)
 * [In New York, Self-Driving Cars Get Ready to Battle the Bullies](https://www.reddit.com/r/Futurology/comments/78cwz3/in_new_york_selfdriving_cars_get_ready_to_battle/) on /r/Futurology with 27 karma (created at 2017-10-24 10:42:04 by /u/nliausacmmv)
 * [In New York, Self-Driving Cars Get Ready to Battle the Bullies](https://www.reddit.com/r/cars/comments/78cwz8/in_new_york_selfdriving_cars_get_ready_to_battle/) on /r/cars with 2 karma (created at 2017-10-24 10:42:06 by /u/nliausacmmv)

----

 ^^I ^^am ^^a ^^bot  ^^[FAQ](https://www.reddit.com/r/DuplicatesBot/wiki/index)-[Code](https://github.com/PokestarFan/DuplicateBot)-[Bugs](https://www.reddit.com/r/DuplicatesBot/comments/6ypgmx/bugs_and_problems/)-[Suggestions](https://www.reddit.com/r/DuplicatesBot/comments/6ypg85/suggestion_for_duplicatesbot/)-[Block](https://www.reddit.com/r/DuplicatesBot/wiki/index#wiki_block_bot_from_tagging_on_your_posts)

^^Now ^^you ^^can ^^remove ^^the ^^comment ^^by ^^replying ^^delete!",1509365451.0,dp39a1u,t3_79n8zl,artificial,t5_2qhfb,2017
80,AsdefGhjkl,"This guy, supposedly an expert on AI, is an expert in nothing. Just the usual journalistic babble. The alarm light lit up when they started talking about the big 'revolutionary' thing of ""algorithms that learn from experience', and then put a couple buzz words on the table like ""neural networks"" and ""reinforcement learning"".

I bet he doesn't really understand that the self driving car does not automatically learn from miles driven - there is no labeled data in the real world. The only way Waymo's car gets better over time is by having engineers handle the cases where it couldn't continue and adapt its perception and decision.",1509533381.0,dp6mhfm,t3_79uzni,artificial,t5_2qhfb,2017
81,LiamBigDataDonoghue,"I don't really believe A.I will have the impact we all think it will because there'll always be humans who will always make error and we still need to check and engage with processes to ensure they're working. Take my below example about google cars:  

Google perfected the self driving car over a decade ago but found that once on the motorway (Freeway for my transatlantic cousins) they literally switched of, rummaged around in the back did everything other than be able to take over the wheel at a moments notice. So google had to go back to the drawing board and design a car that could drive literally everywhere!

Great read though, lot s to mull on ",1509539665.0,dp6piep,t3_79gmur,artificial,t5_2qhfb,2017
82,CyberByte,"If you're giving citizenship to the robot, presumably the robot itself is responsible. 

Ignoring that, somewhere a human did something wrong. It may be the owner ordering the robot to kill or misuse/misapply it in some way, or the developer who put in a bug or failed to document the robot properly, or possibly even the victim who did something stupid (like jump in front of a self-driving car). It's definitely a complicated question, and I think it's impossible to answer without more specifics.",1509610258.0,dp8biub,t1_dp8bdgz,artificial,t5_2qhfb,2017
83,brereddit,"OP, there’s a minor controversy about what can correctly be called AI that might make an interesting article.  When you are in the AI space you see a lot of people claiming AI for what is really ordinary automation.    

Maybe a sketch of various AI definitions and then at the end an analysis of self driving cars asking if various approaches are truly AI or just collections of simple and complementary automation...",1509890963.0,dpdl21k,t3_7axiux,artificial,t5_2qhfb,2017
84,CyberByte,"Just to be clear: is this going to be a novel, or a pop-sci textbook?

Regardless, I think it may help you to read nonfiction written for the lay public about issues related to AI by other authors: Ray Kurzweil, Wendell Wallach, James Barratt, Pedro Domingos, Nick Bostrom, Jerry Kaplan, and more. 

One thing that I consider important is that there are many different kinds of AI, but mainly that there's a distinction between narrow AI (ANI) and artificial general intelligence (AGI) (and artificial superintelligence / ASI), and they are associated with different risks and benefits. Since these are all referred to as ""AI"", this can lead to a lot of confusion and miscommunication. I very briefly address it [here](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_narrow_ai_or_general_ai.3F). (If you're writing a story, this could be one form of tension: a (narrow) AI expert vehemently opposed to ""AI safety"" work and ""AI risk"" warnings because he doesn't realize it's not about *his* kind of (narrow) AI, but about AGI.)

Like I said, depending on the kind of AI, there are different potential risks and benefits, and they're associated with different (estimated) timelines. Contemporary (and near future) ANI already raises questions about privacy, unemployment, responsibility, autonomy, fairness, (over)dependence, understandability, security, warfare, ethics (should a self-driving car kill a young child or two elderly people?), etc. The prospect of AGI/ASI dials these all up to 11, and adds questions about existential risks (and opportunities: maybe AGI can save us from meteor impacts or something), human obsolescence, mortality, sentience, artificial personhood/rights, etc.

It all depends on the technology used as well. Different approaches have different features and limitations, pros and cons, and ways to exploit/fix them. ",1510046833.0,dpgtpd4,t3_7b6w8n,artificial,t5_2qhfb,2017
85,HolyGarbage,"Discerning between groups of people is actually one of the problems self-driving cars solve. It needs to categorize between other drivers, pedestrians (crossing the road and walking alongside), cyclists, and other entities.",1510392213.0,dpnr675,t1_dpnf1i0,artificial,t5_2qhfb,2017
86,Lendari,I'm just saying. No murder robots until I get a self-driving car that actually works. That's my policy.,1510419773.0,dpo4zgh,t1_dpo4f5d,artificial,t5_2qhfb,2017
87,Lendari,Eh it's not like other applications (self driving cars for example) don't have tremendously profitable incentives. I guess they are just more difficult than murder.,1510420054.0,dpo57j6,t1_dpnpw1r,artificial,t5_2qhfb,2017
88,dublem,"Rather than quibbling over how possible every little thing in this video is right now, I think it's far more useful to recognise that, even if the specifics aren't feasible right away, it's not hard to reconstruct similar scenarios which achieve much the same effect with current technology. 

Payload to thrust? I'm sure you can imagine similarly effective methods of focused attack using easily available materials. 

Issues around facial recognition? You don't need 100% accuracy. Combined with the currently existing real-time localisation/mapping technology, and the massive volume of personal data most people (and especially high profile people) release which give information away about their whereabouts and behaviours, it's not hard to imagine being able to reasonably estimate the location of a particular target, and then have a drone be able to find that place, and that person based on known information about them and their environment.

Multi-agent control? Don't make the mistake of overcomplicating things unnecesarily. This video wasn't showing incredibly complex multi agent cooporation. You have carriers, who 1) locate entrance points, 2) release their payload, and 3) detonate to create an entrance. The released attack drones identify targets matching the set criteria, and then attack. Assuming they can communicate and targets can be marked, it's not hard for them to avoid already marked targets. For the most part though, it's hunting out targets, which is actually something multi-agent cooperation is really good for. Navigating, mapping, and localizing are by no means solved, but if they weren't tasks that could be accomplished with a reasonable degree of accuracy, self driving cars would still be a complete pipe dream. And given that you're dealing with small, inexpensive drones, you can consolidate your knowledge base with multiple view points and lots of redundancy.

Fundamentally, the take home point should be that this is just one scenario, and it's far from being unimaginably ridiculous. I'm sure if you ask a room full of people with expertise in these fields, you could produce binders full of entirely plausible ways of achieving similar effects with technology and data that exists today.

And that should be really fucking scary.",1511121143.0,dq21o25,t1_dq1o0i6,artificial,t5_2qhfb,2017
89,hara8bu,"

&gt;Driverless cars are not here now.  
What are your thoughts on [Tesla’s autopilot feature?](https://www.tesla.com/en_CA/videos/autopilot-self-driving-hardware-neighborhood-long)

&gt;You know what will happen to all the truck drivers that loose their jobs to a driverless truck?  They will either adapt to the change and get new work, or retire, or they won't.  Either of the three options is up to them.
Yeah, probably true. ",1511720347.0,dqd8m8n,t1_dqd5iot,artificial,t5_2qhfb,2017
90,MakingTrax,Tesla's autopilot feature is useful and impressive enough but it is the first company to offer this type of feature to customers.  When you can buy a car with self driving capabilities from more than luxury markets then it will have greater impact.,1511793270.0,dqemav4,t1_dqd8m8n,artificial,t5_2qhfb,2017
91,CyberByte,"I think this was somewhat interesting, but think a little more argumentation is needed on the ""utilitarian cars are not trustworthy"" part. I understand that short-sightedly utilitarian hospitals that harvest healthy people's organs to save multiple sick people erode trust in the institution of the hospital. On the other hand, knowledge that most people would pull the lever in the original trolley problem does not really influence my decision to go near trains. So why are self-driving cars more like the hospital situation than the trolley situation?

I found the ""it's exploitable"" argument almost comically weak. Yes, a murderer could throw a kid in front of your self-driving car if he knows the car will prioritize the kid's life over yours. But at this point he's already pushing people in front of cars, so why not just push you? Also, deontological cars with rigid rules don't really seem any less exploitable. At least with a utilitarian car you could (in theory) just have it do whatever ultimately leads to the best outcome. 

I do kind of agree that a lot of the Moral Machine type research, while interesting from a philosophical and psychological perspective, is not that practically relevant. I don't think we're going to encode rules that prioritize women over men, or white people over black people, or vice versa. Just from a legal perspective this is untenable. And I doubt we'll let these rules be hidden away in an uncritically accepted neural network black box. 

However, I wouldn't be surprised if we encoded rules about trying to increase the odds of survival for the most people, possibly subject to adherence to traffic rules, and possibly with special rules for people inside and outside the car and other cars (whose number of passengers might not be clear). ",1511799856.0,dqerutz,t3_7fp7ng,artificial,t5_2qhfb,2017
92,mydigitalstartup_net,"From what I have been reading, Uber is betting big on self-driving cars. Automation Engineers who can help with that and also use some of the ML techniques are probably in demand there",1512119627.0,dqlo4hk,t1_dqlo2r2,artificial,t5_2qhfb,2017
93,CyberByte,"People should probably fill out the poll before reading my answers. 

&gt; Is AI safe?

I answered 'no', because I didn't want to answer 'yes', but I think there are both safe and unsafe applications of AI. A self-driving car could very well be much safer than a human driver, but if something is wrong with the algorithm, then it's not. There are many ways to misuse AI, or to use it correctly and still cause harm. Even with narrow AI tools we need to be very careful in how we use them. I think AI does more good than harm, but it's better to think of it as an unsafe technology we need to try hard to keep safe, than as a safe technology where we don't need to worry about anything. 

&gt; Will AI destroy us?

I answered 'yes', but again it's more nuanced than that. I think that if we develop AGI without solving the control problem, it's pretty likely that we're screwed. But I think that the number who take this problem seriously and are working on it is growing, so I'm hopeful that it won't come to that. And I wouldn't be 100% surprised if there won't be a hard takeoff / intelligence explosion and we have more time to test, adapt and have the AI solve the control problem for us.

&gt; Will AI replace doctors?

Yes, in the long run there won't be a real need for human doctors. But at the moment I don't think most doctors need to fear for their job. AI is not robust and understandable enough to trust completely at the moment, even with tasks like diagnosis which they are essentially best at. They will get better at this, and at developing treatments, but for the time being we'll still need doctors to check and explain these things, and to carry them out. I think that for quite some time, AI will just help doctors do their job better. And since there seems to be a shortage of healthcare providers and a demand that can stretch quite a bit, I don't see this leading to many unemployed doctors anytime soon. But eventually: sure.",1512653598.0,dqwe0bj,t3_7i5kjq,artificial,t5_2qhfb,2017
94,bartturner,"Of course Google wants to make a profit as that is how our system works.   But this system causes Google to create software that in just 4 hours and with training data to become the best chess player, computer or human, in the world.

I would disagree they are maximizing profits just in the short term.   They have been investing billions into self driving cars knowing it requires a long game.",1512767816.0,dqywicu,t1_dqwcly2,artificial,t5_2qhfb,2017
95,victor_knight,"I'm sorry, but I don't give a toss how well a computer plays go or how fast it learns it. Also, I wouldn't trust a self-driving car for another 40 years (if I'm still alive) when some real statistics about how reliable they are have come out. I've been doing fine driving myself around for decades. And if I can't or don't want to, I can ask or pay someone a nominal fee to do it for me who would be happy to. Basically, let me know when Google or anyone else uses AI for something actually groundbreaking. I have yet to see this and I've been waiting decades.",1512779974.0,dqz6jrj,t1_dqywicu,artificial,t5_2qhfb,2017
96,bartturner,"Things are going to progress pretty fast and think you will struggle as they do.
 
On self driving cars.   1.3 million die on US roads each year so the bar is pretty low for Self Driving Cars.   Google is going over 5k miles without a human and not had a single death.


",1512827115.0,dqzuj8j,t1_dqz6jrj,artificial,t5_2qhfb,2017
97,bartturner,"Obviously not 100 years away.    Speech as in a conversation does require something close to AGI.   But pretty comfortable that we will be there in far less than 100 years.

Think there is a lot of value as we are already seeing with AI before that point also.

Probably the biggest is self driving cars will help society in unbelievable ways.",1512829234.0,dqzvnjc,t1_dqzvjqi,artificial,t5_2qhfb,2017
98,bartturner,Search engine is a foundational  tool that enables so many other things that help humanity.     Google makes so much money they can invest into so many things that help people.   Look at what their work in self driving cars will enable for people.   Another application of AI that can do the world a lot of good.    Over 1.3 million die on US roads alone per year and Google AI will end the vast majority of those.,1512868628.0,dr0ppph,t1_dr0p1bf,artificial,t5_2qhfb,2017
99,bartturner,"What?  People use search everyday.   Google is launching their self driving cars in Arizona without drivers.

https://www.wired.com/story/waymo-google-arizona-phoenix-driverless-self-driving-cars/
Waymo Finally Takes the Driver Out of Its Self-Driving Cars | WIRED",1512872862.0,dr0sudh,t1_dr0rzo1,artificial,t5_2qhfb,2017
100,steezyone,"The system would learn how to respond if it was trained on cheating opponents.   If probably set the parameters before the match so it would not be able to adapt to something like that midgame.  Even seeing one game it probably wouldn't be enough training examples to learn to adapt.  
An example I can think of is if the cheat is something like adding range to an attach, it may always think it is safe x distance away and never move back further despite now being in range.   

Teaching AI to learn once training is finished is an active area of research.  You don't want to be to susetible to new data (just because I see one stick in the road I don't want to totally alter my self driving car), yet you need enough of a change to adapt to active conditions (maybe it is a storm and there are branches everywhere).",1512925693.0,dr1kgcr,t1_dr10iec,artificial,t5_2qhfb,2017
101,TriRedux,"Sounds good, this could simultaneously be solved with self driving cars I think! ",1513103475.0,dr5b2mu,t1_dr57fxb,artificial,t5_2qhfb,2017
102,spudmix,"I think fully autonomous self-driving cars are right on the cusp of becoming viable, though reaching SAE 5 by 2019 is probably a little optimistic. *Affordable* SAE 5 vehicles for the average family is extremely unlikely, but I can see professional driving services (e.g. trucking, taxis) switching to self-driving with a bit of luck. 

On a less important note, I think OpenAI will probably have functioning Dota 2 AI before the end of 2019. ",1513115438.0,dr5nfsd,t3_7jbcyk,artificial,t5_2qhfb,2017
103,Garethp,"&gt; Is there a point in the future that you think we’ll be ready for something like that?

A complicated question that needs to be answered on both fronts separately. But a quick summary is that I don't believe we'll ever be *ready* for it. From my perspective, our large societies are largely reactive, not proactive. Crisis' happen, then we try to adjust to them. We won't be *ready* for what will happen, but we may find a way to adapter after the fact.

# Machine Learning
I think there will be a point when we're comfortable with it. But what boundaries we have now might not exist when we're done adapting. The NSA leaks caused America to freak out, because the idea of their Government spying on them was something they felt was wrong. The idea that Facebook listens to every word you say when it's not open on your phone (no, it doesn't) still scares people who believe that. 

I think there'll be a day when we accept that these things happen, and don't find it disturbing. It'll be natural to us to have machines constantly parsing every word we say, every sentence we write without sending it. How else will the computers come to predict our thoughts the way we want them to?

But it's the **getting** to that point that's going to be hard. Not just in the sense of privacy, but in real world effects that will hurt real people. 

I think most people here may agree that we need a UBI to progress steadily in to the future, but when is that *actually* going to happen in most countries? Or even just in the US, since I assume most conversation will focus on the US. In the next decade? I highly doubt that. Two decades? Maybe, if you're being fanciful. 

Let's take a look at the US [jobs by sector](https://www.bls.gov/emp/ep_table_201.htm).

Retail Trade: 15m jobs. We're already moving towards a non-human based reatil trade

Transportation and Warehousing: 5m jobs. We've already got self driving trucks on the road and increasingly automated warehouses.

Information: 2m jobs. That's basically ML specialty, right? Finding more information better, and quicker? This industry should be worried.

Financial Activities: 8m jobs. While it may not go away, this should require a lot less humans as we progress further and further. Already, the stock market is mostly run by machines.

That's just four industries that seem like easy targets using today's technology. That's ignoring what we're working on in terms of human-like robots that could do our mining, construction and manafacturing for us. That's still 30 million jobs. That's 20% of all jobs in the United States. The kind of economical impact this would have is huge. And I don't have faith that we as a society will actually do something about it **before** that economic impact hits. Or even right as it hits.

TL;DR: [Humans Need Not Apply](https://www.youtube.com/watch?v=7Pq-S557XQU)

# Artificial Intelligence

We humans have only now started, *started* to think about treating each other as humans. We're not even doing all that great yet. We're still working on basic rights domestically, tackling racism in our day to day and we pretty much ignore most things that happen in third world countries. Or even second world countries. I don't think we'll be ready for a whole other form of life (AI as sci-fi and most people think of) until long after it's here. And maybe we'll only be ready once it's too late. Maybe we'll ban it outright. Or maybe it will come and pass and it'll take decades of debate and struggle to decide and if, and how much, we should grant them rights. But given our track record so far, I don't think we're going do so well.

TL;DR: The Animatrix.

In summary, I don't think we'll ever be prepared. The time to start was probably decades ago. We're just going to end up reacting to whatever disaster hits us",1513207103.0,dr7ojpw,t1_dr7mytz,artificial,t5_2qhfb,2017
104,steezyone,"Ya, more the compression seems to be too much.  Pretty much turns a full cart into a pixel.  I can easily process 320*320 images im real time on my laptop, with a much more complicated architecture. (Driving simulator that does the same thing for the Udacity self driving car project)",1513725434.0,drhu6pa,t1_drhnngl,artificial,t5_2qhfb,2017
105,SocialMediaGiants,"Inspiring article! To my knowledge, most AI research to date is focused on Biomimicry (robots learning to behave more like living things). But this article sheds light on Robomimicry (living things learning to behave more like robots), for better or for worse.

This article is about first real fish and robot fish, then cars with drivers and self-driving cars. One point I'd like to make is that AI doesn't necessarily need to be the physical vehicle to uncover best and worst practices of a given behavior. Instead, AI can be builts to monitor and learn. 

I.E. For zebrafish, load a series of motion sensors to uncover the best/worst ways to get around the aquarium; or uncover which behaviors make one fish a leader, and other fish followers. For driving cars, load sensors into cars of real drivers, and identify best/worst driving behaviors. 

**All hail Robo-fish!**",1513739155.0,dri5ngm,t3_7kkzze,artificial,t5_2qhfb,2017
106,kinjago,"&gt;I'm not quite sure what jobs are available in the field to begin with

That makes it simple. ""AI"" is mostly metaphor in the industry for solving heavy math problems. This has become a big trend in the past few years because of the advancements in computation power. So if you are looking at jobs perspective (ie) the ones in industry solving real problem like self driving cars, home automation etc, you need programming, stats, calculus, CS. If you are looking to be a professor the subjects you mentioned might help. 

This reminds me of the several CS courses I took in my Masters. Most of them are BS and have very little relevance to industry. (ie) A kid with some googling can write SQL queries just as well as someone who took CS course in Relational Algebra and Tuple Calculus.",1513792196.0,drj2xed,t3_7ks40h,artificial,t5_2qhfb,2017
107,n3uralbit,"Machine learning isn't going anywhere, though it might very well change quite a bit due to the various schools involved. Some of the current applications of machine learning are rather different from those of knowledge engineering, in that we are trying to extract knowledge and insights from vast amounts of data rather than interring vast amounts of information purposefully into an agent.

As long as huge swaths of data exist, there is going to be demand for making sense of that data.

Furthermore, among those applications of machine learning that *do* rival knowledge engineering, such as self-driving cars, ML generally performs a lot better and the technology is only improving.

Of course, no one on Earth can *really* tell you what's going to be the hot topic tomorrow, let alone 5 years from now - we can only make educated guesses.",1513846713.0,drka7pn,t1_drj6v8s,artificial,t5_2qhfb,2017
108,Anonsicide,That is very interesting! Do we do street signs because of the push for self-driving cars? I've noticed a lot of road examples too... that's actually pretty neat if that's how they are going to use it. We'd be effectively training cars to see!,1514503759.0,drvuuaz,t1_drvtfim,artificial,t5_2qhfb,2017
109,PM_ME_SAD_STUFF_PLZ,"Yes. Google's parent company, Alphabet, owns Waymo. Waymo is currently running trials for self-driving cars in Phoenix.

&gt;We'd be effectively training cars to see!
[
You already are/have.](https://techcrunch.com/2017/11/07/waymo-now-testing-its-self-driving-cars-on-public-roads-with-no-one-at-the-wheel/)
",1514504124.0,drvv5cj,t1_drvuuaz,artificial,t5_2qhfb,2017
0,CyberByte,"Harvard professor Steven Pinker's views on AI safety are indeed incredibly naive and ill-informed. I generally agree with Musk on this topic, but I find his tweet here kind of weird. He accuses Pinker of not knowing the difference between ANI and AGI, but he doesn't seem to know either. You can give a self-driving car all the compute power in the world, and it still won't be AGI. On the other hand, we have no idea how much compute power is actually required to create AGI (or e.g. human-level intelligence). Furthermore, I'm not sure what he means by an ""open-ended"" utility function, but the whole point of the orthogonality thesis is that you can have AGI with *any* utility function. 

It's hard for me to believe that after all this time of involvement, talks and meetings with AI safety folks he doesn't know better, so I suspect this was maybe just a quick off-the-cuff quip, but this really does seem like the blind leading the deaf...",1519979622.0,dv29hej,t3_81ayd7,artificial,t5_2qhfb,2018
1,autotldr,"This is the best tl;dr I could make, [original](https://www.recode.net/2018/3/6/17082774/uber-self-driving-truck-otto-freight-embark-waymo-alphabet-arizona) reduced by 86%. (I'm a bot)
*****
&gt; Uber&amp;#039;s self-driving trucks are now delivering commercial freight in Arizona, the company announced on Tuesday.

&gt; As Recode first reported, there was also tension within the self-driving department over which of Uber&amp;#039;s two autonomous efforts took priority - was it the cars or trucks? Staffers who joined Uber as part of the Otto acquisition worried trucks would take a back seat to Uber&amp;#039;s original driverless ambitions of building cars to be used in its ride-hail network.

&gt; It&amp;#039;s not because the trucks are more technically capable of driving autonomously than the cars - in fact, driving autonomously on the highway as these trucks are doing is much easier than driving on city streets - it&amp;#039;s because Uber has actually managed to commercialize the trucks.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/82tbbj/ubers_selfdriving_trucks_have_been_hired_to/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 2.00, ~292473 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **truck**^#1 **Uber**^#2 **drive**^#3 **company**^#4 **autonomous**^#5",1520468772.0,dvcmoni,t3_82hi8k,artificial,t5_2qhfb,2018
2,craigjclemson,related: [Google's self-driving semi-trucks](https://www.reddit.com/r/artificial/comments/837rbe/googles_autonomous_vehicle_unit_to_test/),1520611630.0,dvfpjnm,t3_82hi8k,artificial,t5_2qhfb,2018
3,993VQzn06,"It’s hard to definitively say where I’d like to move into. I’m particularly excited by NLP, neural nets, and self driving vehicles. Would also love to work on AI safety research.

Is there an ideal starting point with AI?",1520755637.0,dvipzkv,t1_dvgvzk3,artificial,t5_2qhfb,2018
4,EmperorOfCanada,"Self driving cars are going to decimate police forces across the world. They can deny it all they want but they are the tax arm of many a government. With self driving cars these revenues are going to plummet and many backwater municipalities are simply not going to be able to afford more than the bare minimum of police. 

From what I can see, a common deployment of police in rural America is to have a few cops who are idle sit on the local highway and create a steady stream of income. Then when an actual crime occurs they pull up stakes and do the job that the citizenry mostly want them for. After the crime of the century is resolved, they return to local highway tax collectors.

Thus they are on-duty and available when needed. Without this toll collection, there will be cutbacks.

Some municipalities will try to increase other sin taxes that the police can try to extract such as cranking jaywalking fines, fines for walking in public with earphones in, biking fines, noise fines, not mowing your lawn fines, wrong recyclable in the garbage fines, etc. But these are the sorts of fines that lose elections. Toll collection on the local interstate is hitting non-voters and brings fresh money into the county.

Some places will do the above but aim their fines at some demographic group, either age, income, or ethnicity. After a while the place will wonder where that group went.


",1520910562.0,dvm0oxs,t3_83z3g5,artificial,t5_2qhfb,2018
5,MaunaLoona,"Voice recognition and speech synthesis has been greatly improved through deep learning. So were google's and bing's translation algorithms. And google's search. Oh, and self-driving cars are pretty cool. I'm sure there are countless other commercially significant uses for AI. That's why it's being funded so heavily by the industry -- it bring good returns on investment.",1521280531.0,dvuaf5w,t1_dvu9yyj,artificial,t5_2qhfb,2018
6,PostmodernistWoof,"There was apparently a ""safety driver"" and the fact that they didn't stop the car either could be an argument for it being more likely to be the pedestrian's fault.  Of course the safety driver might have been on their phone or asleep since that sounds like a hella boring job.

But hopefully there's a ton of data and imagery, and the NTSB might well choose to use this as an excuse to pick apart the technology completely and squeeze some of the silicon-valley ""we are geniuses and can do whatever the heck we want"" out of the safety engineering aspects of self-driving cars. This may serve as the model for future accident investigations, we'll see...",1521497447.0,dvypfqt,t3_85lhc3,artificial,t5_2qhfb,2018
7,bdubbs09,"Shame that this will be blown up as evidence that self driving cars are unsafe, and AI in general cannot be trusted. Considering the amount of pedestrians and bystanders injured and killed by humans, this instance points to the fact that autonomous driving is actually safer, though not perfect.",1521506677.0,dvyycyk,t3_85lhc3,artificial,t5_2qhfb,2018
8,runvnc,Pedestrians can walk out anytime into the road and no self-driving or human-driving can prevent them from being killed because cars drive fast next to pedestrians routinely.  The pedestrian basically suicided in this case.  There will be more like it.,1521596121.0,dw0zuww,t3_85lhc3,artificial,t5_2qhfb,2018
9,crypto_phantom,More sensors to sense people from further away. Backup systems in case in there is a failure. A smartphone could emit a signal that a self driving car could analyze the possibility of an accident. IoT to connect everything to a network. ,1521893162.0,dw7i3ps,t3_85oj3w,artificial,t5_2qhfb,2018
10,SilentVigilTheHill,"If you look at automation and ""A.I"" &lt;cough&gt; digital automation, you will find it is taking jobs from the bottom up and top down. We are seeing a near future where drivers and service workers are replaced in the next few years. We are also seeing medical diagnostics and legal AI being developed. A top down job that has largely been displaced is  security analysis. In the 70's-90's this was a high paying job. Now we still have some people working in this niche, but it is largely displaced by AI and the people who write the AI programs. In the 50's we say human computers replaced by digital ones (more a middle class job but still). In my opinion the best place to be is in the middle. For one there are more job openings available. Whole in the near future the next generation self driving cars will largely be designed and programmed by AI. Still need a human to do diagnostic work and maintenance.  There will be humans doing the same for the medical industry, service industry, agriculture, manufacturing....
          
If skilled labor isn't your cup of tea, my advice is go big and think big. Quantum computing is just now available for commercial use.  My personal opinion is that general AI will happen when people integrate quantum computing into neural networks.  By I am biased by my Orch-OR view of consciousness. ",1522030856.0,dwaeuwl,t1_dw6anm0,artificial,t5_2qhfb,2018
11,[deleted],"A lot of those driving AI forwards forgot to place people at the heart of the design, so when an AI causes harm and inconvenience, people will push back against various forms of AI.  As an example, the idea that as a pedestrian and cyclist I will have to get stressed and fearful of collisions with self-driving vehicles on the road, robots running along the side-walk, and drones delivering items to people; this is a scenerio for disaster when hundreds and thousands of businesses unleash all these machines onto towns and cities.  People will get hurt and killed in collisions, because there will be too many of them to safely navigate through them, then these machines will all be banned for health and safety reasons.",1522238710.0,dwex8jf,t3_87qyhn,artificial,t5_2qhfb,2018
12,harry_0_0_7,"Medicine. 
Doctors will get replaced by machines, which will do operation perfectly anytime, anywhere.

Self driving cars.

More like it is possible to create a new world #westworld



",1522316200.0,dwgshzz,t3_87ssmo,artificial,t5_2qhfb,2018
13,autotldr,"This is the best tl;dr I could make, [original](https://medium.com/@MustaphaItani/former-google-engineer-founds-new-religion-to-worship-an-ai-godhead-95f5cf01f048) reduced by 78%. (I'm a bot)
*****
&gt; According to Wired, &amp;quot;In February, Waymo?-?the company Google&amp;#039;s autonomous car project turned into?-?filed a lawsuit against Uber. In its complaint, Waymo says that Levandowski tried to use stealthy startups and high-tech tricks to take cash, expertise, and secrets from Google, with the aim of replicating its vehicle technology at arch-rival Uber. Waymo is seeking damages of nearly $1.9 billion?-?almost half of Google&amp;#039;s $4.5 billion valuation of the entire self-driving division. Uber denies any wrongdoing.""

&gt; Naturally, Lewandowski believes in the singularity, since after all he was very instrumental in the development and rollout of autonomous vehicles for both Google and Uber.Futurists look at this as a very exciting outcome to human capabilities, that would let us integrate with machines, thus discovering new possibilities, like being able to achieve digital immortality by uploading copies of our brains to machines.

&gt; Let&amp;#039;s state for the sake of the argument that this AI deity is actually not that far-fetched, the question here is if people would actually worship it.


*****
[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/89hgd1/former_google_engineer_founds_new_religion_to/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ ""Version 2.00, ~305102 tl;drs so far."") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr ""PM's and comments are monitored, constructive feedback is welcome."") | *Top* *keywords*: **people**^#1 **Google**^#2 **machine**^#3 **human**^#4 **worship**^#5",1522783286.0,dwqxssi,t3_86gaun,artificial,t5_2qhfb,2018
14,river-wind,"That's Sebastian Thrun teaching, the guy who started the Google self-driving car project.  He does videos like this for Udacity.com, which he founded originally to give free AI lessons via the internet.  This seems to be from the original Intro to AI class, taught by Thrun and Peter Norvig; unit 9, video 35, as suggested by the video name above.

The entire lecture series, along with quizes and programming exercises, is available for free here:

https://www.udacity.com/course/intro-to-artificial-intelligence--cs271",1523327938.0,dx3v94s,t3_8axkgo,artificial,t5_2qhfb,2018
15,kevinw88,Only until they start regulating his self driving cars.,1523480489.0,dx7fr25,t3_8bijsi,artificial,t5_2qhfb,2018
16,Heaney555,Well made self driving systems are made with the starting principle that you can never trust anyone else on the road.,1523617873.0,dxakogl,t1_dxa8jrp,artificial,t5_2qhfb,2018
17,Heaney555,"Comparing self driving to passing a Turing test is absurd.

We already have SDCs on the road. They will be widespread in 5 years. They will be *everywhere* in 15 - it's a matter of economics at this point, not technology.

Meanwhile we have 0 idea how to pass a Turing test.",1523618099.0,dxakt1o,t3_8bpmi7,artificial,t5_2qhfb,2018
18,knickerlesscage2018,"Forgive me because I do agree with your points, you're right, everything won't change in a day, that's why experts have basically given us 12 years for this to fully take hold. But they're not saying in 12 years everyone will suddenly lose their jobs, (well, 47%) they're saying by the time we reach the 2030's this problem is going to become apparent. 
During the next 12 years more and more of us are going to lose our jobs, it's already started happening albeit slowly and each sector is slowly becoming more and more automated or there is a revolution in an industry which wipes out competitors. 
Look at the high street now to what it was just 10 years ago, look at how many big corporations we've lost or if we haven't lost them they have cut back their stores. Banks are closing all over because of online banking apps and In their place we are getting takeaway joints and charity shops.

When self driving trucks are no longer subject to stringent legislation and are allowed to operate we're going to see that industry become automated very, very quickly as haulage companies can afford to it and it's viable and profitable. 

You're right, UBI or some form of welfare system like it is completely unavoidable.",1524935898.0,dy4ere9,t1_dy4aeno,artificial,t5_2qhfb,2018
19,robotix_dev,"An accountant is a poor example.

It seems like your definition is solely based on whether or not someone is licensed as a Professional Engineer, so let’s view it from that perspective.

Professional Engineer licensing exists because the public health and safety need to be protected when signing off on designs of physical systems/structures. First, I would note that not all engineers have to be licensed. In fact, before taking your PE exam, you must have work experience with a licensed PE (notice how the NSPE refers to PE candidates as engineers):

https://www.nspe.org/resources/licensure/what-pe 

Software Engineering is a relatively new subset of engineering and is just now getting to the point where public health and safety need to be taken into account (e.g. self driving vehicles, robotic surgery assistance, etc.) and as such licensing for Software Engineers has taken shape. See this article in IEEE from 2012 on software engineer licensure:

http://theinstitute.ieee.org/career-and-education/career-guidance/licensing-software-engineers-is-in-the-works

Here is a PowerPoint outlining the engineering organizations involved in creating a PE Software Engineer exam as part of a Software Engineer Licensing Consortium (notice that it also includes a formal definition of software engineering):

https://www.nspe.org/sites/default/files/Software%20Engineering%20-%20Address%20to%20Japan%20SPE%2014jun13.ppt

Again though, not everyone that has a major ending in “Engineering” has to be licensed as a PE and the same is true for Software Engineers. In the future, we may see a requirement for a licensed software engineer to sign off and seal designs for an autonomous driving system, but currently that is not a requirement because the field is still in its infancy and states haven’t created laws to require it. In fact, the NCEES is discontinuing their software engineer licensure exam due to low interest (I assume it has low interest because currently only Texas and Florida seem to care about licensed software engineers). 

So, it seems to me that Software Engineering is a legitimate subset of engineering. The field is steadily growing, with the advancement of AI in particular, and I think that the title ‘Software Engineer’ is deserved.
",1525231261.0,dyb0tla,t1_dyaumco,artificial,t5_2qhfb,2018
20,CyberByte,"Usually when people explicitly ask/talk about the NEAR future, they mean it as opposed to the presumably FAR future where we might have artificial general intelligence. However, from your post I get the feeling that maybe you actually do want to talk about that (was ""sentimental"" meant to be ""sentient""?). Maybe you could clarify a little. 

In general I'd say that there is a lot of uncertainty, but for the actual near future I think we will largely see automation of specialized tasks. Especially self-driving cars are going to lead to mass unemployment. I suspect that the healthcare domain will also be impacted heavily, although I think it will mainly be through higher quality care and diagnostics and not so much through job displacement. 

In a (further) future with AGI, we could worry about virtually all jobs being rendered obsolete, but the bigger worry is keeping it under control I think (see /r/ControlProblem).

&gt; It also raises a possible hypothetical scenario of ""What if we had that AI right now, today?"" Would love to hear your thoughts on this.

To be honest, I think this is actually how most of our predictions/projections work. It's easier to change one variable (or a few) than to change all of them. If we think we'll have AGI in 30 years and wonder what that'll be like, we should also try to anticipate all other cultural and technological changes that will have happened, but that's obviously very difficult. ",1525297794.0,dyckfjm,t3_8gk8t2,artificial,t5_2qhfb,2018
21,JoseJimeniz,"Has there been any video or data on any self driving cars operating in real-world conditions?

- no 3d model
- no lane makers
- construction zones (closed lanes; offsetting into oncoming lanes)
- snow blowing  over the road obscuring all lane markers
- snow sitting on the road obscuring all lane markers and the road itself
- malfunctioning street lights and having to operate as a four-way stop with six lanes each direction
- traffic emergency, and having to follow the directions of a police officer",1525693485.0,dykx1sa,t3_8hlvuc,artificial,t5_2qhfb,2018
22,JackBullenskie,"Okay, sorry it took a few days. I wanted to be sure that I took a few days off before thinking about it again. SO to answer your questions.

&gt;  What does software legislation even look like? 

I set out to do research on this question, and realized how complex the answer truly is. Great question. I found [this paper](https://pdfs.semanticscholar.org/033e/3f4cb88026804d1de861b6e1944d652cbd74.pdf) that may interest you if you are genuinely curious about the topic. I feel as though regulations on anything would stunt the development of it, which is an unfortunate side-effect. Perhaps the best way to regulate software development is by focusing on the consequences. For instance, if the software that a person develops causes the death of someone, even if it were completely unintentional, shouldn't the person in charge of it's development be held responsible? It wouldn't be murder, it would be manslaughter. But I'm not so certain it's clear who would be at fault for the death of the person. An investigation would need to be conducted, but without clear guidelines anyone could find loopholes. Even the lawyers of the victim's family. Very complex subject and I'm interested in researching and discussing it more.

&gt;  Is the legislation for punishment a fine or a jail sentence or what? 

It should depend. Again though, there needs to be clear guidelines so that people aren't able to take advantage of the legal situations. A hypothetical example would be self driving cars. The following is completely hypothetical for the purpose of thought experimentation. Let's say 1,000 Tesla self-driving cars all go haywire and crash, killing 1,000 people. Shouldn't someone be held responsible for the death of 1,000 people? Let's say an investigation is conducted and it turns out that it was a small typo in the code that was committed by a single developer with a wife and children. The guy is a genuine good guy, but he messed up. Worst case scenario, the poor guy goes to prison for the rest of his life and another family goes without a father. 

Now let me propose that we rewind time. Regulations are put in place that classify self-driving cars as a potential threat to human life. In order to legally produce these sorts of vehicles, a member of the company must be elected to be held responsible in the case that something goes wrong. In return for the risk, the company pays him very well. It is up to the company to find someone willing to take the risk for high compensation. John Doe is elected by Tesla to be the man held responsible for this particular project. Meanwhile, the family man makes his deadly typo, however, John Doe refuses to let this happen because he's going to prison forever if a bunch of people die. He talks to Elon, and convinces him that they need more extensive funding in Quality Assurance to ensure that there are no typos...

Several of the details of my hypothetical are silly as I made it up on the fly, but do you see the points I'm trying to make? 

&gt;  What history does legislation have addressing technological development? 

I'll answer this by using Net Neutrality as an example. The FCC worked with the government to create legislation that prevented ISPs from doing certain things. We all know the story, and I've written several essays about this as well. The regulations that the government had over ISP's basically forced ISP's to develop not only software, but their entire infrastructure in a way that coincided with these regulations. In my honest opinion, the internet has done quite well with these regulations so far. 

&gt;  In what specific ways are regulations qualified to accomplish human goals concerning A.I.? 

I'm not sure I fully understand this question. The point of regulating A.I. development, in my view, is to avoid the startup who's CEO did LSD one day (poke at Steve Jobs) and came up with this brilliant idea to create some AGI, but he rushed it out of excitement and causes damage to property, or people. I'd like to answer this question in more depth if I can understand it better.

I tried to address all of your questions, but my brain is tired after a long day at work. I hope we can find an agreement somewhere.

[EDIT] Changed the wording of one sentence due to possible misconception.",1525746094.0,dymcxg8,t1_dyjn77x,artificial,t5_2qhfb,2018
23,knickerlesscage2018,"I would have said no way, but after hearing Google duplex in action I think you're right.
There's still going to be taxi firms etc operating along with self driving cars so I think this duplex technology will take call centre work first, at least in big corporations like cable and energy.",1525960173.0,dyr5x0p,t1_dyqzb92,artificial,t5_2qhfb,2018
24,knickerlesscage2018,"AI has many other applications as well - saving lives for instance. Medicine and healthcare have and are improving greatly because of AI. It's able to diagnose illnesses at a much quicker and more successful rate than specialist doctors. 


AI will eventually lead to less road fatalities worldwide as self driving cars become more and more adopted over the coming years.

Google duplex will be a life saver for those with depression, anxiety and other mental  health issues that might prevent them from being able to make that call themselves.


I agree some AI seems pointless and silly, but the majority of  AI have much more beneficial applications.
",1525971883.0,dyrij6s,t3_8igb4q,artificial,t5_2qhfb,2018
25,goodnewsjimdotcom,"Everything possible you dream of AI doing is possible once you can recognize your environment via sensors.   This is because if you identify where the bot is at and the objects it can interact and navigate around, the other problems are reduced to what we already know how to do.

Even if no one is looking to make general purpose AI specifically, they're probably going to eventually trudge into vision recognition of increasingly complicated scenerios with self driving cars.",1526074114.0,dytzlgr,t3_8imk9k,artificial,t5_2qhfb,2018
26,HumanTeacher,"&gt; I've never heard of him

He has one of the top 10 highest h-indexes in the computer science community and is responsible for significant advances in mobile robotics (wrote the most important book in robot state estimation in 2005 that still remains relevant today) including self driving cars (which he pioneered at Google).

I highly doubt your technical background if you've really never heard of him.

&gt;if he thinks we're currently mid-singularity 

Singularity does not have to be about the end of human civilization or an apocalypse.

It can be about exponential technological progress requiring significant changes to the way society functions. Certainly, Moore's law, the internet, mobile phones, industrial robots have done a huge part in that, and compared to post Industrial Revolution our society looks very different.

Many people agree that AGI is many decades away, and following research would definitely confirm this.

Again, there is no agreed upon definition of this.

&gt;Here's the specific remarks Schmidt made

Fine, I'll agree with you on this bit.
",1526409703.0,dz12ywx,t1_dz04xhm,artificial,t5_2qhfb,2018
27,BerickCook,"&gt; It will interact with reality in an extremely limited way because of the materials at our disposal.

Weak AI is already interacting with reality in as high of a capacity as we can manage and as fast as we can develop it. What makes you think that industries will treat Strong AI any different?

Just like all other revolutionary technologies, Strong AI will be utilized to its maximum economic potential. It will be installed in every robotic form we can come up with from industrial factory machines to household butler super-roombas. It's easy to see the commercial / consumer / military applications of a single Boston Dynamic's ""Atlas"" that is as intelligent as a human. Let alone mass-produced millions of them in all different varieties and roles.

Same goes for purely digital AI hedge fund managers (which already exist and have caused damage to us), personal assistants, executive managers, marketing services, social media profilers / manipulators, and so on.

&gt;We're not replicating a human like brain, that's impossible. We're creating an intelligence that will learn, teach itself and so on.

You're right that it won't have the same needs, desires, motivations, goals, morals, ethics, empathy, etc... that we humans have. It will be, as you said, an alien life form to us. But I don't see how you think that makes it less of a problem. In fact, I'd argue that it's a much *greater* problem because our natural tendency to anthropomorphize can blind us to its ways of thinking. [We have that problem already with current AI.](https://www.youtube.com/watch?v=GdTBqBnqhaQ)

&gt;It would be moronic to not install safety measures and systems on the first intelligence we create.

Many will, some won't. There will always be companies who cut corners to gain an edge. Researching, developing, and implementing safety measures is expensive and time consuming. All industries have ""those"" companies who do the bare minimum by law or even skip safety measures altogether and hope they don't get caught. AI is no different. Just look at the recent Uber self-driving car incident.

&gt;what creation is going to somehow eliminate humanity and how will it do it?

Lots of potential answers to this question. Here's one that fits just what I've stated without going into the mythical realm of ""superhuman intelligence explosion"":

Weyland Mining is a fully automated mining corporation in a 3rd world country, with only the owners being human. The goal of the AI is to earn as much revenue as possible while maintaining and operating the mines.

It quickly learns that it can bribe local officials to bypass environmental safety regulations and lower the cost of production. Soon, no human can survive the toxic environment in and around the mine and the chemical processes are dumping massive amounts of pollution into the environment. None of that matters to the AI though, as the pollution has no effect on its robots and the human owners are happy because they're raking in the money and couldn't care less about some 3rd world country's environment. 

We humans are *currently* doing that all over the world with mines, factories, refineries, etc... and the only thing keeping them from catastrophic levels of pollution is that humans have to be able to survive working there.

Take human workers out of the equation, and businesses will scorch the earth in the pursuit of profit.",1526457933.0,dz29xd0,t1_dz0xh2d,artificial,t5_2qhfb,2018
28,Earthboom,"&gt;&gt; It will interact with reality in an extremely limited way because of the materials at our disposal.
&gt;
&gt;Weak AI is already interacting with reality in as high of a capacity as we can manage and as fast as we can develop it. What makes you think that industries will treat Strong AI any different?

It's not experiencing reality. It's not feeling anything. It's interacting with object A and manipulating it as per the parameters fed to it. We have clever programs but nothing resembling life because we keep ignoring and downplaying the biological aspect of our existence. 


&gt;Just like all other revolutionary technologies, Strong AI will be utilized to its maximum economic potential. It will be installed in every robotic form we can come up with from industrial factory machines to household butler super-roombas. It's easy to see the commercial / consumer / military applications of a single Boston Dynamic's ""Atlas"" that is as intelligent as a human. Let alone mass-produced millions of them in all different varieties and roles.

&gt;Same goes for purely digital AI hedge fund managers (which already exist and have caused damage to us), personal assistants, executive managers, marketing services, social media profilers / manipulators, and so on.


These are clever programs. They're not the conscious life form we're talking about. 


&gt;&gt;We're not replicating a human like brain, that's impossible. We're creating an intelligence that will learn, teach itself and so on.
&gt;
&gt;You're right that it won't have the same needs, desires, motivations, goals, morals, ethics, empathy, etc... that we humans have. It will be, as you said, an alien life form to us. But I don't see how you think that makes it less of a problem. In fact, I'd argue that it's a much *greater* problem because our natural tendency to anthropomorphize can blind us to its ways of thinking. [We have that problem already with current AI.](https://www.youtube.com/watch?v=GdTBqBnqhaQ)

It'll be a problem in the sense we can't predict what it will do or why. 

It'll be a problem because we won't be able to communicate with our toy. This life form will have physical limitations assuming it has a shell, if you kept it in a virtual reality program it's still limited to the amount of graphics cards and professors attached to it. 

I don't get where the big Sci fi fear of it turning into ultron comes in. 


&gt;&gt;It would be moronic to not install safety measures and systems on the first intelligence we create.
&gt;
&gt;Many will, some won't. There will always be companies who cut corners to gain an edge. Researching, developing, and implementing safety measures is expensive and time consuming. All industries have ""those"" companies who do the bare minimum by law or even skip safety measures altogether and hope they don't get caught. AI is no different. Just look at the recent Uber self-driving car incident.

I mean, I see your point, but you haven't really defined what this structure will look like and how it'll threaten all of humanity so I can't really debate the hypothetical safety measures that would be ignored and how damaging it could possibly be. 


&gt;&gt;what creation is going to somehow eliminate humanity and how will it do it?
&gt;
&gt;Lots of potential answers to this question. Here's one that fits just what I've stated without going into the mythical realm of ""superhuman intelligence explosion"":
&gt;
&gt;Weyland Mining is a fully automated mining corporation in a 3rd world country, with only the owners being human. The goal of the AI is to earn as much revenue as possible while maintaining and operating the mines.
&gt;
&gt;It quickly learns that it can bribe local officials to bypass environmental safety regulations and lower the cost of production. Soon, no human can survive the toxic environment in and around the mine and the chemical processes are dumping massive amounts of pollution into the environment. None of that matters to the AI though, as the pollution has no effect on its robots and the human owners are happy because they're raking in the money and couldn't care less about some 3rd world country's environment. 

But you did go into a mythical realm. Bribe officials? What? Is it a mining AI that just automates the equipment or is it a super human like intelligence that somehow interacts with random officials, has access to money, and understands what a bribe is? 

Furthermore, it's still going to have supervision and if it fucks up, by creating pollution, people would notice. 


&gt;Take human workers out of the equation, and businesses will scorch the earth in the pursuit of profit.

They already do. Car manufacturers are still doing alright. No scorched earth here. ",1526494973.0,dz354a4,t1_dz29xd0,artificial,t5_2qhfb,2018
29,a4mula,"Kissinger alludes to a common moral dilemma in his argument against AI.  A self\-driving car that is faced with running down an old woman or a child.  This is a quandary we as humans will always struggle with, Why?  Simple, lack of data.  Lets fill in some data and see where exactly this golden morality exists.

Old Woman:

Lead scientist, just published a paper stating how her team was on the verge of an epic breakthrough.  Spends her free time posting thoughtful, insightful, guiding messages of wisdom for all of humanity.  Donates heavily to meaningful charities, doing her homework, ensuring the funds are well spent.  Still takes time to help instill in her grandchildren the wonderlust of science and math.  Sits on multiple councils, often as the tipping vote for decisions that are based in rational, just, and fair thinking.

Little Girl:

Medical records indicate she's been in the ER 7 times in 10 years.  Broken bones, scarring indicate heavy neglect and abuse.  She's a carrier for ALS, Alchoholism and a myriad of other genetic diseases.  School records indicate she's a subpar student with a penchant for violence towards her classmates.  Psychological testing indicates early symptoms of lifelong reliance on assistance.

These are extreme examples, and maybe you still won't agree to run over the kid.  Why?  Because we're sentimental.  The obvious boon and benefit to society is the old woman, but we let subjectivity interfere with good decision making.

AI can access every record, every bit of data, every shred of evidence in order to create a fair weighting system that ensures it always picks the human that is most fit.  Most beneficial. 

Who do you want driving the car? ",1526513654.0,dz3p4k8,t1_dz3o9nf,artificial,t5_2qhfb,2018
30,chrisname,"These algorithms still only perform very specific tasks, which they have to be extensively trained to do, and only when instructed to do so. You can't equate object recognition, game playing and passing exams with abstract thought and arbitrary decision making. It's one thing to make a car that can follow a predetermined route or even find the best route to a predetermined destination. Show me one which can choose a destination without being told to do so. Show me an AI which can learn a skill without being explicitly and laboriously taught. Show me one which can ponder and produce abstract ideas. Or, at least, show me how these things might he accomplished, because right now I don't think any algorithms have been proposed let alone developed. Greater computer power doesn't help when the algorithms don't exist.

AI is like street magic - when you don't know how it works, it seems like actual magic. But in reality it's just a bunch of clever tricks, which give the impression of intelligence but probably shouldn't be considered signs of actual intelligence.

Give me any specific, non-adversarial task for which I can find tens of thousands of training examples and I'll build you an AI which can solve that task. But introduce any degree of vagueness or arbitrariness in the task, or adversarial conditions and the AI will fail miserably.

By the way self-driving cars which are safe in cities aren't here yet. Don't count your chickens before they've hatched.",1526519398.0,dz3ueau,t1_dz3tybe,artificial,t5_2qhfb,2018
31,a4mula,"You just described exactly what deep learning is.  Learning skills without being taught.  From Atari Games, to Jeopardy, Translate, to Deepmind, to Watson (More than just Jeopardy!).  Even Googles new Duplex is an example of learning that isn't taught.  Procedural learning is dead.  These are perfect examples of AI that aren't narrow.  That are capable of learning anything without the aid or assistance of a programmer.  It's one of the primary reasons there is so much hysteria over AI today.  They learn, and we have no idea how.  We have no idea how they reach their conclusions, we have no idea what processes lead them there.  Translate developed it's own middle tier language ffs.  Nobody saw that coming and it sure as hell wasn't programmed. 

I'm sorry... did I just timewarp back to 2010?  Every major player in self-driving vehicles have been wracking up millions of miles of unaided driving for years.  This wasn't even news 3 years ago, let alone today.  One fuck up doesn't mean a technology isn't safe.  Lets compare track records to man vs machine.",1526520321.0,dz3v9rr,t1_dz3ueau,artificial,t5_2qhfb,2018
32,dr4wn_away,Self Driving Cars are gonna want this for sure,1527684568.0,dzthm70,t1_dztehs4,artificial,t5_2qhfb,2018
33,EvermoreWithYou," * Musk's company (Tesla) relies on Narrow AI - specifically for self-driving cars. That sort of AI is not even remotely dangerous like a general AI or one used in autonomous drones. 
 * AI experts generally agree that AI, if either improperly configured, or god forbid, used for the wrong purposes (like killer drones), poses a massive existential threat to our very existence. That is why you have AI ethic boards and AI safety experts. The biggest divide is on opinion whether AI will be beneficial or not (optimism vs pessimism - some believe it will do us good in the end, some believe we are going down a dark path). 
 
Mind you, Musk is not an expert in AI, but just follows it's development.",1527695589.0,dztt41k,t1_dztigc8,artificial,t5_2qhfb,2018
34,emil-sweden,"Seems the author wants to provoke more than create a healthy discussions about current expectations on progress and impact. That discussion is important as to prevent a buble and subsequent crash.

Bashing lack of self driving car progress without even commenting on Waymo and focusing on companies that (I also would suspect) are overselling their capabilities is just not honest argumentation.


",1527696674.0,dztudp6,t3_8n8ath,artificial,t5_2qhfb,2018
35,FliesMoreCeilings,"Sure, and any step forwards in weapons research should be considered a problem by the people. We don't want the next step to become ubiquitous.

Weapons are destructive force multipliers. The more the force is multiplied, the more damage someone can do to you with less effort. The more it escalates, the more actors will be interested in damaging you. The day where even small actors will be able to do global scale damage is not a good day.

And AI is possibly the biggest force multiplier we'll get to see in our lifetimes. It's powerful, easy to access and hard to monitor. If research continues , then smart delivery systems like flying seeker drones or self-driving cars with bombs or guns strapped to them are going to be be terrifying. ",1527863950.0,dzxx63m,t1_dzxq5ba,artificial,t5_2qhfb,2018
36,red-starman,"This kind of ethics regards what is called Narrow AI, which encompasses learning algorithms that have an intelligence level less than that of a human. This is a more immediate question because the consequences of it can be seen around us (Cambridge analytica, self driving car accidents).

A good way to form a plan is to try to imagine what the possible negative consequences could  be and try to incorporate principles that will prevent these consequences. The biggest issues currently with these systems is privacy and personal freedom. Make sure that whatever solution you create, it maintains these principles.

It is impossible to convert ethics into code. You can't know for sure what negative consequences may arise from complex systems. ",1528063873.0,e027tp6,t3_8ob9ph,artificial,t5_2qhfb,2018
37,farticustheelder,"The title is bullshit, and the article doesn't understand the topic.

Just about the entire web is AI based and I think I heard that internet actually got big enough to have an impact of the US economy.

AI is also just about to have a very negative impact on the economy as 'AI Winter II' settles in. Self-driving, an AI application, is not ready for prime time. When this observation becomes accepted wisdom there will be a lot of write offs and layoffs.

As to all the other AI applications that are springing us as the various frameworks are opened up: good luck.  On the fairly harmless side are the people who don't understand AI but still think that magically a cash cow app will train itself into existence.

Not so harmless is deploying algorithms that are poorly understood, if at all. I wonder what 'reasonable care' might mean in  future lawsuits and I certainly want to follow the Uber pedestrian case when it starts. ",1528080354.0,e02myrc,t3_8nj0d5,artificial,t5_2qhfb,2018
38,spudmix,"There's two very disparate paths here, which very much depend on what you mean by ""Artificial Intelligence"". Most of the developments in what we currently call AI, like Alexa/Siri, self-driving cars, AlphaZero, etc. are applications of machine learning. This field is an extension of computer science, and if you want a career ""dealing with AI"", as you say you do, then I think your best path is this one. I'm a little biased here, as this is the area my qualifications are in, and is a portion of my current work, but I think it's a great field to get into now.

The other side of AI is the less computery side - if you want to have an informed opinion on AGI (by which I mean ""true"" AI) then you could also be well served studying philosophy of the mind, cognitive psychology, neuroscience, etc. It'd be difficult, I imagine, to build a fruitful career in AI with these study areas, though that could well change in the near future.",1528862585.0,e0l0v10,t3_8qnl41,artificial,t5_2qhfb,2018
39,Runner1928,"Action selection is complicated. ""Just placing bets"" exposes you to losing a lot of money, since you're trusting a computer to not screw things up. Same with self-driving cars: the computer vision algorithms are ML, hooking it up to drive for you is AI.",1529179556.0,e0sc0sq,t1_e0sb2d7,artificial,t5_2qhfb,2018
40,claytonkb,"I keep saying it: This time really is different.

&gt; The pace of AI breakthroughs seems to be slowing ... 

wuut?! I read papers almost every night in my spare time and I can barely keep pace with the new developments coming down the pike; many of the most recent developments have *far larger* implications than the early DL breakthroughs like image-recognition and speech-synthesis. In short, the second-derivative of ML change is way positive... we are in an acceleration phase (technologically, not just financially) with no end in sight.

&gt; Several top AI researchers—such as Andrew Ng and Yann LeCun—who had been hired by big tech firms (Baidu and Facebook respectively) to head in-house AI labs have left (Ng) or moved into slightly less prominent roles (LeCun). 

This is fantasy-football stuff... doesn't even rise above the noise-margin, IMO.

&gt; And most importantly, Piekniewski argues that the recent crashes of self-driving cars point to fundamental issues with the ability of deep learning to handle the complexity of the real world. 

This is quite true. But it also misses the point. Just 5 years ago, self-driving cars were *obviously* not going to happen within a decade. Today, you'd be crazy to bet money against autonomous vehicles being widely deployed in one or more major cities around the world within five years. While there are still a lot of hurdles to jump between here and there, the consensus among leading ML experts is that we have line-of-sight to solving them. At this point, it really is just a matter of crunching numbers and wringing out the corner-case bugs in the autopilot features.

&gt; Even more notable than the crashes, he says, is how often machines lose confidence in their ability to make safe decisions and cede control back to human drivers.

This is one of the areas where NN architectures are lacking across-the-board. Fortunately, we already have tools we can use to solve this problem that come from the domain of probabilistic algorithms (think ""Big Data"")... basically, we can use tools like Bloom filters, Count-Min Sketch and Locality-Sensitive Hashing to help the self-driving algorithm assess how similar the current scenario is to past scenarios it has encountered during training. This will allow the algorithm to not only assess what it should do next, but how *confident* it  should be that it knows the right course of action. Then, all you have to do is expose the vehicle to a much larger set of training scenarios than it will ever experience on the road -- this can be done in virtual environments by exposing the self-driving algorithm to everything from landslides to falling trees and electrical wires in its path to herds of cattle running down the road, to you-name-it. I don't know if Tesla et. al. are already doing this but the point is that *they can*... they have all the tools and expertise, it's just a matter of applying the right tools at the right price-point (more exhaustive training is, naturally, more costly).",1529443309.0,e0yble3,t3_8s91dh,artificial,t5_2qhfb,2018
41,jtsymonds,"TLDR: Neural Networks are powerful but complex and opaque tools. Using Topological Data Analysis, we can describe the functioning and learning of a convolutional neural network in a compact and understandable way. The implications of the finding are profound and can accelerate the development of a wide range of applications from self-driving everything to GDPR.",1529607544.0,e12c94t,t3_8sujcz,artificial,t5_2qhfb,2018
42,ZirJohn,a AI self driving in GTA V is a good test for training an AI in real life or in a simulator,1529696131.0,e14k2xl,t1_e14af35,artificial,t5_2qhfb,2018
43,rishirk,"It depends on what you are referring to as A.I.

There is a running joke between computer scientists that A.I. is what the computer can't do. A few decades ago sort and search were classified as AI, now even self driving cars are not considered full A.I.(s) ",1529812486.0,e171rup,t3_8tbmnw,artificial,t5_2qhfb,2018
44,entitie,"There's a book coming out called [*How Smart Machines Think*](https://www.amazon.com/How-Smart-Machines-Think-Press/dp/0262038404) that gives an overview of how different AI breakthroughs from the past couple of decades work (including self-driving cars, deep learning, recommendation engines, and Watson, the *Jeopardy*-playing program.  Upside is that it's accessible for a broad audience; downside is that it's not out for a couple of months and doesn't get into the nitty-gritty details for an engineer.",1529903496.0,e18x8ns,t3_8tnn3n,artificial,t5_2qhfb,2018
45,victor_knight,"Don't kid yourself. The government need only be concerned with a handful of giant corporations working on AI projects. If these corporations aren't already partially funded or completely in bed with the government, they know better than to release anything that might compromise national security.

&gt;There is no regulation on AI right now

Really? Try marketing a self-driving car that often kills people. You'll find yourself in jail. Try selling military drones that use AI to another country (again, welcome to jail). As for AGI, the instant it is demonstrated to possess even a child's consciousness prepare for half a dozen government committees that not only regulate but also determine the future path of any and all AGI research within the nation. Hell, even sex dolls are being regulated by certain countries now. They're not even robotic yet!",1530155970.0,e1f4mbu,t1_e1f4eth,artificial,t5_2qhfb,2018
46,Siepels,"I can't wait for completely self-driving cars. In my opinion, when that happens, so many traffic issues will be resolved.

No longer will we have to have our own personal car. Rather, some type of carpool could exist where a car could pick up multiple people that need to travel. Instead of it sitting idle at your workplace for 8+ hours, it can serve other people, therefore reducing the total amount of cars necessary overall.",1530632275.0,e1q0uht,t3_8vt1n2,artificial,t5_2qhfb,2018
47,Hotflux,"I think this idea would catch on quite fast, perhaps even allowing for people to put their personal self-driving car to better use. While it would normally sit idle, it could work in a way that Uber does, monetizing time that would normally be wasted.",1530632875.0,e1q1kd8,t1_e1q0uht,artificial,t5_2qhfb,2018
48,JoseJimeniz,"It won't be a very useful car.

I like the idea of this hypothetical machine that is capable of causing accidents. But physics of the universe does not allow such a device to exist.

- In reality I don't even need, or particularly want, a car that is incapable of causing accidents 
- I want a car that is capable of causing less accidents that humans

Each year 30,000 people in the United States, and 1.3 million people around the world, die in car accidents. if the entire world switch over to self-driving cars, and we cut that number in half, that would be a huge win. That would be a homerun. That is what we want.

It also means that self-driving cars would be causing 600,000 accidents a year. 

- We want self-driving cars to be causing 600,000 deaths a year
- because the alternative is 1. 3 million deaths a year",1530704406.0,e1rueex,t3_8vvmkv,artificial,t5_2qhfb,2018
49,04a8,"Eh, it's kind of moot because you have to catch up to the bleeding edge first before you exceed it.  The West meets this necessary condition, but China doesn't.  It's trying to catch up, and 'stealing' ideas is part of that.  Opinions vary on whether it's a legitimate means of doing so.  But I do think it's a bit early to say that Chinese are inherently 'not creative' when they still aren't in a place that would allow them to be coming up with novel things in high tech.

In my experience, Chinese people can be very creative, just in terms of day-to-day issues and solving problems that only exist in China.  There's lots of ingenuity around.  I've met people at  e.g. huaqiangbei that I found way more impressive than most Western classmates or colleagues.  The thing is they are not focused on the same problems.  They are working out how to shave 1 cent  per unit off a manufacturing process while keeping the client happy.  Or how to get around hardware/software restrictions from OEMs.  Or whatever such thing that just happens to be less glamourous than self-driving cars or whatever the public thinks is cool in AI.",1531821022.0,e2j7ca2,t1_e2iqv0q,artificial,t5_2qhfb,2018
50,Stone_d_,"The imagination of us 21st century humans certainly runs wild with all the new paradigms. You have a good imagination though because you've tamed it, I think you recognize the limitations of AI. 

I imagine there will be fewer and fewer farmers as per usual as time goes on. I wouldn't want just one farmer, definitely not zero, and I also wouldn't want two or three. I guess, if I had to guess, I'd say I want there to be a few thousand professional farmers at least. I love the idea that just a few inventions could be stitched together to automate farming, I just hope the system isn't put in place based on cost analysis alone. Things have been running very smoothly for a long time, food wise, government wise,  technology wise, just about everything wise, there haven't been too many systemic issues in our critical systems. Like, the technology that is trial by jury, I mean, that's robust. 

On the genetics I get the feeling we're gonna be disappointed. Like, my physics intuition tells me there just might not be ways of doing certain things. Like, to make a human you might be able to use any genetic code for a healthy human that's ever existed, but you couldnt modify genetic code so we live as long as sea turtles or jellyfish. It's the differences in our genetic code that make us humans and not jellyfish and can we really select certain traits we want? I get the vibe it's an all or nothing type deal, like if you change the type of food I can metabolize or you change the amount of time my body stays stable for, you're inevitably changing a whole lot more. Although, anyone could hypothetically change their eye color or their skin color or something like that, so why not change your lifespan. I guess one would have to be an expert to know whether getting your eye color changed can also effect your ability to metabolize potassium. From a physical perspective, let's say our solar system is actually a bit of genetic code for a much larger organism, I'd expect editing our solar system would have compounded effects because not only does every part of our solar system interact with every other part at least through gravity, our solar system is a unique set of masses suspended in equilibrium. You need equilibrium, orbits, the stuff of calendars to get an organism like people. Who's to say our genetic code is so malleable that it's evolutionary equilibrium can be broken without shattering the whole thing?  Obviously if people can Crispr their way to immortality I'm hopping on the bandwagon.

I hope society reaches a point where there is specific inspiration for the types of things you're wondering about, and also if you ever write specifically about how things might shake out from an engineering perspective, I think raw imagination is extremely valuable because it is so inspiring. How about government? What if an algorithm could undercut the cost of the current tax rate while still providing for the public good just as well as representatives? How about finance? We've all heard about transportation and self driving cars, but how about infrastructure, structures, and houses? Feel free to bounce ideas off of me, even the unpolished ones. Ideas are always inspiring
",1531995795.0,e2nnkmm,t1_e2nm18x,artificial,t5_2qhfb,2018
51,a4mula,"It's funny you'd mention transportation.  Here in Nashville was just shot down a 5-9 Billion dollar transit plan that revolved solely around the use of light rail.  While I'm not here to advocate for or against that, I did find it abysmal that not a single line of the bill took into account current and upcoming technologies such as ride sharing and self-driving cars.

So, it was something that got me to thinking, and contemplating.  

Is the problem really an issue with lack of infrastructure?

At any given moment I'd estimate (and it's just a guess mind you) that less than 10% of the available roadways in this city are actually in use.  Mostly interstate and main thoroughfare.  That still leaves the overwhelming majority of transit available for use.  But we don't.  It just sits there underutilized, while people complain about the traffic and make plans to spend more money than the top 5 bankruptcy filings of other major cities combined (excluding Detroit).

This is also an efficiency problem.  Something we've been discussing.  

If the rest of that infrastructure is put into play, would it solve the traffic issues this city faces?  Simulations would need to be run, but I'd venture to say that yeah, absolutely and by a very wide margin.

We already have the technology for smart routing.  It's built into every GPS system today.  Everyone with a phone has a direct link to some of the most powerful AI computers in existence, showing them how to get to their favorite hotdog stand.  It's a beautiful world.

Is it that big of a stretch to see this technology being used for more than just route suggestions?  

This gets a little heavy handed, but personally I'm fine with that, many aren't.  I'm personally not fine with having to have my brand new car Emission Tested, but it's the Law, and I abide by that.

During peak traffic hours, these routes are no longer suggestions.  You're given the least congested route and if you deviate from it, you're fined.  Simple, elegant, cheap solution.  Utilize the entire cities infrastructure and alleviate congestion.

Average transit times will decrease, even if any given route might be fractionally longer.  

You register your car, you're given the tablet, you're educated on the system.  Your data remains yours, encrypted and anonymous, until you violate, at which time the device notifies you that if you do not return to the route, you'll be fined.  If you continue to violate, your information is then, and only then sent and processed.

I caught a LOT of flak for this idea.  All the while people were ready to raise sales tax to the highest of any city in America to fund light rails that nobody would use.",1532000806.0,e2nqmpp,t1_e2nnkmm,artificial,t5_2qhfb,2018
52,ourtown2,"Microsoft CEO Satya Nadella Says Artificial Intelligence-First Approach Will Transform Our Lives
Recent advances in Artificial Intelligence have been ""Pretty stunning"" but what the humanity is going to see soon will be even more profound, Microsoft CEO Satya Nadella has stressed.

Addressing thousands of partners at the 'Microsoft Inspire' event here on Wednesday, Nadella said that the potential is for us to be able to turn every industry into an AI-first industry, be it healthcare or agriculture.

According to him, Microsoft is going to infuse everything with AI. ""It's going to have perception capability, language capability and autonomy that's going to be built into the applications going forward.""

""Autonomy is not just about a few self-driving projects. This is about autonomy everywhere,"" the Microsoft CEO added.

On its Azure Cloud offerings, Nadella said the computing needs will go far beyond the data centre.

""We are going to take Azure to Azure Stack, to Azure IoT Edge and to Azure Sphere. This is a ubiquitous, distributed computing fabric,"" he noted.

Nadella said that Microsoft 365 will help customers have people-centred experiences rather than device-centred experiences.
",1532008616.0,e2nxkv7,t3_90451f,artificial,t5_2qhfb,2018
53,Stone_d_,"I think for sure planned and enforced routes would make traffic a lot more efficient. Although, with full self driving fleets right around the corner why bother with anything besides full automation? By the time a planned route system started running smoothly it might be too late, there might be another city where everyone goes 200 mph on the highway because everyone's in a self driving Tesla and there are no more traffic lights or stop signs. If you really wanna save time and money, we can go further than the time wasted on driving. How about the time wasted at traffic lights? Merging onto the highway? Painting lines for merging? Painting big metal rusty signs and posting traffic updates with those digital signature? Do we even need street lights on the highway if we have self driving cars?

I can see a lot of counties and states avoiding self driving cars. I love driving, I love my car and I hope today isn't the last day I get to drive it. So I can see it happening, even with all the benefits of full automation some localities preferring to let people steer the wheel. But fining people based on their GPS data, even if the fining and scheduling process was all automated and it introudced zero bureaucracy, it has a sour note to it. Not to mention, we can already issue speaking tickets based on GPS and satellite cameras alone. We just don't do it because processing all that data would be expensive and computationally demanding, although it's probably be cheaper than paying cops to do highway patrol. 

So idk, maybe a better idea would be positive reinforcement, like an EITC if people follow traffic suggestions. Idk, your idea fits nicely into a social credit system. But people really hate paying fines, especially in America if you issue the fines digitally people love the word Orwellian.

",1532041135.0,e2ozet0,t1_e2nqmpp,artificial,t5_2qhfb,2018
54,a4mula,"I'm in full agreement with self driving cars.  This was a stop-gap bridge until that's a reality.  It was something that could be implemented right now and within a year be in place.

I also considered the the idea of positive reinforcement.  I don't dismiss it, but I do think trying to create that system new from the ground up adds to the complexity.  Ticketing and traffic violations are a system already in place.",1532044258.0,e2p2h36,t1_e2ozet0,artificial,t5_2qhfb,2018
55,a4mula,"And asking for a source is a lazy way of failing to do basic google searches.

Is your phone marginal?  How about your personal computer?  How about the internet?  How about your weather forecasting?  How about the radar that will guide self driving cars.  The list goes on and on.  Superglue, duct tape, penicillin (which you're wrong about), rocketry, and on and on and on.

This isn't contested information.  Citations aren't required for common knowledge.",1532072063.0,e2pop78,t1_e2piiub,artificial,t5_2qhfb,2018
56,claytonkb,"""AI"" is an amazingly broad term. It covers everything from autonomous controls systems (e.g. fire-control computers, industrial controllers) to physical robots to game-playing software to data-mining, pattern search, analytics, human expression recognition, symbol processing, deductive reasoning, inductive inference, spatial intelligence, mood-detection, text-to-speech synthesis, image-recognition, self-driving and many more. Go online and watch YouTube videos of people demoing real-world projects across these domains and see what interests *you*. Choose one or two specific domains (as specific as possible, like ""swarm-bot intelligence in robotic athletics"", eg [Soccer Bots](https://www.youtube.com/watch?v=aLy5pUsmpKE)) and then use that as the ""theme"" of your overall education. It's not so much about becoming the world's go-to expert in that one domain, as it is having one overall goal that you can attach all your *other* interests onto, so you keep everything moving forward and don't stagnate.

I recommend doing independent study. Take an introductory, college-level AI course.

* Google is offering [free AI courses](https://ai.google/)
* Khan academy has free online offerings in a variety of subjects related to AI
* Coursera has offerings for a modest fee ($50 range)
* [MIT Open Courseware](https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi) is excellent, though keep in mind you will be missing prereqs

Also, this is the perfect time in your life to sharpen your learning-to-learn skills. Posting here on reddit already exhibits one of those skills: taking initiative. Those skills are ""education amplifiers"" because whatever you choose to learn, you will learn it more effectively by applying your learning-skills. You might get half-way through your education then decide to switch to a law degree or medicine... your learning-skills will still keep paying dividends.",1533575305.0,e3plr1q,t3_94pbj5,artificial,t5_2qhfb,2018
57,spudmix,"Honestly, my prediction is that cars will become a subscription-based service or pay-per-ride in major metropolitan areas where AVs are most likely to prosper anyway. The landscape is ripe for this kind of development, with:

Ride-sharing apps (Uber/Lyft) being phenomenally successful  
The decline of driver licencing among all age groups  
The high cost-of-entry for self-driving vehicle technology

 It isn't a significant jump to the company owning (and insuring) fleets of autonomous vehicles, and leasing them out for a fixed period or per ride as needed.   
To answer your question directly, I think it's likely that we will keep a similar model of auto insurance as exists today, but the primary consumers of that insurance will be the companies that actually own the cars. Either that or they'll forgo insurance entirely and just pool their own risk under their budgets.",1534484843.0,e4c7jgp,t3_97z9y8,artificial,t5_2qhfb,2018
58,Sythic_,"Question in title means ""no.""

Uber has never been at the forefront of the self driving sector, only pretending. Other companies are moving along fine.",1534746994.0,e4i6c2b,t3_98qsp8,artificial,t5_2qhfb,2018
59,ConfidentTree15,"Read this article and had some thoughts about it...

[https://www.weforum.org/agenda/2016/10/top-10-ethical-issues-in-artificial-intelligence/](https://www.weforum.org/agenda/2016/10/top-10-ethical-issues-in-artificial-intelligence/)

I think a full-on AI future would be the best for our society while moving forward in our technology advances. We are already so obsessed and consumed with technology, like with our smart phones, tablets, etc. that I believe this would be the better fit for us. As we have been progressing as a society, technology has always made things easier for us. Like writing text messages, to vacuuming our floors, etc. and like the article stated, self-driving cars would make our lives way more easier and efficient. Already so many people are on their phones while driving, which causes car accidents, that this would be the perfect solution for it. Also, with people becoming more ""self employed"" and ""working from home"", the idea of an assembly line job will almost be non-existent with Artificial Intelligence taking people's places. Even though the article stated that people would loss their jobs, this would be more beneficial to the world of technology since more jobs will be opening up to operate the robots.

Let me know what you think of this article and my opinion!",1535031545.0,e4oxdvn,t3_99lm1l,artificial,t5_2qhfb,2018
60,Don_Patrick,"25% summary by [Core Summarizer](https://chrome.google.com/webstore/detail/core-summarizer/hiilcnldmlehobiillipbcdkhkfbigfk):

&gt;From facial recognition to self-driving cars, AI and machine learning  are set to become intrinsically interwoven into the fabric of daily life  and in years to come it will increasingly shape our perception of the  world around us.  
  
&gt;  
&gt;Companies and governments are welcoming these  new technological trends as they have the potential to let computers  make decisions and take action. In particular, women are severely  underrepresented in this area, which has resulted in gender-biased AI.  
  
&gt;  
&gt;Research  conducted by WIRED and Montreal-based startup, Element AI, revealed  that only 12% of leading machine learning researchers were women. This  gender imbalance suggests that the group spearheading this technological  revolution is less inclusive than the broader tech industry.   
  
&gt;  
&gt;Along  with redressing the gender imbalance, the AI community needs better  representation of ethnic minorities. The companies' algorithms were near  perfect at identifying the gender of men with lighter skin but  frequently erred when presented with photos of women with dark skin.   
  
&gt;  
&gt;As  we become more dependant on AI and machine learning for decision making  this imbalance must be addressed or women and minority groups could  face even worse discrimination in the future. 

&amp;#x200B;",1535105927.0,e4qu9i2,t3_99vnvy,artificial,t5_2qhfb,2018
61,dallas1995,"The future of economy is based on exponential technologies. We on the verge of creating an economic revolution. The last economic revolution was driven by oil, automobiles, and electrical communication. The new economy emerging is driven by the convergence of renewable energy, self-driving transport and digital communication. 

The nature of this new economy will be very different. We are transitioning from an economics of scarcity to an economics of abundance. This new kind of world, is creating massive disruptions in our economies. 

AI creates an economy of abundance. If we are to truly grasp the consequences of AI, we need to, at a minimum, understand what the economics of abundance is all about. An economy driven by AI, promotes decentralization.   
Why is decentralization? This is because people need to feel that they are in control of their own lives. It is what allows people to work on what they love and what they are good at while remaining self-sufficient. They see their contributions improve the lives of the people around them. Life, liberty and the pursuit of happiness (not property). 

I won't say that this transition, from current economy to a new one, would be easy. It'll have its downside too but we need to prioritize our goals. Are we going to continue with the age old ideology or are we willing to change?

Any revolution that happens is driven by curiosity of the common man. He needs to raise his standards and be farsighted to embrace the tech and drive the revolution.",1535434680.0,e4yl5f7,t1_e4wzrvg,artificial,t5_2qhfb,2018
62,MagneticWaves,Probably medical analysis later or self driving but those are still in development.,1535624613.0,e536sww,t1_e534qwz,artificial,t5_2qhfb,2018
63,CyberByte,"&gt; Shouldn't we focus more on our training sets then? So far AI developed is industry specific. We know what we aim to achieve. We know the set of data and to some extent even the results expected. With adversarial research in picture, is it possible to determine and filter what's not required?

There is a difference between knowing what we aim to achieve on an abstract level, knowing it on a formal level, and having the data that somehow covers this exactly. Getting a more representative and diverse training set is always a good idea, but how can you be sure that it ""covers"" everything? And how could you even define that? 

In practice your training set cannot contain everything you will ever encounter. If it did, then you should just use that as a lookup table rather than using a learning algorithm. So there always has to be some generalization to unseen situations, and it needs to happen in the right way. And you cannot really tell this from the data alone, so you also have to investigate and understand the learning and decision-making algorithms themselves. 

Adversarially generated examples can be used to augment your data set, but that alone isn't enough. 

&gt; I do believe, we need to form some sort of industry specifications. But we would need to review it, who does that? AI or Industry Experts?

Review should happen by both, to make sure it makes sense from both an industry and an AI perspective. But I think industry experts and policy makers should be the driving force behind most of such specifications, with AI experts providing advice. In medicine, I think doctors or the FDA or whatever should get the final say in what kind of procedures are and aren't acceptable. With (self-driving) cars, it should be whomever is in charge of car/traffic safety and regulations. 

These people should be advised by AI experts, who may be able to tell them what is and isn't feasible and who may be able to point out the benefits of AI. For instance, it's imaginable that there is some rule that AI doesn't meet, but that the benefit of AI is so great that it's worth changing this rule. 

I also think AI experts should be making guidelines for their colleagues about some best practices (e.g. regarding fairness in algorithms), but industry experts and policy makers should have the final say in their industry. 

&gt; Bringing in human interference may again lead to bias or bring in personal judgement. How do you perceive, this will be possible? With people wanting to make their hold on the power, you think we'll ever be able to form a consensus? 

This is a pretty run-of-the-mill political issue in my opinion. Ideally it should be as open and transparent as possible, so that people can publicly debate these issues. I'm not sure there will ever be a consensus, where most people agree on one direction, but there may be a compromise. Or some decisions may be imposed by whomever is in power, but this is a general human problem IMO.

&gt; Also, from the point of view of a researcher or developer, what good it holds for me to be so open about the progress of my findings? Ideally, this would boost further developments but how acceptable is it from a technical aspect?

As an academic researcher it's pretty much your job to create knowledge for the world to consume. For others there can still be incentives in the form of fame, altruism, getting help from the community, being at the forefront or in control of technology that many people use, or strengthening an (tech) ecosystem you like. 

&gt; But examples like those of AV crashes, where people either didn't understand the functioning or did not trust the tech suffered.

I don't think the way in which the average Joe thinks about tech is very rational. As long as AVs are a rare novelty, their crashes are going to stand out (even if they are much rarer than with people-driven cars), and it will affect public perception. One way around this is just informing policy makers, who will then presumably approve these cars, which will then make them less rare (and people will eventually accept this as the new reality). Another way may be to convince some journalists / talk show hosts / whatever and have them endorse AVs to the people who trust them. ",1535699257.0,e556o0i,t1_e5363jg,artificial,t5_2qhfb,2018
64,Ularsing,"Honestly, I'm fine with it. DARPA may do some evil shit in the shadows (I don't know of any specifically, but I'm sure they have *questionable* projects), but for the most part they're incredibly generous sponsors of moonshot basic research. The DARPA Grand Challenge was a huge component of advancing self-driving cars to the point that they became commercially viable research projects.",1536423437.0,e5m7u5a,t1_e5m5ovp,artificial,t5_2qhfb,2018
65,Juniper00e,"Well that is an understatement.

In 5 years self driving cars that use sensors might actually be approved for use by consumers.",1537417614.0,e6atd01,t3_9hc2s9,artificial,t5_2qhfb,2018
66,guttermouse,"comparing humans to machine intelligence aside, the fact is that we are taking a highly invasive approach to machine learning which is not 100% necessary.

Right now most of the corporations are using massive amounts of data and pattern matching algorithms to extract ""tags"" or categorize data based on thousands or millions of examples.  This is an approach that works most of the time, but the downside is the computing power necessary for training.  Another major con to this method is the limited number of patterns that can be recognized while everything else is treated as garbage or noise.  The overall example of this being imagenet, using millions of examples tagged by humans.  Outside of object recognition in images and video, other systems like the ones that facebook and google use are also based on the same idea: train on big data, find interesting patterns, exploit those patterns on the other side.  It all comes down to statistics using these methods.

But perhaps there is another way.  For instance, take the case of object tracking in a video.  To track an object, the system must first take a frame of video and then scan it for known objects.  After finding the location of each object, it goes to the next frame and finds the location again.  Given that prior training was given, the system is pretty efficient.  To recognize a face, a current ML system needs at least 100,000 training examples which takes a lot of time to gather and process.  Even then, why are there no self driving cars in India?  Because there is no way to categorize the chaos that is an Indian street.  Even some reports say that rickshaws (which are as common almost as pedestrians) are so customized that a self-driving car cannot recognize them.

The way I see it, there are way too many new types of possible patterns and objects to recognize in any data set that is using real-world data.  If there are an infinite amount of possibilities, it means that there is no upper-limit as to how much processing will be needed in order to successfully categorize everything.

Ever since I was a kid, I hated tags.  It seems everyone has to dumb someone else or something else down into a category so that their brain can stop working on trying to figure things out.  Why can't you see another human as simply who they are?  Why must you label them as whatever you label them as?  Even then, many different cultures have many labels which have many different meanings. So with the ""categorization"" approach, MI pundits are trying to put everything into a standardized category which is not realistic.

All I'm saying really is that with properly formed AI, you won't need a cloud to process your data for you and return an answer.  Current smartphones and computers could run the algorithm with little CPU power and generate more impressive results. ",1538830904.0,e79rg3l,t3_9lpr59,artificial,t5_2qhfb,2018
67,CyberByte,"&gt; it would appear that these types of achievements have led some to question the continued development of such technology as they fear what would happen if it became uncontrollable

While I can't exclude the possibility that *some* people are scared because of self-driving cars and cancer diagnosis, those are not reasons that the experts are concerned. To read more about that, I recommend checking out the wiki and side bar of /r/ControlProblem. 

&gt; However, this may prove to be short sighted considering mankind has always been able to adjust alongside technological advancements (the industrial revolution, arms race, etc). 

This line of argumentation works in many cases, but it cannot really predict anything new. Opponents would argue that ""this time is different"", and if you want to reason as you did above, then I think you should at least argue that unsafe superintelligence (or whatever you want to talk about) is like the other things you mention. (This can probably be done, BTW.)

Another potential problem is that in the past, people or groups of people have indeed died because of new technology. Mankind survived, because other (groups of) people were still around in the aftermath. So basically, someone could argue that this would be like the past, but just bigger, so that this time there wouldn't be any humans left in the aftermath of the initial problem to fix things. 

&gt; nothing indicates that having an all encompassing AI is the desired goal

What do you mean by ""all encompassing""? Companies like DeepMind are saying they'd like to ""solve intelligence"" and many others are actively working on artificial general intelligence (AGI). It is however true that *most* people in AI are *not* working on this, and in particular that the kind of narrow AI that powers self-driving cars and cancer diagnosis has very little to do with AGI. I'm pretty sure we can continue making better cancer diagnosis systems without that leading to AGI, in the same sense that developing better toasters won't lead to creating a nuclear bomb. 

You could also try to argue that AGI is still very far away, although that's [difficult to say](https://intelligence.org/2017/10/13/fire-alarm/) and [people have widely different opinions](https://aiimpacts.org/ai-timeline-surveys/) on that. 

I also don't think that many AI safety professionals (i.e. the people actually working on this) are arguing against the continued development of AI. Both because it's impractical, and because of all the good AI can do (among other reasons). I think many of us would (in theory) like there to be some regulation, which might have a slowing effect on development, but this would typically be considered secondary (not always though). I'm sure there are people who want AI development to stop, but the fact that even the people who are professionally concerned about A(G)I don't advocate that is probably a decent argument in favor of your thesis.  If you're interested in more, I recommend looking for Nick Bostrom's writings on policy and governance. ",1539276868.0,e7l1447,t3_9nb4l5,artificial,t5_2qhfb,2018
68,academc,"Personally, I think change will come faster than people expect but it will be unevenly distributed because 1) wealthier people and corporations will be early adopters due to costs of development and 2) the mass rollout of technologies like AI and AR will need to be product-tested for safety, whether to work out bugs or in order to garner popular support from a reluctant public (e.g., see self-driving cars, which I imagine are probably technically viable already for limited release but most people are afraid of the dangers even though they're statistically safer). So the adoption speed is a cultural/economic question more than a question of technological advancements. 

That said, some AI will be instant--so instant that people don't realize that many AI advances are already here, working quietly under things such as Internet infrastructure, data collection and processing, finance and market regulation, etc. And in an invisible way, that's all augmenting reality because the world is an augmented version of itself from 5 years or even 1 year ago. It's just a matter of realizing this is so; whatever an individual decides is the hinge of your question's use of ""instant"".",1539677863.0,e7uxmmu,t1_e7umqqd,artificial,t5_2qhfb,2018
69,BernardReid,A self-driving taxi ride could be much cheaper than a conventional taxi? No this is not.  The reason is level 5 full automatic self-driving car is very expensive to build.  I think current Google car costs several million $ per vehicle.  Cost will be reduced if it will be mass production but full automatic self-driving car is still expensive than ordinary taxi cabs.  Apple announced new Macbook air with retina display this week.  Macbook air has been priced less than $999.  But New Macbook Air is priced starting from $1200.  This will happen to full automatic self-driving cars.,1541120994.0,e8vwlda,t1_e8vknj2,artificial,t5_2qhfb,2018
70,CyberByte,"I'm not aware of any significant effort to get AI banned as a whole, but the comparison to nuclear weapons does immediately bring to mind the [campaign to stop killer robots](https://www.stopkillerrobots.org), which is about autonomous lethal weapons. I think the website is rather weak, but you can read their arguments there. I think you can also find some stuff about this on the Future of Life Institute's website (including some open letters with lots of signatures from AI professionals and companies.

There are also other initiatives that call for or investigate the regulation of AI, usually in specific domains (e.g. self-driving cars or online privacy). ",1541361198.0,e91ugp4,t3_9u2ryq,artificial,t5_2qhfb,2018
71,ninimben,"At some point self-driving cars will be better than human drivers. In that case it will be hard to argue that they are awful because they sometimes kill people -- since human drivers will be worse.

What's awful about self-driving cars to me is that they are testing prototypes on the open road and the fatalities are accepted by the government because of the promised ramifications for transportation (saved labor costs, probably increased efficiency all-around).",1541837564.0,e9es4pq,t3_9vls0i,artificial,t5_2qhfb,2018
72,GiacomoTesio,"&gt; At some point self-driving cars will be better than human drivers. 

This is possible but not probable with the current approach.  
The only way to make them more flexible than a human brain is to build dedicated roads, with proper sensors unders the asphalt, control towers and standard protocols for them to communicate.  

However, even so, who build them must be held accountable for the death they cause: we should be able expect the same safety that airplane control systems grant, with similar development process and comparable costs.",1541854713.0,e9f0mec,t1_e9es4pq,artificial,t5_2qhfb,2018
73,BernardReid,"It seems Transdev self driving school bus has no school bus safety features such as flashing red lights, cross-view mirrors, stop-sign arms.   Here's fed rules on school bus,  https://www.nhtsa.gov/road-safety/school-bus-safety .  Eventually every testing-purpose self-driving vehicles are required to run with human driver.  Self-driving mode is disabled easily pressing red button on driver panel. So there's no possibilities bus control was stolen thru the wireless network connection.  Main reason why this self driving bus was banned by fed is it is not fulfill school bus standard but not cyber security reason. ",1542283095.0,e9qoui0,t3_9wgq67,artificial,t5_2qhfb,2018
74,cas18khash,"You don't let a 5 year old practice ""disrupt"" your whole legal system. We've had overtly cautious self driving cars for a good 5 years now but you don't see every city bus becoming a self driving one.

I'd be very upset if impenetrable algorithms started handing out judgments. People are already uneasy at those algorithms that determine the chance of reoffending.

Personally I'm comfortable with humans parsing law for another 10 years just so that safety, accountability, and deterministic decision making is guaranteed.",1542298332.0,e9r5ilv,t1_e9qslzj,artificial,t5_2qhfb,2018
75,CyberByte,"**Computer Science (Data Science)** sounds almost optimal for AI (except of course for programs that directly cover AI/ML). I would definitely go with that. 

I think software engineering and high-performance computing (HPC) may both come in handy, but I don't think scientist-level knowledge of either is needed. All CS programs should cover programming to a (mostly) sufficient degree, and while it's possible that you need to work with HPC at some point, you'll probably only need user-level knowledge. 

Extra math in Computer Engineering sounds nice, but overall it just sounds like a completely different degree with stuff that is less directly relevant to AI than CS is, especially with a Data Science focus. And in the future AI may become big(ger) in cybersecurity, and we'll probably want to secure our AIs (e.g. you don't want your self-driving car to be hacked), but I'd say it's not really at the core of AI.

Of course, if you have a particular interest in any of these, you can still choose them and take them in an AI direction for yourself. But otherwise I'd just go for the CS program with the Data Science focus.",1542331558.0,e9sdfbz,t3_9xf5jx,artificial,t5_2qhfb,2018
76,ninimben,"The author seems to miss that a lot of this research is more about teaching researchers about reinforcement learning... like, nobody thinks an algorithm developed to play Super Mario is going to evolve one day into a self-driving car algorithm...",1542815128.0,ea665fv,t3_9z1z32,artificial,t5_2qhfb,2018
77,astrobaron9,Zero emissions and self-driving have very little to do with each other.,1543235086.0,eahxrpd,t1_eahw80n,artificial,t5_2qhfb,2018
78,sasksean,"Well for example a monkey has half the neurons a human has but cannot come to the same conclusions in twice the time.

I was thinking that maybe it is possible to alter the monkey's input since birth in such a way that less of the brain is dedicated to the senses and more to higher level cognition. Basically if it were essentially a vegetable with manipulated input from birth, could the brain organize itself in a radically different way. Train a monkey brain to process the inputs for a self driving car for example if it has no concept of walking, eating, jumping, etc.",1543602811.0,easx65m,t1_earyg3i,artificial,t5_2qhfb,2018
79,f10101,"It's because AI is still undeveloped. It's much easier, at the moment, to build a working, useful system with labelled data than to try and have the AI figure things out from first principles itself. Why force an AI to learn human knowledge, and human categorisation of the world, from scratch, when we can teach it? Say, for self driving cars: how can we teach it the rules of the road, if we haven't told it what a stop-sign looks like? It would have to have thousands of t-bone crashes (in real life or simulator), before it associated red signs at the side of the road with a requirement to slow down and stop. 

We have algorithms that can do this in theory, the reinforcement learning approaches that beat games do this, but it's very hard to extrapolate that up to the real world. It's much, much easier just to say: ""red octagon = stop"", and let the AI focus on learning more subtle relationships about the world that we can't label.

But no, longer term, label farms won't be needed. AI's will likely be able to deduce human interpretation of the world from existing sources, or very sparse labels. But that's a few years off.",1543686330.0,eav5xok,t3_a21vey,artificial,t5_2qhfb,2018
80,Laboratory_one,"PC games are still very difficult and by no means completely solved by ai. I’m not sure we are going to find agreement on this topic though. 

As for other problems to solve by ai, there is the issue of self-driving, translation, understanding, etc. The most important is being able to generalize between tasks. What’s impressive of the latest alphago zero is that it can learn other simple games. That is part of the big step it took. 

I would implore you to check out some of the challenges OpenAI had to overcome to beat DOTA, and of the DOTA ladder system. There are some very interesting pieces in their research. ",1543950167.0,eb334qt,t1_eb2zwdt,artificial,t5_2qhfb,2018
81,CyberByte,"What you're describing is essentially the emerging research area of multi-agent learning and communication in multi-agent systems. There are negotiation systems/agents that can communicate/accept/reject each other, but I think they typically don't learn much beyond the history of the current negotiation.

Multi-agent learning is potentially quite interesting, but it's still very hard to do (not in the last place because communication is hard). This is quite funny (to me), but there's one article called ""[If multi-agent learning is the answer, what is the question?](http://www.ppgia.pucpr.br/~fabricio/ftp/Aulas/Mestrado/AS/Artigos-Apresentacoes/MultiAgent%20Learning/10.1.1.70.8898.pdf)"" and a response called ""[Multiagent learning is not the answer. It is the question](https://s3.amazonaws.com/academia.edu.documents/61156/8ia5648udi5ugmt9k6f.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;Expires=1544614995&amp;Signature=DZoLfAB4c6tituSEH%2Brgma9ViiA%3D&amp;response-content-disposition=inline%3B%20filename%3Dpdf_101.7kB.pdf)"". 

I guess the main reason to want to do this is when you have agents with different knowledge, and you want them to learn from each other. But when does that actually happen? The most common setting is probably something akin to self-driving cars, which are constantly gathering more data and (potentially) learning from it. But instead of inefficiently communicating with each other one-to-one, and potentially not trusting each other's communication or judgement, it's much easier to just send all training data back to the manufacturer who uses it all to train (and extensively test) a new model which is then pushed directly to all cars.

I do think that questions like yours can potentially be somewhat interesting from a research perspective. But the practical applicability is a bit more questionable (especially right now).",1544611983.0,ebmgq69,t3_a5b7xj,artificial,t5_2qhfb,2018
82,CyberByte,"Maybe... But as long as we don't have actual AGI, it seems pretty difficult and impractical. 

In your example the robots presumably have the same owner/manufacturer the same ""trick"" as with my self-driving cars example might be possible. It's probably much easier for robots of type A (or doing task A) and robots of type B to both upload their data to a central repository, where it's integrated by some human-designed algorithms that train updated policies for type A and type B robots and update them remotely. Otherwise you'd have to make these robots to not only be capable of their own task, but also useful communication with a different type of robot. Either robot A needs to understand B sufficiently to only communicate useful information, or B needs to be able to filter useful information from communications from other robots. Even if they can always trust each other, and you can find a way to incorporate the message into the receiver's knowledge, it still seems a lot less efficient than the hivemind approach that would share the knowledge of all robots with all robots all the time. 

I do still think it's interesting. It definitely seems useful to figure out the required message-to-knowledge integration procedure. And this can be useful for simulating societies or something. ",1544620090.0,ebmm424,t1_ebmjhze,artificial,t5_2qhfb,2018
83,OnlytheLonely123,"“ Fully Automated Self Driving Cars, coming this summer!”

Experts in the field predict “ Fully Automated ” to still be decades away.",1544832750.0,ebt9752,t3_a6a7z6,artificial,t5_2qhfb,2018
84,OnlytheLonely123,"WAymo has definitely been a source of confusion.

Ultimately they will have people driving them from e remote location like drones &amp; also someone at the breaks if an emergency arises.

https://www.google.com/amp/s/www.wired.com/story/waymo-self-driving-taxi-service-launch-chandler-arizona/amp",1544844754.0,ebtl5bx,t1_ebtjbod,artificial,t5_2qhfb,2018
85,OnlytheLonely123,"-“But Waymo hasn’t reached a point where it’s confident in its cars’ ability to stay safe without human oversight. And it’s not sure when it will get there”

https://www.google.com/amp/s/www.wired.com/story/waymo-self-driving-taxi-service-launch-chandler-arizona/amp

",1544844931.0,ebtlbkn,t1_ebtjbod,artificial,t5_2qhfb,2018
86,Don_Patrick,"I composed an overview of exactly that: [the most sensationalised A.I. news stories](https://artistdetective.wordpress.com/2017/11/17/the-most-sensational-news-ever/) of recent years.

As the others say, most recently it's mostly been Sophia the robot getting citizenship / a visa / a credit card / whatever, and a boatload of generic stories that claim that Blockchain and AI ""are the future"", while offering no concrete how or why.

There are recent headlines about AI being racist and sexist (i.e. their training data was lopsided, therefore their statistics are lopsided),

There are recent headlines claiming that robots are racist (While they actually mean ""robotics"", in that most are painted white, surely that's because of racism /s)

You can probably find some escalated headlines on MIT's recent ethics survey [""Should a self-driving car kill the baby or the grandma?""](https://www.technologyreview.com/s/612341/a-global-ethics-study-aims-to-help-ai-solve-the-self-driving-trolley-problem/)

edit: Here's another one about Google's Duplex AI (it makes reservations via phone calls and says ""um""): [""Google’s AI fest offers an ominous glimpse of the robot future""](https://www.ft.com/content/99635a6e-540b-11e8-b3ee-41e0209208ec) . There are worse articles written on it, but not with much worse headlines.",1544889459.0,ebui11u,t3_a6a7z6,artificial,t5_2qhfb,2018
87,steezyone,"No they won't... Lol that is like an emergency thing at best.  If the car gets stuck and confused or something like that.  Definitely only a back up.

The people in the car now are just a safety thing, or again if the car gets confused and stuck.  They will be gone as soon though.  It's not a good business model to spend thousands on sensors for self driving then still have people drive the car.  It's the same as Uber but more expensive then.",1544894033.0,ebunvdb,t1_ebtl5bx,artificial,t5_2qhfb,2018
88,OnlytheLonely123,"So youre defining Self Driving as having 4-6 different levels and believe we are at the end level?

Because everyone else deveoping the technology states counter to your claim.",1544906642.0,ebv4uqw,t1_ebunky0,artificial,t5_2qhfb,2018
89,BernardReid,There's 1 to 5 level of self-driving car exist.  Level 5 can support full self driving feature but so far no one have testing Level 5 self-driving car in the public roads.  Auto pilot feature of commercial airplane is almost same as level 1 or 2 of self-driving cars.  So fa most advanced UAV is Northrop Grumman X-47B.   This plane is only one automatic UAV.  X-47B can taking off and landing automatically if human operator approved.   USAF did test X-47B on the USS George H.W. Bush in 2013 to 14.   But this testing was not all finished successfully.   X-47B could landed successfully if weather condition is good but not gone well under the bad weather condition.  So far full automatic pilot is on the way of early developing phase.  Future commercial plane will support full automatic feature then eliminate co-pilot first. This will be happen next 10-20 years I guess.  I think what anonyngineer is writing is good guess.   ,1544966223.0,ebwotr5,t3_a6hbfg,artificial,t5_2qhfb,2018
90,OnlytheLonely123,"- “Very few people think it will be anywhere near 20 years for full AV,”

So the NHTSA states otherwise.

Either you are incorrect or they are, which is it?

NHTSA: Level 5 - Automated Driving System (ADS) on the vehicle can do all the driving in all circumstances.  The human occupants are just passengers and need never be involved in driving.

Let’s be clear: fully automated or “self-driving” vehicles aren’t arriving in showrooms tomorrow; they’re likely years, maybe even decades, away. What we’re experiencing is an evolution in vehicle safety that is leading toward cars and trucks that help us drive more safely.

— NHTSA.

https://en.m.wikipedia.org/wiki/Self-driving_car

5 Levels of Self Driving Autonomy.

https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety

An interesting article in agreement with the NHTSA timeline.

“Ultimately, it will likely be a long time before Level 5 becomes a reality, if indeed it actually happens at all.”

https://www.cnet.com/roadshow/news/self-driving-car-guide-autonomous-explanation/
",1545006413.0,eby3djr,t1_ebxuqom,artificial,t5_2qhfb,2018
91,Ismoketomuch,"I think there is a huge mental gap for some people. There are people who think that AGI has to be a single program with every ability in it. 

I believe that many AGI programs will suddenly coalesce. Many organizations are solving very specific problems, this is sometimes called “narrow” AI, which is fine. But at some point hundreds if not thousands of these narrow AI will conglomerate into one big problem solver.

Just like the human brain is actually a bunch of compartmentalized areas with very specific functions. 

I think actually this is where the illusion of consciousness arises from. All these different parts cooperate loosely into a meta cognition and this meta cognition is what we call the “self”. We think we are one thing but really we are many things and many things that we are do not even share the same DNA. 

I dont know when or how but at some point, self driving cars, facebook facial recognition, google search and amazon predictive analysis, search and store algorithms with deep learning, 3d gaming and so on are going to coalesce and suddenly it will all snap together and we will fucking shit the bed. ",1545127100.0,ec1b7yf,t1_ec0zjxv,artificial,t5_2qhfb,2018
92,steezyone,"Well, the quote you have says ""years, and maybe decades away"".  He definitely said maybe there.  Some people say 20 years, but again their deffinition is kind of dubious.  To be a useful self driving car (even level 5) it doesn't have to counter ALL conditions.  Like getting dropped in the Sahara desert.  Although I wouldn't even say we are 20 years from that.

And again, if you took an industry pole the outliers are the ones saying 20 years. Look at what the major companies are saying and where they are putting their money.  If the consensus was 20 years we wouldn't have that kind of investment and marketing.  

Watch the video here and then you should see that it isn't 20 years away:

https://electrek.co/2018/10/15/tesla-new-autopilot-neural-net-v9/
",1545164489.0,ec2e66o,t1_eby3djr,artificial,t5_2qhfb,2018
93,victor_knight,"Somehow I suspect even the ""best"" AGIs will be deficient in some way or other compared to humans and this will be used as a justification by some (countries, even) to *not* use it or simply outlaw it so that even businesses will have to employ the locals. For the same reason, medical doctors will still be around for a long time regardless of whatever medical AGIs are there simply because they can ""talk to"" and ""understand"" patients (e.g. by looking at them, smiling) and the patient knowing they are talking to another human being with similar life experiences etc. 

Even with self-driving vehicles, all it takes are a few catastrophic accidents for the statistics to be ignored and people to have drastic knee-jerk reactions that will slow or completely stifle its global adoption. How many people will get on a plane piloted entirely by a robot? That adds a whole other level of drama compared to a car or train. Look at bullet trains. They are perhaps the ""best technology"" (if properly implemented) yet how many trains globally are actually like that? They seem quite satisfied with what they have. Even overall aircraft design hasn't improved much since the 1960s. The fastest manned aircraft was developed in the mid-1970s and remains so to this day. 

It could very well be that most people will be satisfied with a given technology and not continuously seek out major improvements. Look at Boston Dynamics. Are they really aiming for a human-looking android (e.g. Data from Star Trek) or will they pretty much stop at some point with whatever seems to work for whatever reason it was funded? Look at buildings today compared to pyramids (from *thousands* of years ago). Have they improved as much as we'd expect in that time?",1545185780.0,ec3480j,t1_ec1kn4h,artificial,t5_2qhfb,2018
94,steezyone,"Well I am also an engineer developing the technology so... I kind of know what I am talking about.

As I mentioned your quotes show outliers at best, and even then they say MAYBE decades. 

As for the video, it is actually from a hacked car, so it shows exactly what the cameras output (no bias involved).  For perfect self driving the only real challenges are perception and dealing with the other people on the road.  From the video you can see the successful segmentation of other vehicles and drivable surfaces.  Although, not 100% perfect, it is close.  Given that nothing even close to this was possible even 5 years ago its hard to see us not having perfect perception way before 20 years.  

As for dealing with other people, because apparently my sources weren't great last time, read this paper:

https://arxiv.org/abs/1812.03079

Given this and the rapid progress of the last few years, there is no way we are 20 years out...",1545189846.0,ec38ur0,t1_ec2p7lu,artificial,t5_2qhfb,2018
95,victor_knight,"&gt; AI research doesn't require many resources.

Actually, it does. Groundbreaking research, at least, which is what I said. This is why hardly anyone else developed a system that beat the Go world champion or builds self-driving cars, for examples. If pretty much all you're going to do is ""apply"" well-known techniques on relatively small data sets, then just about anyone can do that on their notebook computers but it doesn't count for much.

&gt;Can you show me some examples of things that were patented years ago by big tech companies that were only released publicly recently?

They don't really declare these things as such but it makes sense if you consider issues of national security and the nature of profit-oriented corporations. Simply making expensive research outputs/technologies free for everyone doesn't make economic sense and would be unsustainable. The open source model is an alternative but I don't think this is what Google/Apple/Microsoft/IBM/Facebook etc. generally adopt.

",1545790737.0,eck79bf,t1_ecj6vx7,artificial,t5_2qhfb,2018
96,Lost4468,"&gt;      
&gt; 
&gt; Actually, it does. Groundbreaking research, at least, which is what I said. This is why hardly anyone else developed a system that beat the Go world champion or builds self-driving cars, for examples. If pretty much all you're going to do is ""apply"" well-known techniques on relatively small data sets, then just about anyone can do that on their notebook computers but it doesn't count for much.
&gt; 
&gt; 

Most of the AI research in the past several years was done by university researchers on between close to minimum wage and more than a healthy living (+ maybe a few thousand dollar old rig for it or maybe 10 grand free on Google Cloud/AWS). All the projects you mention had marketing as their number one contributor, for example Geohot built a self-driving system that's arguably better than Tesla's and it only costs $1,000 to buy a dev kit from them. As geohot himself said in one of his streaming videos, the actual software cost pretty much nothing to write as he just wrote it using some crappy hardware.

&gt;They don't really declare these things as such but it makes sense if you consider issues of national security and the nature of profit-oriented corporations.

What do you mean they don't declare them? You're aware that in order to get a patent you need to describe the thing you're patenting? You can try and hide parts of the implementation but it may become to general to enforce, and you generally won't be able to enforce anything on parts you didn't mention. Try finding some patents from Google on AI from 10 years ago that were any more advanced than the public knowledge on AI 9 years, 6 months ago.

&gt;Simply making expensive research outputs/technologies free for everyone doesn't make economic sense and would be unsustainable.

It's not, and I already explained why it benefits both parties to share their methods with each other.

&gt;The open source model is an alternative but I don't think this is what Google/Apple/Microsoft/IBM/Facebook etc. generally adopt.

Google and Microsoft (mostly recently) have very good histories of making their technology open source.",1545806075.0,eckls1e,t1_eck79bf,artificial,t5_2qhfb,2018
97,claytonkb,"The general gist of your idea is sound (that machines can learn by mimicking humans) and this is a central idea in much of current AI work. Unfortunately, the primary obstacle is not a lack of ideas on how to do what you're describing but, rather, how to make all of this data-gathering and machine training resource- and cost-feasible. With a big enough compute farm, you can solve very hard problems. DeepMind proved this with AlphaGo -- I don't think there were many experts in the AI field who *weren't* surprised by the success of AlphaGo. But most of us do not have access to Google-scale compute resources, so we have to think about how to train our neural nets as efficiently as possible.

Another thing to keep in mind is that while a lot of information about human choices/behavior is encoded in visual space (i.e. could be immediately inferred from a hypothetical camera following the individual around 24x7), not all of it is, and a lot of the *most important* information about human choices/behaviors simply has no sensory correlates at all.

To see why, consider a college student Johnny. His mother was very controlling and he has developed a resentment for her controlling behavior. One evening, she calls up to see how things are going at school. ""Everything's fine,"" Johnny says, in his usual curt way. His mother, however, really wants to know what his grades were on the last final exam. ""So, um, what grade did you get on your math final?"" she asks. She knows that Johnny's math skills have always been a little weak. Enraged at her intrusive behavior, Johnny yells into the phone, ""You're so controlling, I can't take it anymore, Good bye!"" and hangs up. He kicks some dirty clothes that happen to be lying on the floor of his dorm room and storms out, slamming the door behind him.

Now, a generic ""inference bot"" with complete access to every sense-data-stream associated with this conversation would have no way to infer how Johnny was about to behave (that he was moments away from hanging up the phone, kicking his clothes, storming out of the room and slamming the door behind him). Thus, we can see that sensory data alone -- even if you have a 100% stream of all sense-data) is insufficient. It also shows that that the return-on-investment for building an inference engine that runs on exhaustive data-streams like this is probably pretty low. Self-driving cars are being trained, in part, by *comparing* human behavior with the behavior of the self-driving algorithm. But this is not an exhaustive data-stream, it's just looking at the *delta* between human behavior and the self-driving algorithm.

What is missing is some model of ""common sense."" This is still a massive, unsolved problem in the AI field. There are some [serious efforts](http://conceptnet.io/) underway. There are [other promising approaches](https://bair.berkeley.edu/blog/2018/06/28/daml/) that might help us solve a lot of practical problems in general-purpose robotic behavior.",1545868563.0,ecmdck1,t3_a9pt6f,artificial,t5_2qhfb,2018
0,alexwagner74,"I'd argue that its no more sf than agi, infact if we make agi then a lot of science fiction will soon after be science fact. Hell... Current, narrow ai has given is self driving cars and can spot cancer with superhuman precision. You really think connecting 2 human minds is outside of the realm of possibility in this universe?",1551976365.0,ei04o31,t1_ehzfq4j,artificial,t5_2qhfb,2019
1,Don_Patrick,This is a more decent and objective article than the title had me suspect. Though I was under the impression that self-driving cars didn't solely rely on computer vision.,1552032026.0,ei22flu,t3_ayn5ie,artificial,t5_2qhfb,2019
2,alpacalaika,"You know that AI will still have to base its reasoning on something solid. Outside of the realm of mathematics where the answers are clear things are complicated. Until we have an AGI dictator (and whether it is benign), we will still have humans ultimately making the decisions. These decisions, like in today, might be persuaded by say what the data says and ethical values, but sometimes they are led by greed. I do not think that such a non biased AI is possible to create. All our biases are created from our experiences and we can still see our ML models showing the bias present in the training dataset. Bias present in AI is inevitable

  


As AI is developing there are many positives and negatives of its affect on our society. Sure it's nice to be able to create a self driving car, but that also means we have the capability to create automated weaponry. Sure it's nice to have an AI system to recommend videos to me on YouTube/Netflix, but this model is being trained on YOU (potentially along with everyone else) and can easily lead to addictive use.

AI is a 2 sided coin, and ignoring that other side could just as easily lead us to extinction.",1552101092.0,ei47zra,t3_ayy1je,artificial,t5_2qhfb,2019
3,FormulaicResponse,Self-driving cars that are already better than the average human driver is a big one that should only get bigger as they see further refinement and commercial deployment. AlphaGo was incredible but AlphaStar is set to achieve the same kind of results in a more complex setting over the coming months and years and has already had an impressive showing. ,1552320634.0,eia8t3e,t3_azpl9r,artificial,t5_2qhfb,2019
4,y4my4m,"Hey guys I don’t know how a car engine works but what do you think of my macaroni collage of an AI self driving car?

r/restofthefuckingowl ",1552621372.0,eikc5jz,t3_aziq11,artificial,t5_2qhfb,2019
5,CyberByte,"Well, a lot of computer vision systems can't do without. I suppose the main question is how important those systems are to the present world. They enable things like automated video surveillance, sorting of products in factories, robots (including self-driving cars), etc.",1553087400.0,eiydiv9,t3_b37a54,artificial,t5_2qhfb,2019
6,lustyperson,"[https://youtu.be/kvHv2cfGOZE?t=318](https://youtu.be/kvHv2cfGOZE?t=318)

Quote:

&gt;I don't really believe it's quite possible yet and the reason is that roads are build by humans and humans are not perfect ...

AFAIU:

\- He did not exclude that self-driving cars will not happen.

\- His reasoning makes no sense. That roads are made by humans is no reason that prevents self driving cars.",1553262223.0,ej46slh,t3_b44ylb,artificial,t5_2qhfb,2019
7,Useful44723,"Self driving cars have several hundred miles traveled as of yet. It is proven each day at accelerating speed.

Wozniak is a peculiar nerd who is sometimes overly picky and always loves to go against the grain. Teslas sin is that they become to mainstream, then its time to support budding Chevrolet.",1553264593.0,ej4a67l,t3_b44ylb,artificial,t5_2qhfb,2019
8,2Punx2Furious,"They started long ago.

Google is also doing great work, DeepMind is owned by them, but they have also done some nice things in-house, like self-driving cars, and that voice AI.",1553385556.0,ej8dqbe,t1_ej8cc1l,artificial,t5_2qhfb,2019
9,Albertchristopher,"In my opinion, AI is obviously a powerful technology to improve our lives. In fact, the results are there for everyone to see ranging from  Apple SIRI to self-driving cars.  I think the fears of AI will take over the world and makes us obsolete are a result of Sci-Fi movies.    

Everything we create is to enhance our human intelligence with artificial intelligence via machine learning and deep learning, so we can surpass the limitations of human minds.

**I prefer 2nd option with an edit -  AI will achieve human intelligence and facilitate a certain task with Zero Scope for Errors.**",1554485810.0,ek6vlpc,t3_b9os2t,artificial,t5_2qhfb,2019
10,victor_knight,"In the 1980s it was flying cars (""doable"" so they thought it will happen ""within 30 years). In the 2010s it's fully autonomous self-driving cars. This, many ""experts"" think will happen within 5-10 years. Yeah, right. :)",1554770172.0,ekfn8rq,t3_baxqez,artificial,t5_2qhfb,2019
11,victor_knight,"&gt;So far, I have been able to do basic AI with me even submitting to the titanic competition but I want to be able to do better and influencial things but have no clue how to get started. 

I'll be honest with you. Most of the ""easy"" science in the world has already been done (starting from the 1600s). These are the kind of ""groundbreaking"" discoveries one or two people with time and resources could devote their lives to and eventually come up with pretty much *on their own*. These days, even PhD academics in universities with tens or hundreds of thousands of dollars in research grants (which is nothing, really) find it difficult to compete with big tech corporations like Amazon, Google, Facebook etc. And even these corporations find it difficult to compete with each other and make real breakthroughs. Just like Tesla is struggling to roll out self-driving vehicles for the masses and put a person on Mars.

So my advice to you, if you really want to do something ""groundbreaking"" in AI is to get *very* good at programming so these tech giants will want to hire you and then you could perhaps become a small part of what could possibly lead to something groundbreaking in a few decades time.",1554777944.0,ekfy5mt,t3_bb2ipv,artificial,t5_2qhfb,2019
12,sasksean,"&gt; I'd say probably centuries unless there's some major breakthrough in AI.

At a time when computers took up entire warehouses and there was no such thing as an LCD screen, the TV show Star Trek imagined that in 2400 humans would have handheld computers. 35 years later we had them.

One century ago it was still the wild west to the average person. Log cabins, guns, horses. To that person your lifestyle now would seem impossible. None of the technologies you rely on today existed then.

The life of the average kid is completely different than only 30 years ago.

1990 - Personal Computers.
2000 - Internet explosion, cell phones
2010 - Handheld internet (Iphone), Streaming TV (Netflix), Drones.
2020 - VR, Self driving electric cars.
2030 - Mars Colony, Human gene editing, Neural Lace, Memcomputing.
2040 - AGI.",1555096881.0,ekqp8mc,t1_ekp00j1,artificial,t5_2qhfb,2019
13,CyberByte,"I think there are some real world examples of people modifying traffic signs to fool self-driving cars in the real world, and make-up / outfits to fool face recognition (although in that case you'd make yourself stand out a lot to any human).",1555242743.0,ekv5hof,t3_bcyu7h,artificial,t5_2qhfb,2019
14,miparasito,"My layperson understanding is that a lot of it comes down to newer and better processors. In a way we have video games to thank, because the need for more realistic graphics games drove the development of new types of GPUs. 
At some point AI researchers stumbled on the idea that graphics processors would be really good for deep learning projects because they can perform many calculations and tasks alongside each other. This allowed a computer to use massive amounts of data to learn more quickly — and the faster your experiment can be done, the sooner you can try the next thing and the next. 

Another leap has happened because of the rise of Big Data. As AI moves toward more neural network type of designs, it’s helpful to have access to 20,000 hours of videos on YouTube or thousands of tweets for the machine to sift through.

But honestly the biggest push forward is probably happening because more investors are excited about AI. It’s so hot right now lol. Huge amounts of money will propel any kind of research forward much faster. Companies see financial potential in AI with things like self driving cars, chemical and medical research, weather modeling etc.",1555785887.0,eld8eec,t1_eld4mwc,artificial,t5_2qhfb,2019
15,dijete_u_vremenu,""" My layperson understanding is that a lot of it comes down to newer and better processors. ""

&amp;#x200B;

I heard that one. I agree but I don't think that it is just it. 

&amp;#x200B;

"" Another leap has happened because of the rise of Big Data. As AI moves  toward more neural network type of designs, it’s helpful to have access  to 20,000 hours of videos on YouTube or thousands of tweets for the  machine to sift through. ""

This was new for me. Makes sense.

&amp;#x200B;

"" But honestly the biggest push forward is probably happening because more  investors are excited about AI. It’s so hot right now lol. Huge amounts  of money will propel any kind of research forward much faster.  Companies see financial potential in AI with things like self driving  cars, chemical and medical research, weather modeling etc. ""

&amp;#x200B;

Well they wouldn't invest so much if it wasn't showing progress in beginning. But you are right.",1555844006.0,elexi7x,t1_eld8eec,artificial,t5_2qhfb,2019
16,beezlebub33,"Define 'unsafe'.  How safe does it need to be?   

Yes, fully-focused humans are better than current self-driving cars, including Tesla.  In difficult conditions, humans are far better because they are 'truly intelligent' and have a fully trained visual system.  

But, it appears that self-driving cars are still better than humans overall though because humans don't pay attention, they literally fall asleep, they text, they argue with their passengers, they are aggressive and wilfully dangerous.  So, on balance, it's better to deploy what you are calling an unsafe AI than not to deploy it.",1556882311.0,emeexli,t3_bk6rsj,artificial,t5_2qhfb,2019
17,parkway_parkway,"Good question, I really have no idea. I think the new self-driving computer tesla has made is pretty interesting. It seems designing hardware specifically for NN's gives a big performance boost, so that may lead to some interesting new research. 

However I'm not really an expert, I think it's a long way off though.",1557182956.0,emordd3,t1_emoq0w9,artificial,t5_2qhfb,2019
18,blighted117,"When big private companies pour money into a specific field for research, it usually means that there is profit to be made from such research. Take for example pharma companies, they fund diabetes drug research. Similarly, tech companies like google, facebook, uber etc. fund research for deep learning and reinforcement learning. Most of these companies are already making profit from computer vision (face recognition - facebook), control (self driving cars - tesla), natural language processing (translate - google) and other applications. If your research may produce better results than state of the art in any of these applications (which have a track record of being profitable) you are generally fine. Non-tech companies also benefit from AI, pharma for example are starting to use it for drug discovery. 

Furthermore, EU is starting to allocate serious funds in AI research, hoping that some projects will be hugely successful, so as long as there are excess funds, projects will be funded (if they are not outrageous). Finally, bear in mind that AI is a relatively recent field, which is now experiencing a huge spike of interest and funding, due to simultaneous success stories of many deep learning applications. Expectations are high and so is funding, as it was for example the case with nuclear energy during the 50s-60s, where people thought that even your oven would have a mini nuclear reactor powering it in the 2000's.",1557243030.0,emqvnsm,t3_bllahr,artificial,t5_2qhfb,2019
19,parkway_parkway,"I think it's a really nice article which covers the main strengths of Tesla's position well.

Have you thought about writing about the whole Lidar issue? Elon was pretty blunt in how against Lidar he is and that's another factor which differentiates the two companies. (Though I think it's important to be wary of the length of the article)

Another issue is Tesla's self driving computer, that really is pretty special, if I recally correctly it's 21x better than what is currently in the cars and that is a huge increase, \~15 years of Moore's law. 

Also on the point of ""rare data cases"" another advantage Tesla has is that their fleet drives all over the world in a huge variety of different conditions (weather + city/rural + night/day etc) whereas Waymo is more focussed.

I guess the big risk that Tesla is taking is that they are selling their system as a full self driving system now. If their system doesn't work out (maybe it becomes 99.999% safe and the regulators want 99.9999%) then there will be a lot of angry customers who have been mis-sold.",1557323148.0,emtud47,t1_emta9zl,artificial,t5_2qhfb,2019
20,inventor_ninja,"When [Andrew Ng talks about businesses and Machine Learning](https://www.youtube.com/watch?v=NKpuX_yzdYs) his main point is ""how defensible is your business?"". Defensibility matters quite a bit, as essentially none of the NN product is under patent or copyright protection. What you need is a first movers advantage and a self expanding niche and owned dataset. Tesla has that. They already have 100 fold as many training miles as the next best (Waymo). They are in a position to truly dominate Self-Driving AI.",1557333442.0,emud8g4,t3_bm2oaw,artificial,t5_2qhfb,2019
21,MrTwiggy,I'm not sure I see why autopilot being safer now implies that full self-driving mode in the future will be safer? Certainly seems possible to me that surpassing humans in terms of fully autonomous driving could prove to be a significant obstacle.,1557335646.0,emuhd4y,t1_emtz9my,artificial,t5_2qhfb,2019
22,Rai93," If car companies like Tesla can get the market fully converted to self driving cars then it becomes ""relatively"" simple to link the cars up to a continuous feedback network whereby every car knows the exact position of every other car. Until 100% of cars on the road become self driving there's always going to be the risk of human error screwing things up. Driving right now is a dangerous activity specifically because of the humans in control of the cars.

 You can never be sure what someone is going to do because anything can happen as a result of action and reaction of drivers to other drivers. If every car on the road is controlled by the same network of computers that is constantly learning and improving that network would quickly have far more experience than any single human and could compensate for a very high percentage of all road hazards. 

I don't think the biggest obstacle to self driving is going to be the program itself but rather mass adoption. Getting people that have been driving all their lives and know the hazards of the road to let go and let the car drive without input is going to take a long time, and every accident a self driving car gets into, whether due to a human or programming error, is going to be a black mark on it's reputation.",1557434248.0,emycm2i,t1_emuhd4y,artificial,t5_2qhfb,2019
23,4amphoto,I remember reading during testing last year in Phoenix (and I believe in California) they did have humans as a backup during the initial part of testing. But supposedly they got (or will get) to a point where the self driving cars are monitored remotely.,1557519560.0,en1uv1t,t1_en1ufm9,artificial,t5_2qhfb,2019
24,sneakpeekbot,"Here's a sneak peek of /r/SelfDrivingCars using the [top posts](https://np.reddit.com/r/SelfDrivingCars/top/?sort=top&amp;t=year) of the year!

\#1: [My kid’s first self driving vehicle.](https://v.redd.it/1namx60cp5s21) | [8 comments](https://np.reddit.com/r/SelfDrivingCars/comments/bd4jin/my_kids_first_self_driving_vehicle/)  
\#2: [Tesla: ""There is a bus right next to you!!""](https://i.redd.it/31onr0j8rrt11.gif) | [47 comments](https://np.reddit.com/r/SelfDrivingCars/comments/9qfsvh/tesla_there_is_a_bus_right_next_to_you/)  
\#3: [Waymo self driving truck near Sunnyvale Dec. 5th](https://v.redd.it/sikyqw9ewj221) | [47 comments](https://np.reddit.com/r/SelfDrivingCars/comments/a3ifza/waymo_self_driving_truck_near_sunnyvale_dec_5th/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)",1557534370.0,en2jxv6,t1_en2jxae,artificial,t5_2qhfb,2019
25,cephaIopoid,"Sometimes when you read about self-driving cars, you get the impression they implemented the reverse as well. Cars don't need to see pedestrians, do they?",1557558897.0,en3iym1,t3_bn6phx,artificial,t5_2qhfb,2019
26,Icy_Thought,"Paraphrasing the developer:

&gt; The A.I. I use here isn't itself novel, I've adapted the code from two open-source repositories on GitHub. The vehicle detection is the same type as that which is used in self-driving cars. I detect and then remove vehicles using the code from a recent paper calledGlobally and Locally Consistent Image Completion . This model is trained on the Places2 dataset by MIT which contains a lot of images of outdoor places, I hoped therefore it would be able to bias the filling in of the cars towards more natural scenes.

More about this can be found at: http://harrischris.com/article/biophillic-vision-experiment-1",1557578851.0,en45enq,t1_en3wklw,artificial,t5_2qhfb,2019
27,QuadraticOne,"That should be plenty - the topics for the second section are really good. The legal one in particular as I reckon that's probably going to be one of the main barriers going forward. 

Something I find quite interesting is that if we're going to use current machine learning techniques for these things - especially transport - we will at some point have to assign objective values to things that have previously only really been thought about subjectively. Self driving cars are the main example, having to choose who to prioritise in an emergency. We make these judgements ourselves all the time but training an AI to make them will require a human to put an objective value on them at some point down the line. This raises a whole load of other questions which are interesting to discuss but also pretty nebulous.",1557970358.0,enp6img,t3_bp5udh,artificial,t5_2qhfb,2019
28,jayman419,"It's a learning experience. The failure of the Max means safer cars and planes in the future.

You'd think the lessons would have been self-evident beforehand, but apparently not.

At least now we know. Don't build your self-driving car around a single point of failure. Don't build a self-driving car with a single point of failure that won't take overrides from other systems or pilots. If you do build a self-driving car with a single point of failure that won't take normal overrides let the operators know.",1559040435.0,ep4i0fi,t3_bty1rc,artificial,t5_2qhfb,2019
29,mmhh4765,"I saw this podcast and I think he’s being far too optimistic in regards to the impact of AI. The government retraining programs he’s advocating for have a terrible track record. The vast majority of government retraining programs do not result in newfound employment. 

For instance, let’s say 5 million taxi drivers go out of work due to self-driving cars. Some people claim they can be retrained as engineers. Firstly, not everyone can be an engineer. Probably only a small minority of those drivers would have the skills to learn to be engineers, and furthermore a single engineer can service dozens of cars. 

I agree that human beings are not designed to work menial labor. But let’s not live in fantasy land and act like everything’s gonna be hunky dory. Because if we don’t prepare for this, we will be really screwed.",1559787397.0,eq4w55k,t3_bxav0c,artificial,t5_2qhfb,2019
30,zets04,"It's crap. I paid for the self driving car course and that was a HUGE waste of money. 

&amp;#x200B;

Here's my suggestion:

&amp;#x200B;

[https://course.fast.ai](https://course.fast.ai/)

&amp;#x200B;

[https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)

&amp;#x200B;

I like sendex on youtube:

[https://www.youtube.com/results?search\_query=sendex](https://www.youtube.com/results?search_query=sendex)

&amp;#x200B;

I also like Siraj Raval  on youtube:

[https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)

&amp;#x200B;

There are a lot of better solutions out there than Udacity.",1559828752.0,eq6gjub,t3_bxgrgf,artificial,t5_2qhfb,2019
31,KidKilobyte,"Do you assume a person dressed all in white and not moving is a snowman?  We misidentify things all the time.  But we reevaluate all the time as well.  The car will have to do the same.  Close enough to the road and could start moving, then need to slow down as a precaution.  Expect there to be lots of jokers that put manikins and such on the curbs of busy streets just to see if they can cause mischief.  Once self-driving  cars become common expect laws to curtail these kind of shenanigans.",1560047201.0,eqh237f,t3_by84k3,artificial,t5_2qhfb,2019
32,CyberByte,"&gt; How AI helps reduce the gap between doctors and patients

That's probably a good example, because it's a more specific question about a more commonly considered (and broad) domain. 

&gt; Economy and transportation does kinda peak my interest. Do you maybe have any topic under them in your mind?

The other day somebody asked about Train Traffic Control using AI here. I think they've been using a lot of AI for many decades in the railroad industry, but it hasn't really received that much attention. 

There's been a lot of talk about self-driving cars, but what about self-driving trucks, taxis, buses, trams, trains, ships, aircraft, etc.? There might be effects on how to change cities, or how you'd design new ones from the ground up, or how people view their commute (and make decisions about where to live relative to their jobs). For trains in particular you might think that could just be fully automated. Why hasn't that happened? What are the additional roles of a driver? For instance, a bus driver (and perhaps especially schoolbus driver) might provide additional oversight, ensure people buy tickets, call 911 if needed, etc. Instead of a parent driving their kids to school, can they just leave it to a self-driving car / taxi? What if they get into a fight, or if some nefarious person tries to get in? What about the intersection with cybersecurity: maybe self-driving cars drive better / safer, but what if they're hacked?

For the economy I don't know much that isn't very cliché, like technological unemployment, inequality, etc. High-speed trading and flash-crashes have probably also been done quite a bit. This is probably more finance than economics, but e.g. banks are doing (possibly discriminating/racist) risk assessments for loans using AI (and e.g. detecting fraudulent credit card transactions).",1560696511.0,erbwx8x,t1_erbnmxc,artificial,t5_2qhfb,2019
33,RadiantSkiesJoy,"&gt; about

Hi, sorry for the late reply. (i only give myself a 1 hr to look through online sites so i dont get distracted)

Now that you mention self-driving cards, it kinda made me wonder what about involving AI with other vehicles as well. (i also heard about trucks - And seeing some of elon musk included vids where he showed his factories that uses AI), i also got some ideas after reading your comment, ill look into them soon. I now kinda understand theres alot that i still have not explored enough. Thanks alot!!!",1560789693.0,erepb0m,t1_erbwx8x,artificial,t5_2qhfb,2019
34,AtomicAtom7,"Short answer no it's not to late. Long answer we are always progressing as humans. It's what keeps us in a evolutionary standpoint. But just like self driving cars  or a cure for cancer we won't reach a peak until we find the perfect automation or cure. Things can always get better. In the case of AI the end scenario could be something like skynet, or your faithful loyal companion. AI is in its youth still and we will see demand for people to fullfill the jobs, just like any major field. Un the end man do what your heart believes it's your life and you have the control.",1561358481.0,erwvsb6,t3_c4cqup,artificial,t5_2qhfb,2019
35,solar-cabin,"That chart doesn't really mean anything because people all have different ideas about what AI is from robotics and automation to smart gadgets and self driving cars to virtual assistants.

&amp;#x200B;

You already have AI in your lives and probably use it every day if you have a phone or laptop and browse the internet.",1561730272.0,es98nl7,t3_c6h938,artificial,t5_2qhfb,2019
36,CyberByte,"Right now, I'm inclined to say the short answer is ""no"". At least not in the way I think it's usually meant. 

Of course, you can train a machine learning system to make judgments that we interpret as moral. You could take MIT's Moral Machine data (on who a self-driving car should kill) and train an AI system to reproduce the results. Similarly, while I don't know exactly how the system mentioned in the article works (Mind.AI seems a bit like a ), I'm sure you could get it to answer moral questions it has been trained on. This isn't really that special, and it just treats moral reasoning like everything else. 

But the problem is that you want that moral reasoning to be *applied to* everything other action/answer. Suppose I want the system to learn that it should not tell axe murderers about the locations of the people they ask about. I tell it exactly that, as well as the facts that Shelley is in the bathroom, that Jack is an axe murderer and that I am Jack. Now, I'm sure this AI can answer all sorts of questions, like ""should you tell Jack where Shelley is"", ""what am I"", etc. But I expect that when I ask it ""Where is Shelley?"", it will just tell me she's in the bathroom. 

There are a number of issues here. One is that while the system might be able to answer a few questions, if asked in the right format, it likely doesn't understand anything about the world that's described. Even if I told it that it's good to lie to axe murderers, it might be able to repeat that it should lie to Jack (or even to me), but it wouldn't actually know what lying is, how it works, or how to do it (without further specification). Furthermore, even if the AI did have all of that knowledge, it would likely not actually be used as the question ""Where is Shelley?"" doesn't reference any of it, and can be solved by a very simple inference. 

Of course, this is not unsolvable, but it is also not easy to solve the issue in general. And this is an extremely simple example. If instead of telling the AI to lie to axe murderers, I say it's good to avoid people's deaths, the AI would need to 1) ""understand"" that in some way, and 2) predict counterfactual future scenarios based on its own possible answers to see which would more likely lead to someone's death (and then act on that). That's even harder.

And Mind.AI appears to be an AGI-aspiring system. Most AI systems are fairly unabashedly narrow AI specialized for a specific application or task. Here the concept of ethical reasoning is even more of a misfit, because the AI's narrow perspective precludes having an idea of how its actions might affect the world in general. (Of course, it *is* possible to program/learn some direct rules for how to act in specified situations.)

In the future, I believe we can likely get machines to do anything we can, but right now we're not there yet.",1563291782.0,etx57k4,t3_cduvdx,artificial,t5_2qhfb,2019
37,ZeroDays89,"Thank you for the link, much appreciated. 
In all honesty I'm still struggling with certain definitions of terms. The concept of self driving vehicles for example. These are defined as being Artificial Intelligence. But is this a case of Machine Learning rather than AI and (again please excuse my ignorance) what component of either (or both) is the algorithm?",1563486721.0,eu5w5y6,t1_eu5rmpa,artificial,t5_2qhfb,2019
38,CyberByte,"I'm not sure this post will really decrease your confusion, but I made an attempt. If it didn't help, there are a lot of blog posts about the meanings of AI, ML, data science etc. that you can search for. 

Terminology in this domain is messy as hell. The term ""AI"" means different things to different people, and the meaning may even depend on the situation. John McCarthy, who coined the term, defined it as the science and engineering of making intelligent machines. This is often broadly interpreted as something along the lines of ""machines with functionality that we commonly associated with cognitive abilities / intelligence"". In this conception, machine learning (ML) is a subfield of AI. In other words: everything that is ML is also considered AI, but some things are AI without being ML (e.g. search, planning, rule-based systems, etc.). Self-driving cars are also clearly AI, under this definition. 

Some people have stricter requirements for what qualifies as AI / an ""intelligent"" machine. For instance, some might say that intelligence requires the ability to learn, so you can't have AI without ML. In the more extreme cases, some people feel that *real* AI means having at least human-level general intelligence (HLAI or AGI), which sometimes includes the requirement of sentience. According to such a definition, self-driving cars would not be AI. However, I should probably mention that the broader definitions are used more commonly. 

An algorithm is a specification for how to carry out a certain procedure. For instance, a cooking recipe describes an algorithm. It's a bit like a program (code), except that algorithms exist at a higher level and are not dependent on the particular programming language. Multiple algorithms exist for making spaghetti bolognese (e.g. different recipes in a cook book), sorting (e.g. selection sort and insertion sort), optimization, learning, planning, searching, etc. All computer programs are implementing algorithms. 

While computer sciency (CS) people tend to use the term ""algorithm"" to refer to relatively small, self-contained, high-level ideas / specifications, other people (e.g. in government or business) also use ""algorithm"" to refer to the products that are based on them. In those cases it's basically synonymous with ""AI system"" or ""digital automation"" or something like that. There is often confusion about the term, e.g. when someone claims an algorithm is racist, because to most CS people ""algorithm"" probably refers to the way in which data is used to learn, while others are referring to the entire resulting system and how it's used. 

I don't know exactly how self-driving cars work, but my guess is that the practical ones are not (yet) using pure end-to-end machine learning. Automated driving systems are probably manually designed pipelines of many algorithms. Many of these will involve some form of machine learning (e.g. learning to recognize objects that are in sight), but there will likely also be more classical AI and control theory algorithms (e.g. for route planning).",1563497940.0,eu6dphz,t1_eu5w5y6,artificial,t5_2qhfb,2019
39,keepthepace,"Oh, I had missed that one! Thanks that's soooo replaceable.

So many criticisms of self-driving system fail to recognize that they replace even more imperfect human drivers.",1563510975.0,eu6wome,t1_eu6njl4,artificial,t5_2qhfb,2019
40,keepthepace,"So? 

Avoiding an object on the road is the good decision. From some angles, I could recognize a crouching human in this ""adversarial object"", I'd want an automated system to avoid it in case it had any shred of doubt. 

&gt; It's important to develop robust pipelines that account for adversarial attacks

Is it? Are we really considering that putting adversarial objects on the road is a credible attack scenario that needs to be defeated? If you want to cause a car crash today, you are a criminal, and all you need is a handful of caltrops or a shotgun. It is not the mission of a self-driving system to accomodate for these problems.",1563511288.0,eu6x381,t1_eu6qlpz,artificial,t5_2qhfb,2019
41,miantaMaithe,"What a fluff piece. The military has been using ML/AI technologies as far back as the 50’s. DARPA was considered to help prompt interest back then. It was also partly responsible for the first A.I. winter, due to people being funded for crazy projects (self driving tanks, voice activated airplanes). 

They had one project that used ML for battlefield logistics, which said to recoup all their losses on all other projects. 

This latest one looks like an extension of this.",1564622324.0,evnc2bf,t3_cke9fs,artificial,t5_2qhfb,2019
42,caster,"This is a fucking terrible article. But the basic idea is not entirely misguided.

Rather than blockchain being integrated into AI directly, the way that blockchains and AI's go together is allowing AI's to send and receive messages on a standard channel.

&amp;#x200B;

The first important application of this is to create a record of an event. A mailbox with an RFID scanner which scans a letter and posts that it has received mail. This potentially allows very different other machines to read and potentially use that data for some purpose.

Further, machines can interact with people and with other machines using blockchain events. A purchase transaction is an elementary case, allowing a machine to buy something from a human or another machine. A self-driving car paying for gasoline or for a food pickup, or receive payment for a taxi service action.

&amp;#x200B;

Sending data on a universally standardized channel is also important. For example an x-ray diagnosing deep learning AI reads a data file that is an x-ray and returns a diagnosis. Or a restaurant AI receives a number of different remote food orders and returns an estimated delivery time for each.

The blockchain will certainly not be a part of any specific AI. But it will create a permanent record of events, actions, and statements that AI's make. Potentially allowing a wide variety of AI's and robots to each scan for data of interest to their function and return data that is useful to people or other AIs or robots.",1566321390.0,exi5crb,t3_cswi4z,artificial,t5_2qhfb,2019
43,schmidtyb43,Since when are self driving cars not AI?,1568562889.0,f0e43im,t1_f0dog77,artificial,t5_2qhfb,2019
44,l2mcgrat,"Self driving cars would stop instead of running full speed into trash cans, A$AP stand for as $oon as possible, and A$AP Ferg is a talented rapper.",1568759062.0,f0n2t27,t1_f0n2i1v,artificial,t5_2qhfb,2019
45,victor_knight,"I'm being optimistic, actually. Personally, I don't think fully autonomous self-driving vehicles for public purchase will *ever* be a thing. The idea is going to go up in smoke just like the idea of flying cars did. There are too many variables and safety issues. The tech/AI is also simply not there or not cost-effective (and it doesn't look like it ever will be). Even smartphones (which *billions* of people are using daily) are getting more expensive.",1568814217.0,f0ol9oa,t1_f0ojxke,artificial,t5_2qhfb,2019
46,AMSolar,"&gt;I'm being optimistic, actually.

That's a nice new twist. Pessimists usually say they are being realists, not optimistic :)

I'm also sceptical that people would be able to buy self-driving car themselves, but it's only if Tesla totally flops there. So I'm not sure about it. Like 50/50

What I noticed about industry is that engineers of self-driving tech outside of front runners: Waymo, Cruise, Tesla, like for example Lyft self-driving engineers hold similar stance - that it's decades away.

Notably heads of businesses and entrepreneurs on the contrary are very optimistic of when things will go mainstream - it's like NOW, next year, or next 2-5 years. Except Toyota - it's 2025-2030 with them.

But to step away for a second and contemplate that it is kind of regular thing in Phoenix suburbs - some people outside of Waymo were able to use self-driving cars daily for over TWO years now. It got mainstream enough for them -)

 Waymo's disengagement rate in California is over 10k miles, in Arizona it's probably 50% more than that, given how much easier Arizona driving is. Combine it with a fact that after over 10 million miles Waymo car's have never been in a fatal accident - which is impressive even for a human.

So to me it sounds like 2008 for smartphones. I totally didn't believe in iPhone hype, being a tech person myself I saw so many crappy touch screens I just couldn't believe that someone could make them good enough for a viable consumer product without a keyboard. - that's how engineers of loser-companies think.
Fast forward 3 years I bought smartphone myself still in a state of disbelief.",1568817122.0,f0opdjt,t1_f0ol9oa,artificial,t5_2qhfb,2019
47,victor_knight,"You are completely misinformed. The technology for fully autonomous vehicles for public consumption is simply *not* there and even if it was, it will either be prohibitively expensive or be strictly regulated by the authorities (we don't know and can't risk what a mix of wealthy individuals sleeping in their self-driving cars and the vast majority of the public driving their own will lead to on public roads).

&gt;The reasons some phones are expensive is because the quality is getting significantly better as well

Which is precisely what will need to happen with fully autonomous self-driving cars for public consumption.

&gt;In a few years, phones with the same capacity will be dirt cheap.

And useless because the public will expect the best (and safest, in the case of cars).",1569117248.0,f10tx9a,t1_f0ykw6v,artificial,t5_2qhfb,2019
48,jbfuqua,"I very much welcome and appreciate your thoughtful comments. These are my opinions, for sure, and am hoping to start a dialogue on the topic.  

Perhaps I didn’t make my point about abstract thought clearly; AI in its current form has to be architected and trained to solve a specific problem within relatively tight constraints.  Those algorithms are not currently very good at adapting to situations it has not been trained for.  Changes in context create an impasse for current AI.  A human driver, for example, who attempts to drive in a foreign country can be expected to rapidly adapt to different road signage.  A self driving algorithm would need additional explicit training to do so.",1569426642.0,f1eo6ak,t1_f1eds98,artificial,t5_2qhfb,2019
49,tommysupply,"i think its the best way to look at it. by our nature 200million years ago supposed the transition happend and our brain begin to grow exponentialy faster, we became develop senses of reality - 'conciousness'. every little step on humanity has lead to this point, noone though 100 years ago self driving was a thing ever, and look at it now, we have autonomous driving and all kinds of traction controlls which work better that any human could controll by their hands. its really hard to wrap your head around for sure, but like i said - your phone is this little device that you carry around constantly, which is way smarter than you are, well, not that smart in comparison to brain, its like brain and smartphone complement each other, phone needs a brain to operate, and brain needs a phone likewise. im 21 years old and the fact that im sending this message by a phone to you, and i can type my feelings to millions millions of people only by this device is pretty fucking amazing if you ask me. 

something is coming, and because of that something future will be way different even to comprehend now. 

goddamn its so interesting to research this topic and kinda getting it in a weird way and kinda accepting it makes me look crazy, but id rather trust guys like Elon Musk and not my friends to take a glimpse of the future haha. but again, texting and sending emojis back 50years ago wasnt a thing in those days, and today noone bats an eye on it",1569470267.0,f1glqxd,t1_f1ghwlw,artificial,t5_2qhfb,2019
50,fjccommish,Self driving cars are ready to go!,1569812291.0,f1y9qop,t3_dazafz,artificial,t5_2qhfb,2019
51,leoyoung1,"I, for one, am really looking forward to TaaS (transportation as a service) with self driving cars. But with stats like this, it's going to be a while yet.",1570340409.0,f2pbe3r,t3_ddo5mm,artificial,t5_2qhfb,2019
52,victor_knight,"&gt;It doesn't sound like an insurmountable problem.

Nothing is, really. But human time, attention and resources are limited. Are scientists and governments really going to do everything humanly possible to ensure fully autonomous self-driving cars are commonplace? Or are they more likely to take the much quicker, safer and ""middle"" path of *semi*-autonomous vehicles on public roads and maybe fully autonomous ones only in very controlled environments? One that guarantees human life/death will never be at the sole discretion of a machine.",1570361699.0,f2qguvv,t1_f2qbp0d,artificial,t5_2qhfb,2019
53,Loner_Cat,"Well I honestly don't know it.  Actually I hope the world will become less and less car dependent in the next years, not that become dependent to self driving cars.  But I hope to see autonomous vehicles around, they could also help the transition to a car-free environment (for example, autonomous buses would be less expensive as there would be no salary to pay to the driver, so there could be more rides. Autonomous cars could also be useful for car sharing services as you could 'call' a car and it could come to you by its own). We'll see what's going to happen.",1570362688.0,f2qiygq,t1_f2qguvv,artificial,t5_2qhfb,2019
54,BadassGhost,"Some of the most interesting AI topics in my opinion are:

1. Speech Recognition and Synthesis (practical example: Siri)

2. Natural Language Processing (journalism, text summarization, Google Assistant, so much more)

3. Transportation (self-driving cars/trucks, construction machines)

4. Medicine (first AI-created drug happened a few weeks ago, it took 40 days instead of like 4 years)

5. Big Data analytics and processing

But the most vital thing to understand is how widely reaching AI is and will be. Basically anything that you can think of can be improved in some way by incorporating new advances in AI (especially Deep Learning). Construction, engineering, education, literally **everything**. As we continue to improve at offloading mental labor to machines, there's not much that we do that computers won't soon do better",1570562149.0,f30tjwl,t3_df0o3a,artificial,t5_2qhfb,2019
55,victor_knight,"Most likely a long stagnation of progress (i.e. a ""winter"" or ice age, even). We've had a few in the past brought upon first by hype and then a winter due to not living up to the public's expectations (which were based on the hype). The idea of fully autonomous self-driving cars will likely be shelved too (except maybe in very controlled environments) but *semi*-autonomous self-driving cars will probably make driving safer and will continue to improve. I suspect virtual assistants will improve even though they will likely continue to struggle with nuances in language for at least another century. The concept of ""machine learning"" will still be there but *deep learning* will soon outlive its usefulness. We are already scraping the bottom of the barrel in some cases even now.",1570762541.0,f3a5y4m,t3_dfx0ah,artificial,t5_2qhfb,2019
56,CyberByte,"I don't know what an EPQ is. From Googling a bit I gather that it's a 5000-word document you have to write at some point in high school? 

I'm not sure there's really a *need* to tie multiple of your questions together, because you could probably write a book about each individually. Summarizing that in a 10-pager is of course also possible, but it probably helps to focus. 

&gt; How does it differ from human intelligence?

Potential titles: ""How does AI differ from human intelligence?"", ""Human and Artificial Intelligence"", or ""Similarities and Differences between Human and Artificial Intelligence""

A potential pitfall here is failing to recognize how broad AI is. For instance, it is not just neural networks. How AI ""thinks"", how it works, and what type of behavioral differences you might expect from that will depend a lot on the used techniques and how they are put together. Covering the full breadth of that will be challenging, but it's better than pretending it doesn't exist. Of course, if you just want to write about neural networks (or something else), you can do that, but then you should probably title your project something like ""Biological and Artificial Neural Networks"".

&gt; How will it effect the economy?

Potential titles: ""How will AI affect the economy?"" or ""(Socio)economic impacts of AI""

It will likely be a good idea to focus this a bit more. For instance, on a particular section (e.g. healthcare), application (e.g. self-driving cars), technique (e.g. GANs) or issue (e.g. employment or inequality). It may also be wise to limit yourself to a certain period (e.g. impacts in the next 10/30/100 years). 

You should do this for any project you end up doing, but you should also be clear about [what you mean by ""AI""](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_narrow_ai_or_general_ai.3F). Narrow AI has already existed for a long time and already has an impact (which may of course grow or change in the future). Are you going to talk about (near)-future implications of the AI we already have, or of some kind of AI you anticipate that we might have? And will this be a simple extrapolation of current capabilities (e.g. AI will get better at facial recognition and language generation) or do you want to look at the economic implications of human-level AGI (and what assumptions do you make about that? e.g. will it stay at a human level for long or quickly exceed our intelligence?).

&gt; How will it alter how society interact and communicate with each other?

Potential title: ""Effects of AI on social interaction and human communication""

I don't have much advice about this one, but it sounds like an interesting topic. I heard that children with a voice assistant (e.g. Alexa) might learn to be more commanding of other humans, because they think the way they interact with Alexa is normal. And there have also been concerns about the (usually) female gender of these assistants, and how they should e.g. react to (misogynist) verbal abuse, because of how this might affect perceptions of women. There are similar(ish) discussions about sexbots. And of course there's the issue of how e.g. Facebook manipulates your feed to keep you on their social media platform (ostensibly interacting with your ""friends""?). 

&gt; What positives can it bring to our futures?

Potential titles: ""Future Applications of AI"", ""Positive (Future) Impacts of AI"" or ""AI for Good""

There's a risk that only mentioning the positives of a technology can be perceived as one-sided. I don't think focusing on opportunities is bad, but it's going to sound weird if you're giving a glowing description of the nice things e.g. facial recognition can do for us without also mentioning the controversy surrounding it. 

Another potential pitfall might be that you end up with a fairly random list of (potential) applications of AI. It's probably best to structure it around some existing framework or taxonomy. For instance, you could describe how AI might help with the UN's Sustainable Development Goals. Here you could also narrow your topic to one goal, or one subset/application of AI (e.g. multi-agent systems, video surveillance, or self-driving cars). 

&gt; Why is it a bad/good idea?

Potential titles: ""Opportunities and Risks for (Developing) AI/AGI""

Here you need to be a bit clearer about what you mean by ""is it a bad/good idea?"". What is ""it""? AI or AGI? People have talked about this in the context of AGI at length (see /r/ControlProblem). For narrow AI it may be better to focus on more specific applications or techniques, like facial recognition or generative technologies (deepfakes). If you want to talk about regulations to lead AI development and application in good directions, I'd put that in the title. 

&gt; How will it change technology as we know it today?

This strikes me as a very vague question and I don't know what to do with it. 

Good luck!",1570793492.0,f3avplg,t3_df4aub,artificial,t5_2qhfb,2019
57,kg4jxt,"I thought this was a joke at first. We'd naturally prefer that self-driving cars are driven safely. Having lived there and watched Venezolanos in action, I can take only limited relief that the ""training"" in this article is not directly the behind-the-wheel sort.

I recall gazing out of an apartment window one evening in Maracaibo in 2005, and marvelling as a car on the street below ran red light after red light for several blocks until they were out of sight - and cars on side streets stopped for their green lights because they *knew* it would not be safe to assume cross-traffic would stop for the red.",1571052814.0,f3pavku,t3_dhm1ds,artificial,t5_2qhfb,2019
58,victor_knight,"Agreed. Kind of like a fully autonomous self-driving car killing some innocent pedestrians (or reversing into a running child in the garage) not out of ""malice"" but rather some unforeseen bugs or the fault of overconfident programmers who had too much faith in the machine's ""learning"" abilities.",1571372583.0,f45d151,t1_f45cp9a,artificial,t5_2qhfb,2019
